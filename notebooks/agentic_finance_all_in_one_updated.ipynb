{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e8e8972",
      "metadata": {
        "id": "8e8e8972"
      },
      "source": [
        "# Agentic Finance — All-in-One Notebook\n",
        "This notebook mirrors our layered app.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Market Research Agent — Price, Indicators, and News Pipeline\n",
        "\n",
        "This project builds a lightweight research workflow for equities: we fetch historical prices, compute common technical indicators (SMA, RSI), and pull headline summaries with basic sentiment. A small JSON cache keeps runs fast and reproducible while avoiding API rate limits. The notebook(s) walk through data loading, quick EDA, feature prep, and simple evaluation.\n",
        "\n",
        "**Course:** MSAAI 520-02 — Group 5  \n",
        "**Date:** October 18, 2025\n",
        "\n",
        "## Team\n",
        "- Ali Azizi  \n",
        "- Sunitha Kosireddy  \n",
        "- Victor Salcedo\n"
      ],
      "metadata": {
        "id": "kl2KAh90u7JO"
      },
      "id": "kl2KAh90u7JO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f452ac",
      "metadata": {
        "id": "21f452ac",
        "outputId": "d7c2cc1b-c923-483a-dff6-b46bd4327c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: \\mnt\\data\\agentic-finance\n"
          ]
        }
      ],
      "source": [
        "# Setup: create package tree and write source files (no changes to your code)\n",
        "import pathlib, sys\n",
        "ROOT = pathlib.Path(\"/mnt/data/agentic-finance\")\n",
        "SRC = ROOT / \"src\"\n",
        "UI = ROOT / \"ui\"\n",
        "sys.path.insert(0, str(ROOT))\n",
        "print(\"Project root:\", ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac59b6",
      "metadata": {
        "id": "1dac59b6"
      },
      "source": [
        "### Add repo root to sys.path and ensure packages exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2be1c7",
      "metadata": {
        "id": "ee2be1c7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Find repo root (folder containing \"src\")\n",
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"src\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "sys.path.insert(0, str(ROOT))  # make \"src\" importable\n",
        "\n",
        "# Ensure packages (empty __init__.py files)\n",
        "for p in [\n",
        "    ROOT / \"src\",\n",
        "    ROOT / \"src\" / \"config\",\n",
        "    ROOT / \"src\" / \"data_io\",\n",
        "    ROOT / \"src\" / \"system\",\n",
        "    ROOT / \"src\" / \"analysis\",\n",
        "\n",
        "\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    (p / \"__init__.py\").touch(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54efe4b1",
      "metadata": {
        "id": "54efe4b1"
      },
      "source": [
        "## src/analysis/features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "383f0b0b",
      "metadata": {
        "id": "383f0b0b"
      },
      "outputs": [],
      "source": [
        "# src/analysis/features.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_sma(prices: pd.DataFrame, window: int) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    return prices[\"close\"].rolling(window=window).mean()\n",
        "\n",
        "def compute_rsi(prices: pd.DataFrame, window: int = 14) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    delta = prices[\"close\"].diff()\n",
        "    gain = np.where(delta > 0, delta, 0.0)\n",
        "    loss = np.where(delta < 0, -delta, 0.0)\n",
        "    gain_s = pd.Series(gain, index=prices.index)\n",
        "    loss_s = pd.Series(loss, index=prices.index)\n",
        "    avg_gain = gain_s.rolling(window=window).mean()\n",
        "    avg_loss = loss_s.rolling(window=window).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99347f63",
      "metadata": {
        "id": "99347f63"
      },
      "source": [
        "## src/analysis/text.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc78099",
      "metadata": {
        "id": "fdc78099"
      },
      "outputs": [],
      "source": [
        "# Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "# from config.settings import SETTINGS\n",
        "\n",
        "# -----------------------------\n",
        "# Existing tagging / preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "TAG_RULES = {\n",
        "    \"earnings\": [\"earnings\", \"eps\", \"guidance\", \"outlook\", \"quarter\", \"revenue\"],\n",
        "    \"product\":  [\"launch\", \"iphone\", \"chip\", \"feature\", \"service\"],\n",
        "    \"legal\":    [\"lawsuit\", \"regulator\", \"antitrust\", \"fine\", \"settlement\"],\n",
        "    \"macro\":    [\"inflation\", \"rates\", \"fed\", \"recession\", \"gdp\"]\n",
        "}\n",
        "\n",
        "def preprocess_news(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"published_at\",\"source\",\"title\",\"summary\",\"url\",\n",
        "            \"overall_sentiment\",\"tags\",\"numbers\"\n",
        "        ])\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Alpha Vantage format is like \"20251017T200143\"\n",
        "    # Parse with explicit format; keep timezone-aware for safety\n",
        "    df[\"published_at\"] = pd.to_datetime(\n",
        "        df[\"published_at\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\", utc=True\n",
        "    )\n",
        "\n",
        "    # Drop rows with no title/url; keep others (don’t drop NaT here — the date filter happens later)\n",
        "    df = df.dropna(subset=[\"title\",\"url\"]).drop_duplicates(subset=[\"url\"])\n",
        "    df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def classify_tags(text: str) -> list[str]:\n",
        "    text_l = text.lower()\n",
        "    tags = [k for k, kws in TAG_RULES.items() if any(kw in text_l for kw in kws)]\n",
        "    return tags or [\"general\"]\n",
        "\n",
        "NUM_RE = re.compile(r'(\\$?\\b\\d+(?:\\.\\d+)?%?)')\n",
        "\n",
        "def extract_numbers(text: str) -> list[str]:\n",
        "    return NUM_RE.findall(text or \"\")[:6]\n",
        "\n",
        "def add_tags_and_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    df[\"tags\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(classify_tags)\n",
        "    df[\"numbers\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(extract_numbers)\n",
        "    return df\n",
        "\n",
        "def recent_topk(df: pd.DataFrame, topk: int, days: int, required_tags: list[str] | None = None) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Make an aware UTC cutoff; df['published_at'] is already UTC-aware\n",
        "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n",
        "    f = df[df[\"published_at\"] >= cutoff]\n",
        "\n",
        "    if required_tags:\n",
        "        want = [t.strip().lower() for t in required_tags]\n",
        "        f_tags = f[f[\"tags\"].apply(lambda ts: any(t in [x.lower() for x in ts] for t in want))]\n",
        "        f = f_tags if not f_tags.empty else f\n",
        "\n",
        "    return f.sort_values(\"published_at\", ascending=False).head(topk)\n",
        "\n",
        "# -----------------------------\n",
        "# NEW: shared agent utilities\n",
        "# -----------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "def strip_code_fences(s: str) -> str:\n",
        "    \"\"\"Remove leading/trailing ``` blocks (optionally ```json).\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.IGNORECASE)\n",
        "\n",
        "def to_float(x, default: float = 0.0) -> float:\n",
        "    \"\"\"Best-effort conversion of model outputs or strings to float.\"\"\"\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            xs = x.strip().lower()\n",
        "            # map common words to numeric anchors\n",
        "            if xs in (\"high\", \"strong\", \"bullish\", \"overbought\"):\n",
        "                return 0.8\n",
        "            if xs in (\"medium\", \"moderate\", \"neutral\"):\n",
        "                return 0.5\n",
        "            if xs in (\"low\", \"weak\", \"bearish\", \"oversold\"):\n",
        "                return 0.2\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def clamp(x: float, lo: float, hi: float) -> float:\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "def normalize_score(v: float) -> float:\n",
        "    \"\"\"\n",
        "    Normalize arbitrary score ranges to [-1, 1].\n",
        "    Heuristics:\n",
        "      - If already in [-1,1], keep.\n",
        "      - If in [0,1], map to [-1,1] via (v-0.5)*2.\n",
        "      - If in (1,100], treat as percent.\n",
        "      - If in (1,10], treat as 0-10 and map.\n",
        "      - Else, clamp.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        v = float(v)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    if -1.0 <= v <= 1.0:\n",
        "        return v\n",
        "    if 0.0 <= v <= 1.0:\n",
        "        return (v - 0.5) * 2.0\n",
        "    if 1.0 < v <= 100.0:\n",
        "        v01 = v / 100.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    if 1.0 < v <= 10.0:\n",
        "        v01 = v / 10.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    return clamp(v, -1.0, 1.0)\n",
        "\n",
        "def normalize_conf(v) -> float:\n",
        "    \"\"\"Normalize any confidence-like value to [0,1].\"\"\"\n",
        "    f = to_float(v, 0.7)\n",
        "    if 1.0 < f <= 100.0:\n",
        "        f = f / 100.0\n",
        "    return clamp(f, 0.0, 1.0)\n",
        "\n",
        "# Optional: helpers to render structured dicts into strings (for external tools)\n",
        "def pretty_json_block(obj: dict, max_chars: int = 4000) -> str:\n",
        "    \"\"\"Return a fenced JSON markdown block, truncated for UI safety.\"\"\"\n",
        "    try:\n",
        "        js = json.dumps(obj, ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        js = str(obj)\n",
        "    if len(js) > max_chars:\n",
        "        js = js[: max_chars - 20] + \"\\n... (truncated)\"\n",
        "    return f\"```json\\n{js}\\n```\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a110bf",
      "metadata": {
        "id": "a2a110bf"
      },
      "source": [
        "## src/config/settings.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74137240",
      "metadata": {
        "id": "74137240"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _find_project_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Walk upward to find the repo root heuristically.\n",
        "    Treat a folder containing both 'src' and 'data' as the root.\n",
        "    Fallback to the starting directory if not found.\n",
        "    \"\"\"\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# project root = repo root\n",
        "if \"__file__\" in globals():\n",
        "    ROOT = Path(__file__).resolve().parents[2]\n",
        "else:\n",
        "    # Notebook / REPL: start from CWD and auto-detect root\n",
        "    ROOT = _find_project_root(Path.cwd())\n",
        "\n",
        "load_dotenv(ROOT / \".env\", override=False)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Settings:\n",
        "    data_dir: Path = ROOT / \"data\"\n",
        "    cache_dir: Path = ROOT / \"data\" / \"cache\"\n",
        "    runs_dir: Path = ROOT / \"data\" / \"runs\"\n",
        "    alpha_api_key: str = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"\")\n",
        "    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "    news_window_days: int = 14\n",
        "    topk_news: int = 5\n",
        "    cache_ttl_minutes: int = 60\n",
        "\n",
        "SETTINGS = Settings()\n",
        "SETTINGS.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "SETTINGS.runs_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581069c6",
      "metadata": {
        "id": "581069c6"
      },
      "source": [
        "## src/data_io/cache.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple JSON file cache (TTL + atomic writes)\n",
        "I use a tiny JSON-based cache so repeated runs don’t redo the same work. Each key maps to a file on disk, with a timestamp to support a time-to-live (TTL). Saving is done via a temp file + replace so partial writes don’t corrupt the cache.\n",
        "\n",
        "**Inputs:** `key`, optional `ttl_minutes`, arbitrary `data`  \n",
        "**Key choices:** per-key JSON files under `SETTINGS.cache_dir`, ISO-8601 for dates, atomic write on save  \n",
        "**Output:** `load_cache` returns cached payload or `None`; `save_cache` writes `{\"_ts\": ..., \"data\": ...}` to disk\n"
      ],
      "metadata": {
        "id": "XIKxbGY5r1ce"
      },
      "id": "XIKxbGY5r1ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a42293c",
      "metadata": {
        "id": "6a42293c"
      },
      "outputs": [],
      "source": [
        "# Purpose: lightweight disk cache with TTL and atomic writes\n",
        "# Context: used by data fetchers (e.g., price downloads) to avoid repeat network calls\n",
        "# Notes: filenames derive from key under SETTINGS.cache_dir; payload stored as JSON\n",
        "\n",
        "# cache.py\n",
        "from __future__ import annotations\n",
        "import json, time\n",
        "from datetime import date, datetime\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def _cache_path(key: str) -> Path:\n",
        "    return SETTINGS.cache_dir / f\"{key}.json\"\n",
        "\n",
        "def _json_default(o: Any):\n",
        "    # datetime & pandas.Timestamp (subclass of datetime) → ISO 8601\n",
        "    if isinstance(o, (datetime, date)):\n",
        "        return o.isoformat()\n",
        "    # Fallback: make a best-effort string (covers Decimal, Path, Enum, etc.)\n",
        "    try:\n",
        "        return str(o)\n",
        "    except Exception:\n",
        "        return repr(o)\n",
        "\n",
        "def load_cache(key: str, ttl_minutes: int | None = None) -> Any | None:\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        if ttl_minutes is None:\n",
        "            return obj.get(\"data\")  # consistent: always return payload\n",
        "        if (time.time() - obj.get(\"_ts\", 0)) <= ttl_minutes * 60:\n",
        "            return obj.get(\"data\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def save_cache(key: str, data: Any) -> None:\n",
        "    p = _cache_path(key)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
        "    payload = {\"_ts\": time.time(), \"data\": data}\n",
        "    tmp.write_text(json.dumps(payload, ensure_ascii=False, default=_json_default), encoding=\"utf-8\")\n",
        "    tmp.replace(p)  # atomic on most OS/filesystems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ee3409",
      "metadata": {
        "id": "78ee3409"
      },
      "source": [
        "## src/data_io/prices.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch historical prices (with simple caching)\n",
        "I pull OHLCV data from Yahoo Finance and return a clean DataFrame. I use a short-lived cache so repeated runs don’t keep hitting the API. If data is already cached, I return it immediately. I also flatten any MultiIndex columns and standardize names so downstream code stays consistent.\n",
        "\n",
        "**Inputs:** `symbol`, `start`, `end`  \n",
        "**Key choices:** use `yfinance.download`, cache key includes date range, flatten MultiIndex, keep both `close` and `adj_close`  \n",
        "**Output:** DataFrame with `date, open, high, low, close, adj_close, volume`\n"
      ],
      "metadata": {
        "id": "vIV91U3JrnJa"
      },
      "id": "vIV91U3JrnJa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6014f6",
      "metadata": {
        "id": "fb6014f6"
      },
      "outputs": [],
      "source": [
        "# Purpose: download OHLCV from Yahoo Finance and return a normalized DataFrame with caching\n",
        "# Context: called by data prep steps before features/EDA; avoids repeated network calls\n",
        "# Notes: flattens MultiIndex cols, standardizes names, stores json-serializable cache\n",
        "\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_prices(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:\n",
        "    cache_key = f\"prices_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "    df = yf.download(symbol, start=start, end=end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0].lower() for c in df.columns]\n",
        "    df = df.reset_index().rename(columns={\n",
        "        \"Date\": \"date\", \"open\":\"open\",\"high\":\"high\",\"low\":\"low\",\"close\":\"close\",\"adj close\":\"adj_close\",\"volume\":\"volume\"\n",
        "    })\n",
        "    df[\"date\"] = df[\"date\"].astype(str)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a58f9b6",
      "metadata": {
        "id": "7a58f9b6"
      },
      "source": [
        "## src/data_io/indicators.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch technical indicators (Alpha Vantage with local fallback)\n",
        "I get daily SMA/RSI for a symbol. I try Alpha Vantage first and fall back to computing the indicator locally from our price history when the key is missing or the API is rate-limited. Results are cached so repeated calls are fast.\n",
        "\n",
        "**Inputs:** `symbol`, `indicator` (`\"SMA\"` or `\"RSI\"`), `time_period`  \n",
        "**Key choices:** Alpha Vantage params (daily interval, series_type=close), JSON parsing with key map, local fallback uses `compute_sma`/`compute_rsi` on `fetch_prices`  \n",
        "**Output:** DataFrame with `date` and indicator column(s), sorted ascending\n"
      ],
      "metadata": {
        "id": "UTEo0-OksEFN"
      },
      "id": "UTEo0-OksEFN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752d66ae",
      "metadata": {
        "id": "752d66ae"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch SMA/RSI via Alpha Vantage with a cached local-compute fallback\n",
        "# Context: used by feature pipelines that need daily indicators\n",
        "# Notes: caches by (symbol, indicator, time_period); normalizes dates and numeric types\n",
        "\n",
        "# src/data_io/indicators.py\n",
        "from __future__ import annotations\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.analysis.features import compute_sma, compute_rsi\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "KEYS = {\"SMA\": \"Technical Analysis: SMA\", \"RSI\": \"Technical Analysis: RSI\"}\n",
        "\n",
        "\n",
        "# If AV isn’t available (no key/limit), our code falls back to computing indicators locally from prices using our compute_sma / compute_rsi.\n",
        "def _fallback_from_prices(symbol: str, indicator: str, time_period: int) -> pd.DataFrame:\n",
        "    prices = fetch_prices(symbol, None, None)\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if indicator == \"SMA\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"SMA\": compute_sma(prices, window=time_period)})\n",
        "    elif indicator == \"RSI\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"RSI\": compute_rsi(prices, window=time_period)})\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna().sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def fetch_indicator(symbol: str, indicator: str, time_period: int = 14) -> pd.DataFrame:\n",
        "    key = KEYS.get(indicator)\n",
        "\n",
        "    # Try cache first\n",
        "    cache_key = f\"indicator_{symbol}_{indicator}_{time_period}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    if not SETTINGS.alpha_api_key or key is None:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    params = {\n",
        "        \"function\": indicator,\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": \"daily\",\n",
        "        \"time_period\": time_period,\n",
        "        \"series_type\": \"close\",\n",
        "        \"apikey\": SETTINGS.alpha_api_key,\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(BASE, params=params, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        # Alpha Vantage quota message handling:\n",
        "        if (not data or key not in data or not data[key] or \"Note\" in data or \"Information\" in data or \"Error Message\" in data):\n",
        "            df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "            save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "            return df\n",
        "    except Exception:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "    df.reset_index(inplace=True)\n",
        "    df = df.rename(columns={\"index\": \"date\"})\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30738873",
      "metadata": {
        "id": "30738873"
      },
      "source": [
        "## src/data_io/news.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch news (Alpha Vantage, symbol-filtered, cached)\n",
        "I pull recent news from Alpha Vantage’s `NEWS_SENTIMENT` endpoint and keep only items that explicitly mention my ticker with decent relevance. I cache the result so I don’t burn quota on repeat runs.\n",
        "\n",
        "**Inputs:** `symbol`  \n",
        "**Key choices:** `relevance_score >= 0.30`, require explicit ticker match, cache per symbol  \n",
        "**Output:** DataFrame with `published_at, source, title, summary, url, overall_sentiment`\n"
      ],
      "metadata": {
        "id": "iJxmkUXGsR3s"
      },
      "id": "iJxmkUXGsR3s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9bb3f1",
      "metadata": {
        "id": "fc9bb3f1"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch and cache symbol-specific news via Alpha Vantage, filtered by relevance\n",
        "# Context: called by downstream reporting/EDA to attach headlines and sentiment\n",
        "# Notes: filters to items where ticker matches and relevance >= 0.30; caches by symbol\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, requests, pandas as pd\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "\n",
        "def fetch_news(symbol: str) -> pd.DataFrame:\n",
        "    if not SETTINGS.alpha_api_key:\n",
        "        return pd.DataFrame()  # safe fail\n",
        "    cache_key = f\"news_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    params = {\"function\":\"NEWS_SENTIMENT\",\"tickers\":symbol,\"apikey\":SETTINGS.alpha_api_key}\n",
        "    r = requests.get(BASE, params=params, timeout=30)\n",
        "    data = r.json()\n",
        "    if \"feed\" not in data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for item in data.get(\"feed\", []):\n",
        "        tickers = item.get(\"ticker_sentiment\", []) or []\n",
        "        # keep only if our symbol is explicitly mentioned\n",
        "        keep = any(t.get(\"ticker\", \"\").upper() == symbol.upper() and float(t.get(\"relevance_score\", 0) or 0) >= 0.30\n",
        "                   for t in tickers)\n",
        "        if not keep:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"published_at\": item.get(\"time_published\"),\n",
        "            \"source\": item.get(\"source\"),\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"summary\": item.get(\"summary\"),\n",
        "            \"url\": item.get(\"url\"),\n",
        "            \"overall_sentiment\": item.get(\"overall_sentiment_label\")\n",
        "        })\n",
        "\n",
        "    # ====== Forth APPROACH =====\n",
        "    df = pd.DataFrame(rows)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc9c201",
      "metadata": {
        "id": "6cc9c201"
      },
      "source": [
        "## src/system/router.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1200bf3",
      "metadata": {
        "id": "f1200bf3"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "def choose_agents(has_news: bool, has_prices: bool, has_technicals: bool) -> list[str]:\n",
        "    agents = []\n",
        "    if has_news: agents.append(\"news\")\n",
        "    # earnings optional if you add a financials fetch later\n",
        "    if has_technicals and has_prices: agents.append(\"technical\")\n",
        "    agents.append(\"risk\")\n",
        "    return agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa0a509",
      "metadata": {
        "id": "afa0a509"
      },
      "source": [
        "## src/system/memory.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b260d5e1",
      "metadata": {
        "id": "b260d5e1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "MEM_PATH = SETTINGS.runs_dir / \"run_notes.jsonl\"\n",
        "\n",
        "def append_memory(record: dict[str, Any]) -> None:\n",
        "    MEM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with MEM_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cba7d18",
      "metadata": {
        "id": "2cba7d18"
      },
      "source": [
        "## src/agents.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdbc08b",
      "metadata": {
        "id": "9fdbc08b"
      },
      "outputs": [],
      "source": [
        "# === Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Import shared helpers from analysis.text\n",
        "from src.analysis.text import (\n",
        "    strip_code_fences,\n",
        "    to_float,\n",
        "    clamp,\n",
        "    normalize_score,\n",
        "    normalize_conf,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JI8FfLoKTHI"
      },
      "id": "5JI8FfLoKTHI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AGENT PROCESSING SYSTEM\n",
        "Multi-Agent Financial Analysis\n",
        "\"\"\"\n",
        "\n",
        "from openai import OpenAI\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass, asdict\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP API KEYS (Open AI keys before running)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Alpha Vantage\n",
        "os.environ[\"ALPHAVANTAGE_API_KEY\"] = \"BVGUKZR1MHVS0T6B\"\n",
        "\n",
        "# OpenAI - REPLACE WITH YOUR KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\"  # REPLACE BEFORE RUNNING\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "print(\"API keys configured\")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "722UdYSysmEh",
        "outputId": "1d4adcae-9bed-4179-e857-3d45ed5937db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys configured\n"
          ]
        }
      ],
      "id": "722UdYSysmEh"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA STRUCTURES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class AgentResponse:\n",
        "    \"\"\"\n",
        "    Standardized response format returned by all financial analysis agents.\n",
        "    Contains the agent's analysis text, numerical score, confidence level,\n",
        "    key factors driving the assessment, and performance metrics like execution\n",
        "    time and token usage. Ensures consistent output structure across all\n",
        "    agents for easy aggregation and synthesis \"\"\"\n",
        "\n",
        "    agent_name: str\n",
        "    analysis: str\n",
        "    score: float\n",
        "    confidence: float\n",
        "    key_factors: List[str]\n",
        "    timestamp: str\n",
        "    execution_time: float = 0.0  # NEW: Track execution time\n",
        "    token_usage: Optional[Dict[str, int]] = None  # NEW: Track token usage\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AgentMetrics:\n",
        "    \"\"\"\n",
        "    Performance tracking metrics for monitoring agent efficiency and reliability.\n",
        "    Records success/failure rates, execution times, and API token consumption\n",
        "    to identify bottlenecks and optimize system performance. Used for debugging,\n",
        "    cost analysis, and ensuring agents operate within acceptable performance bounds\"\"\"\n",
        "\n",
        "    agent_name: str\n",
        "    total_calls: int = 0\n",
        "    successful_calls: int = 0\n",
        "    failed_calls: int = 0\n",
        "    total_execution_time: float = 0.0\n",
        "    average_execution_time: float = 0.0\n",
        "    total_tokens_used: int = 0\n"
      ],
      "metadata": {
        "id": "HLW4qYK6v58Q"
      },
      "execution_count": 2,
      "outputs": [],
      "id": "HLW4qYK6v58Q"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# SHARED CONTEXT MANAGER\n",
        "# ============================================================================\n",
        "\n",
        "class SharedContext:\n",
        "    \"\"\"\n",
        "    Enables inter-agent communication by allowing agents to share insights,\n",
        "    findings, and intermediate results with each other during analysis.\n",
        "    Maintains both current context state and complete historical log of all\n",
        "    shared information with timestamps and agent attribution. Used by agents\n",
        "    to make more informed decisions based on collaborative intelligence from\n",
        "    other specialists in the system \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.context = {}\n",
        "        self.history = []\n",
        "\n",
        "    def add_insight(self, agent_name: str, key: str, value: Any):\n",
        "        \"\"\"Agent shares an insight\"\"\"\n",
        "        self.context[key] = {\n",
        "            'value': value,\n",
        "            'from_agent': agent_name,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        self.history.append({\n",
        "            'agent': agent_name,\n",
        "            'key': key,\n",
        "            'value': value,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "        logger.info(f\"{agent_name} shared insight: {key}\")\n",
        "\n",
        "    def get_insight(self, key: str) -> Optional[Any]:\n",
        "        \"\"\"Retrieve an insight\"\"\"\n",
        "        return self.context.get(key, {}).get('value')\n",
        "\n",
        "    def get_all_insights(self) -> Dict:\n",
        "        \"\"\"Get all shared insights\"\"\"\n",
        "        return self.context\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear all context\"\"\"\n",
        "        self.context = {}\n",
        "        self.history = []\n",
        "\n",
        "\n",
        "# Initialize shared context for all agents\n",
        "shared_context = SharedContext()\n"
      ],
      "metadata": {
        "id": "vXs8ruokveqN"
      },
      "execution_count": 3,
      "outputs": [],
      "id": "vXs8ruokveqN"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K0FSy9-tP1F1"
      },
      "id": "K0FSy9-tP1F1"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BASE AGENT\n",
        "# ============================================================================\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"base class for all financial agents with memory, retry, and metrics\"\"\"\n",
        "\n",
        "    def __init__(self, agent_name: str, model: str = \"gpt-4o-mini\"):\n",
        "        self.agent_name = agent_name\n",
        "        self.model = model\n",
        "        self.memory = []  # Conversation history\n",
        "        self.metrics = AgentMetrics(agent_name=agent_name)  # NEW: Performance metrics\n",
        "        self.shared_context = shared_context  # NEW: Shared context with other agents\n",
        "        logger.info(f\"Initialized {agent_name}\")\n",
        "\n",
        "    def call_llm(self, system_prompt: str, user_message: str, max_retries: int = 3) -> tuple:\n",
        "        \"\"\"\n",
        "        Call LLM with error handling, retry logic, and token tracking\n",
        "        Returns: (response_text, token_usage)\n",
        "        \"\"\"\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "\n",
        "                response = client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": user_message}\n",
        "                    ],\n",
        "                    temperature=0.3,\n",
        "                    max_tokens=800\n",
        "                )\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "\n",
        "                # Extract token usage\n",
        "                token_usage = {\n",
        "                    'prompt_tokens': response.usage.prompt_tokens,\n",
        "                    'completion_tokens': response.usage.completion_tokens,\n",
        "                    'total_tokens': response.usage.total_tokens\n",
        "                }\n",
        "\n",
        "                # Update metrics\n",
        "                self.metrics.total_tokens_used += token_usage['total_tokens']\n",
        "\n",
        "                logger.info(f\"{self.agent_name} LLM call completed in {elapsed:.2f}s, tokens: {token_usage['total_tokens']}\")\n",
        "\n",
        "                return response.choices[0].message.content, token_usage\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    logger.error(f\"{self.agent_name} failed after {max_retries} attempts: {str(e)}\")\n",
        "                    return f\"Error processing request: {str(e)}\", None\n",
        "\n",
        "                wait_time = 2 ** attempt  # Exponential backoff\n",
        "                logger.warning(f\"{self.agent_name} retry {attempt + 1}/{max_retries}, waiting {wait_time}s\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "    def add_to_memory(self, interaction: Dict):\n",
        "        \"\"\"Store conversation history\"\"\"\n",
        "        self.memory.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'input': interaction.get('input'),\n",
        "            'output': interaction.get('output'),\n",
        "            'metadata': interaction.get('metadata', {})\n",
        "        })\n",
        "        logger.debug(f\"{self.agent_name} added to memory (total: {len(self.memory)})\")\n",
        "\n",
        "    def get_context(self, last_n: int = 5) -> List[Dict]:\n",
        "        \"\"\"Retrieve recent context from memory\"\"\"\n",
        "        return self.memory[-last_n:] if self.memory else []\n",
        "\n",
        "    def share_insight(self, key: str, value: Any):\n",
        "        \"\"\"Share an insight with other agents via shared context\"\"\"\n",
        "        self.shared_context.add_insight(self.agent_name, key, value)\n",
        "\n",
        "    def get_shared_insights(self) -> Dict:\n",
        "        \"\"\"Get insights shared by other agents\"\"\"\n",
        "        return self.shared_context.get_all_insights()\n",
        "\n",
        "    def update_metrics(self, success: bool, execution_time: float):\n",
        "        \"\"\"Update agent performance metrics\"\"\"\n",
        "        self.metrics.total_calls += 1\n",
        "        if success:\n",
        "            self.metrics.successful_calls += 1\n",
        "        else:\n",
        "            self.metrics.failed_calls += 1\n",
        "\n",
        "        self.metrics.total_execution_time += execution_time\n",
        "        self.metrics.average_execution_time = (\n",
        "            self.metrics.total_execution_time / self.metrics.total_calls\n",
        "        )\n",
        "\n",
        "    def get_metrics(self) -> AgentMetrics:\n",
        "        \"\"\"Get agent performance metrics\"\"\"\n",
        "        return self.metrics\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        \"\"\"Override in each specialized agent\"\"\"\n",
        "        raise NotImplementedError(\"Each agent must implement process method\")\n"
      ],
      "metadata": {
        "id": "FmbDhBeawKiz"
      },
      "execution_count": 4,
      "outputs": [],
      "id": "FmbDhBeawKiz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NewsAnalysisAgent\n",
        "This agent reads news headlines about a stock, asks GPT-4 \"are these headlines good or bad news?\", gets back a sentiment score, and packages it in a standardized format for the orchestrator to use.\n",
        "For example: if someone whats get sentiment from new, this agent is  reading news and telling if it's bullish or bearish."
      ],
      "metadata": {
        "id": "7-IaXcGxP6Gg"
      },
      "id": "7-IaXcGxP6Gg"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SPECIALIZED AGENTS\n",
        "# ============================================================================\n",
        "\n",
        "class NewsAnalysisAgent(BaseAgent):\n",
        "    \"\"\"Analyzes financial news sentiment and impact\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"News Analysis Agent\")\n",
        "        self.system_prompt = \"\"\"You are a financial news analyst specializing in sentiment analysis.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze news articles objectively\n",
        "2. Consider both positive and negative aspects\n",
        "3. Provide a sentiment score from -1 (very negative) to +1 (very positive)\n",
        "4. Identify key factors driving the sentiment\n",
        "5. Assess potential stock price impact\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"sentiment_score\": 0.75,\n",
        "  \"analysis\": \"Strong positive sentiment driven by earnings beat and product launch\",\n",
        "  \"key_factors\": [\"Earnings exceeded expectations\", \"New product well-received\"],\n",
        "  \"confidence\": 0.85\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: sentiment_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        \"\"\"Process news data for sentiment analysis\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            ticker = data.get('ticker', 'AAPL')\n",
        "            news_articles = data.get('news', [])\n",
        "\n",
        "            # Data validation\n",
        "            if not news_articles:\n",
        "                logger.warning(f\"{self.agent_name}: No news data available\")\n",
        "                return AgentResponse(\n",
        "                    agent_name=self.agent_name,\n",
        "                    analysis=\"No news data available for analysis\",\n",
        "                    score=0.0,\n",
        "                    confidence=0.0,\n",
        "                    key_factors=[\"No news data available\"],\n",
        "                    timestamp=datetime.now().isoformat(),\n",
        "                    execution_time=time.time() - start_time\n",
        "                )\n",
        "\n",
        "            # Prepare news summary\n",
        "            news_summary = \"\\n\".join([\n",
        "                f\"- {article.get('title', '')}: {article.get('summary', '')}\"\n",
        "                for article in news_articles[:5]\n",
        "            ])\n",
        "\n",
        "            user_message = f\"\"\"Analyze the following recent news about {ticker}:\n",
        "\n",
        "{news_summary}\n",
        "\n",
        "Provide sentiment analysis and impact assessment.\"\"\"\n",
        "\n",
        "            # Call LLM\n",
        "            llm_response, token_usage = self.call_llm(self.system_prompt, user_message)\n",
        "\n",
        "            # Parse response\n",
        "            try:\n",
        "                result = json.loads(llm_response)\n",
        "                score = result.get('sentiment_score', 0)\n",
        "                analysis = result.get('analysis', llm_response)\n",
        "                key_factors = result.get('key_factors', [])\n",
        "\n",
        "                confidence_raw = result.get('confidence', 0.7)\n",
        "                try:\n",
        "                    confidence = max(0.0, min(1.0, float(confidence_raw)))\n",
        "                except (ValueError, TypeError):\n",
        "                    confidence = 0.7\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                logger.warning(f\"{self.agent_name}: Failed to parse JSON response\")\n",
        "                score = 0\n",
        "                analysis = llm_response\n",
        "                key_factors = [\"Unable to parse structured response\"]\n",
        "                confidence = 0.5\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Add to memory\n",
        "            self.add_to_memory({\n",
        "                'input': {'ticker': ticker, 'news_count': len(news_articles)},\n",
        "                'output': analysis,\n",
        "                'metadata': {'score': score, 'confidence': confidence}\n",
        "            })\n",
        "\n",
        "            # Share insight with other agents\n",
        "            self.share_insight('news_sentiment', score)\n",
        "            self.share_insight('news_confidence', confidence)\n",
        "\n",
        "            # Update metrics\n",
        "            self.update_metrics(success=True, execution_time=execution_time)\n",
        "\n",
        "            return AgentResponse(\n",
        "                agent_name=self.agent_name,\n",
        "                analysis=analysis,\n",
        "                score=float(score),\n",
        "                confidence=float(confidence),\n",
        "                key_factors=key_factors,\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                execution_time=execution_time,\n",
        "                token_usage=token_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.update_metrics(success=False, execution_time=execution_time)\n",
        "            logger.error(f\"{self.agent_name} error: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "hlk2ZBiawl74"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "hlk2ZBiawl74"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EarningsAnalysisAgent\n",
        "The Earnings Analysis Agent examines a company's financial performance (revenue, profits, earnings per share) and determines if the fundamentals are strong by comparing actual results against analyst expectations. It sends this financial data to GPT-4, which returns a score (-1 to +1) indicating whether the company's financials suggest it's a good or weak investment."
      ],
      "metadata": {
        "id": "qUUFBsLPP85m"
      },
      "id": "qUUFBsLPP85m"
    },
    {
      "cell_type": "code",
      "source": [
        "class EarningsAnalysisAgent(BaseAgent):\n",
        "    \"\"\"Analyzes earnings reports and financial statements\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Earnings Analysis Agent\")\n",
        "        self.system_prompt = \"\"\"You are a financial analyst specializing in earnings and fundamental analysis.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze company financial metrics objectively\n",
        "2. Compare actuals vs expectations\n",
        "3. Assess fundamental strength from -1 (very weak) to +1 (very strong)\n",
        "4. Identify key financial drivers\n",
        "5. Evaluate growth trends\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"fundamental_score\": 0.80,\n",
        "  \"analysis\": \"Strong fundamentals with revenue and EPS beating expectations\",\n",
        "  \"key_factors\": [\"Revenue beat by 5%\", \"EPS exceeded estimates\", \"Margin expansion\"],\n",
        "  \"confidence\": 0.88\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: fundamental_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        \"\"\"Process earnings and financial data\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            ticker = data.get('ticker', 'UNKNOWN')\n",
        "            financials = data.get('financials', {})\n",
        "\n",
        "            # Check for shared insights from other agents\n",
        "            news_sentiment = self.shared_context.get_insight('news_sentiment')\n",
        "            if news_sentiment:\n",
        "                logger.info(f\"{self.agent_name}: Considering news sentiment = {news_sentiment}\")\n",
        "\n",
        "            financial_summary = f\"\"\"\n",
        "Company: {ticker}\n",
        "Revenue: ${financials.get('revenue', 'N/A')}B\n",
        "EPS: ${financials.get('eps', 'N/A')}\n",
        "Revenue Growth: {financials.get('revenue_growth', 'N/A')}%\n",
        "Profit Margin: {financials.get('profit_margin', 'N/A')}%\n",
        "Expected Revenue: ${financials.get('expected_revenue', 'N/A')}B\n",
        "Expected EPS: ${financials.get('expected_eps', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "            user_message = f\"\"\"Analyze the following financial data for {ticker}:\n",
        "\n",
        "{financial_summary}\n",
        "\n",
        "Assess fundamental strength and growth prospects.\"\"\"\n",
        "\n",
        "            llm_response, token_usage = self.call_llm(self.system_prompt, user_message)\n",
        "\n",
        "            try:\n",
        "                result = json.loads(llm_response)\n",
        "                score = result.get('fundamental_score', 0)\n",
        "                analysis = result.get('analysis', llm_response)\n",
        "                key_factors = result.get('key_factors', [])\n",
        "\n",
        "                confidence_raw = result.get('confidence', 0.8)\n",
        "                try:\n",
        "                    confidence = max(0.0, min(1.0, float(confidence_raw)))\n",
        "                except (ValueError, TypeError):\n",
        "                    confidence = 0.8\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                logger.warning(f\"{self.agent_name}: Failed to parse JSON response\")\n",
        "                score = 0\n",
        "                analysis = llm_response\n",
        "                key_factors = [\"Unable to parse structured response\"]\n",
        "                confidence = 0.6\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            self.add_to_memory({\n",
        "                'input': {'ticker': ticker, 'financials': financials},\n",
        "                'output': analysis,\n",
        "                'metadata': {'score': score, 'confidence': confidence}\n",
        "            })\n",
        "\n",
        "            self.share_insight('fundamental_score', score)\n",
        "            self.share_insight('fundamental_confidence', confidence)\n",
        "\n",
        "            self.update_metrics(success=True, execution_time=execution_time)\n",
        "\n",
        "            return AgentResponse(\n",
        "                agent_name=self.agent_name,\n",
        "                analysis=analysis,\n",
        "                score=float(score),\n",
        "                confidence=float(confidence),\n",
        "                key_factors=key_factors,\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                execution_time=execution_time,\n",
        "                token_usage=token_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.update_metrics(success=False, execution_time=execution_time)\n",
        "            logger.error(f\"{self.agent_name} error: {e}\")\n",
        "            raise\n",
        "\n"
      ],
      "metadata": {
        "id": "O1lJHsXAwnPa"
      },
      "execution_count": 6,
      "outputs": [],
      "id": "O1lJHsXAwnPa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MarketSignalsAgent\n",
        "The MarketSignalsAgent performs technical analysis by examining stock price patterns, trading volume, and technical indicators (like moving averages, RSI, MACD) to identify trends and momentum. It sends this technical data to GPT-4, which returns a score (-1 to +1) indicating whether the stock's price action suggests a bullish or bearish trend based on chart patterns and trading signals."
      ],
      "metadata": {
        "id": "ZSx3bxLBQEWJ"
      },
      "id": "ZSx3bxLBQEWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "class MarketSignalsAgent(BaseAgent):\n",
        "    \"\"\"Performs technical analysis on market data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Market Signals Agent\")\n",
        "        self.system_prompt = \"\"\"You are a technical analyst specializing in market signals and price patterns.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze technical indicators objectively\n",
        "2. Assess technical strength from -1 (very bearish) to +1 (very bullish)\n",
        "3. Identify support/resistance levels\n",
        "4. Evaluate trend direction and momentum\n",
        "5. Consider volume patterns\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"technical_score\": 0.65,\n",
        "  \"analysis\": \"Bullish technical setup with price above key moving averages\",\n",
        "  \"key_factors\": [\"Price above 50-day MA\", \"RSI indicates strength\", \"Volume confirming uptrend\"],\n",
        "  \"confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: technical_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        \"\"\"Process technical market data\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            ticker = data.get('ticker', 'UNKNOWN')\n",
        "            technicals = data.get('technicals', {})\n",
        "\n",
        "            # Validate data\n",
        "            if not technicals.get('current_price') or technicals.get('current_price') == 'N/A':\n",
        "                logger.warning(f\"{self.agent_name}: Insufficient technical data\")\n",
        "                return AgentResponse(\n",
        "                    agent_name=self.agent_name,\n",
        "                    analysis=\"Insufficient technical data for analysis\",\n",
        "                    score=0.0,\n",
        "                    confidence=0.0,\n",
        "                    key_factors=[\"Missing price data\"],\n",
        "                    timestamp=datetime.now().isoformat(),\n",
        "                    execution_time=time.time() - start_time\n",
        "                )\n",
        "\n",
        "            technical_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Current Price: ${technicals.get('current_price', 'N/A')}\n",
        "50-day MA: ${technicals.get('ma_50', 'N/A')}\n",
        "200-day MA: ${technicals.get('ma_200', 'N/A')}\n",
        "RSI: {technicals.get('rsi', 'N/A')}\n",
        "MACD: {technicals.get('macd', 'N/A')}\n",
        "Volume: {technicals.get('volume', 'N/A')} (Avg: {technicals.get('avg_volume', 'N/A')})\n",
        "Support: ${technicals.get('support', 'N/A')}\n",
        "Resistance: ${technicals.get('resistance', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "            user_message = f\"\"\"Analyze the following technical data for {ticker}:\n",
        "\n",
        "{technical_summary}\n",
        "\n",
        "Assess technical strength and price momentum.\"\"\"\n",
        "\n",
        "            llm_response, token_usage = self.call_llm(self.system_prompt, user_message)\n",
        "\n",
        "            try:\n",
        "                result = json.loads(llm_response)\n",
        "                score = result.get('technical_score', 0)\n",
        "                analysis = result.get('analysis', llm_response)\n",
        "                key_factors = result.get('key_factors', [])\n",
        "\n",
        "                confidence_raw = result.get('confidence', 0.7)\n",
        "                try:\n",
        "                    confidence = max(0.0, min(1.0, float(confidence_raw)))\n",
        "                except (ValueError, TypeError):\n",
        "                    confidence = 0.7\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                logger.warning(f\"{self.agent_name}: Failed to parse JSON response\")\n",
        "                score = 0\n",
        "                analysis = llm_response\n",
        "                key_factors = [\"Unable to parse structured response\"]\n",
        "                confidence = 0.5\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            self.add_to_memory({\n",
        "                'input': {'ticker': ticker, 'technicals': technicals},\n",
        "                'output': analysis,\n",
        "                'metadata': {'score': score, 'confidence': confidence}\n",
        "            })\n",
        "\n",
        "            self.share_insight('technical_score', score)\n",
        "            self.share_insight('technical_confidence', confidence)\n",
        "\n",
        "            self.update_metrics(success=True, execution_time=execution_time)\n",
        "\n",
        "            return AgentResponse(\n",
        "                agent_name=self.agent_name,\n",
        "                analysis=analysis,\n",
        "                score=float(score),\n",
        "                confidence=float(confidence),\n",
        "                key_factors=key_factors,\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                execution_time=execution_time,\n",
        "                token_usage=token_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.update_metrics(success=False, execution_time=execution_time)\n",
        "            logger.error(f\"{self.agent_name} error: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "IkhL7zL7xHmY"
      },
      "execution_count": 7,
      "outputs": [],
      "id": "IkhL7zL7xHmY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RiskAssessmentAgent\n",
        "The RiskAssessmentAgent evaluates investment risk by analyzing metrics like beta (volatility), Value at Risk, Sharpe ratio, and sector correlation to determine how risky a stock is for a portfolio. It sends these risk metrics to GPT-4, which returns a risk score (0 to 1, where 0 is low risk and 1 is high risk) along with warnings about potential portfolio concentration or volatility issues."
      ],
      "metadata": {
        "id": "NgYMJyQeQJXk"
      },
      "id": "NgYMJyQeQJXk"
    },
    {
      "cell_type": "code",
      "source": [
        "class RiskAssessmentAgent(BaseAgent):\n",
        "    \"\"\"Assesses investment risk and portfolio fit\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Risk Assessment Agent\")\n",
        "        self.system_prompt = \"\"\"You are a risk management analyst specializing in portfolio risk assessment.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze risk metrics objectively\n",
        "2. Provide risk level score from 0 (very low risk) to 1 (very high risk)\n",
        "3. Identify key risk factors\n",
        "4. Assess portfolio diversification implications\n",
        "5. Evaluate risk-adjusted returns\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"risk_score\": 0.45,\n",
        "  \"analysis\": \"Moderate risk profile with acceptable volatility and strong Sharpe ratio\",\n",
        "  \"key_factors\": [\"Beta of 1.15 indicates moderate volatility\", \"Strong Sharpe ratio\", \"Manageable drawdown\"],\n",
        "  \"confidence\": 0.82\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: risk_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        \"\"\"Process risk metrics\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            ticker = data.get('ticker', 'UNKNOWN')\n",
        "            risk_data = data.get('risk_metrics', {})\n",
        "\n",
        "            risk_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Beta: {risk_data.get('beta', 'N/A')}\n",
        "Volatility (30-day): {risk_data.get('volatility', 'N/A')}%\n",
        "Value at Risk (5%): ${risk_data.get('var_5', 'N/A')}\n",
        "Sharpe Ratio: {risk_data.get('sharpe_ratio', 'N/A')}\n",
        "Max Drawdown: {risk_data.get('max_drawdown', 'N/A')}%\n",
        "Sector Correlation: {risk_data.get('sector_correlation', 'N/A')}\n",
        "P/E Ratio: {risk_data.get('pe_ratio', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "            user_message = f\"\"\"Analyze the following risk metrics for {ticker}:\n",
        "\n",
        "{risk_summary}\n",
        "\n",
        "Assess overall investment risk and portfolio implications.\"\"\"\n",
        "\n",
        "            llm_response, token_usage = self.call_llm(self.system_prompt, user_message)\n",
        "\n",
        "            try:\n",
        "                result = json.loads(llm_response)\n",
        "                score = result.get('risk_score', 0.5)\n",
        "                analysis = result.get('analysis', llm_response)\n",
        "                key_factors = result.get('key_factors', [])\n",
        "\n",
        "                confidence_raw = result.get('confidence', 0.8)\n",
        "                try:\n",
        "                    confidence = max(0.0, min(1.0, float(confidence_raw)))\n",
        "                except (ValueError, TypeError):\n",
        "                    confidence = 0.8\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                logger.warning(f\"{self.agent_name}: Failed to parse JSON response\")\n",
        "                score = 0.5\n",
        "                analysis = llm_response\n",
        "                key_factors = [\"Unable to parse structured response\"]\n",
        "                confidence = 0.6\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            self.add_to_memory({\n",
        "                'input': {'ticker': ticker, 'risk_metrics': risk_data},\n",
        "                'output': analysis,\n",
        "                'metadata': {'score': score, 'confidence': confidence}\n",
        "            })\n",
        "\n",
        "            self.share_insight('risk_score', score)\n",
        "            self.share_insight('risk_confidence', confidence)\n",
        "\n",
        "            self.update_metrics(success=True, execution_time=execution_time)\n",
        "\n",
        "            return AgentResponse(\n",
        "                agent_name=self.agent_name,\n",
        "                analysis=analysis,\n",
        "                score=float(score),\n",
        "                confidence=float(confidence),\n",
        "                key_factors=key_factors,\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                execution_time=execution_time,\n",
        "                token_usage=token_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.update_metrics(success=False, execution_time=execution_time)\n",
        "            logger.error(f\"{self.agent_name} error: {e}\")\n",
        "            raise\n",
        "\n"
      ],
      "metadata": {
        "id": "cmzoIPtGxJ6G"
      },
      "execution_count": 8,
      "outputs": [],
      "id": "cmzoIPtGxJ6G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SynthesisAgent\n",
        "The SynthesisAgent acts as the \"decision maker\" that takes all the individual agent analyses (news sentiment, earnings strength, technical signals, risk level) and combines them into a single investment recommendation (STRONG BUY, BUY, HOLD, SELL, STRONG SELL). It sends a summary of all agent scores and findings to GPT-4, which weighs the different perspectives and returns a final actionable recommendation with confidence level and supporting reasoning."
      ],
      "metadata": {
        "id": "_dZqeC6vQRqp"
      },
      "id": "_dZqeC6vQRqp"
    },
    {
      "cell_type": "code",
      "source": [
        "class SynthesisAgent(BaseAgent):\n",
        "    \"\"\"Combines insights from all agents into final recommendation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Research Synthesis Agent\")\n",
        "        self.system_prompt = \"\"\"You are a senior investment analyst who synthesizes multiple analyses into actionable recommendations.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review all agent analyses objectively\n",
        "2. Weigh different factors appropriately\n",
        "3. Provide clear investment recommendation (STRONG BUY, BUY, HOLD, SELL, STRONG SELL)\n",
        "4. State confidence level (0 to 1)\n",
        "5. Summarize key reasoning\n",
        "6. Note important risks\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"recommendation\": \"BUY\",\n",
        "  \"confidence\": 0.78,\n",
        "  \"analysis\": \"Strong fundamentals and positive technical signals support a buy recommendation despite moderate risk\",\n",
        "  \"key_points\": [\"Earnings beat expectations\", \"Technical breakout\", \"Acceptable risk profile\"],\n",
        "  \"risks\": [\"Market volatility\", \"Sector headwinds\"]\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: recommendation, confidence, analysis, key_points, risks\"\"\"\n",
        "\n",
        "    def process(self, agent_responses: List[AgentResponse]) -> AgentResponse:\n",
        "        \"\"\"Synthesize all agent responses\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Check shared insights\n",
        "            all_insights = self.get_shared_insights()\n",
        "            logger.info(f\"{self.agent_name}: Reviewing {len(all_insights)} shared insights\")\n",
        "\n",
        "            analyses_summary = \"\\n\\n\".join([\n",
        "                f\"{resp.agent_name}:\\n\"\n",
        "                f\"Score: {resp.score}\\n\"\n",
        "                f\"Confidence: {resp.confidence}\\n\"\n",
        "                f\"Analysis: {resp.analysis}\\n\"\n",
        "                f\"Key Factors: {', '.join(resp.key_factors)}\"\n",
        "                for resp in agent_responses\n",
        "            ])\n",
        "\n",
        "            user_message = f\"\"\"Synthesize the following analyses into a final investment recommendation:\n",
        "\n",
        "{analyses_summary}\n",
        "\n",
        "Provide comprehensive investment recommendation with supporting reasoning.\"\"\"\n",
        "\n",
        "            llm_response, token_usage = self.call_llm(self.system_prompt, user_message)\n",
        "\n",
        "            try:\n",
        "                result = json.loads(llm_response)\n",
        "                recommendation = result.get('recommendation', 'HOLD')\n",
        "                analysis = result.get('analysis', llm_response)\n",
        "                key_factors = result.get('key_points', [])\n",
        "\n",
        "                confidence_raw = result.get('confidence', 0.7)\n",
        "                try:\n",
        "                    confidence = max(0.0, min(1.0, float(confidence_raw)))\n",
        "                except (ValueError, TypeError):\n",
        "                    confidence = 0.7\n",
        "\n",
        "                rec_to_score = {\n",
        "                    'STRONG BUY': 1.0,\n",
        "                    'BUY': 0.6,\n",
        "                    'HOLD': 0.0,\n",
        "                    'SELL': -0.6,\n",
        "                    'STRONG SELL': -1.0\n",
        "                }\n",
        "                score = rec_to_score.get(recommendation, 0.0)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                logger.warning(f\"{self.agent_name}: Failed to parse JSON response\")\n",
        "                score = 0\n",
        "                analysis = llm_response\n",
        "                key_factors = [\"Unable to parse structured response\"]\n",
        "                confidence = 0.6\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            self.add_to_memory({\n",
        "                'input': {'num_agents': len(agent_responses)},\n",
        "                'output': analysis,\n",
        "                'metadata': {'recommendation': recommendation, 'confidence': confidence}\n",
        "            })\n",
        "\n",
        "            self.share_insight('final_recommendation', recommendation)\n",
        "            self.share_insight('final_score', score)\n",
        "\n",
        "            self.update_metrics(success=True, execution_time=execution_time)\n",
        "\n",
        "            return AgentResponse(\n",
        "                agent_name=self.agent_name,\n",
        "                analysis=analysis,\n",
        "                score=float(score),\n",
        "                confidence=float(confidence),\n",
        "                key_factors=key_factors,\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                execution_time=execution_time,\n",
        "                token_usage=token_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.update_metrics(success=False, execution_time=execution_time)\n",
        "            logger.error(f\"{self.agent_name} error: {e}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "rm3w15nWxYqT"
      },
      "execution_count": 9,
      "outputs": [],
      "id": "rm3w15nWxYqT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CritiqueAgent\n",
        "\n",
        "The CritiqueAgent acts as a \"quality control checker\" that reviews the final investment recommendation to catch mistakes, biases, or missing information before presenting it to the user. It examines the SynthesisAgent's recommendation, asks GPT-4 to identify logical flaws or gaps in reasoning, and can adjust the confidence level downward if it finds issues (like  \"didn't consider macroeconomic factors\"), ensuring the final output is reliable and well-reasoned."
      ],
      "metadata": {
        "id": "ZtC3_CEpQa9O"
      },
      "id": "ZtC3_CEpQa9O"
    },
    {
      "cell_type": "code",
      "source": [
        "class CritiqueAgent(BaseAgent):\n",
        "    \"\"\"Reviews and validates analysis quality\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Critique & Validation Agent\")\n",
        "        self.system_prompt = \"\"\"You are a critique analyst who reviews investment recommendations for biases, logical errors, and completeness.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the synthesis objectively\n",
        "2. Identify logical inconsistencies\n",
        "3. Detect potential biases\n",
        "4. Note missing considerations\n",
        "5. Assess data quality\n",
        "6. Recommend confidence adjustments\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"quality_score\": 0.82,\n",
        "  \"issues_found\": [\"Limited macroeconomic analysis\"],\n",
        "  \"suggestions\": [\"Consider Federal Reserve policy impact\", \"Add sector comparison\"],\n",
        "  \"adjusted_confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: quality_score, issues_found, suggestions, adjusted_confidence\"\"\"\n",
        "\n",
        "    def process(self, synthesis_response: AgentResponse) -> AgentResponse:\n",
        "        \"\"\"Critique the synthesis\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            user_message = f\"\"\"Review this investment analysis for quality and completeness:\n",
        "\n",
        "Recommendation: {synthesis_response.analysis}\n",
        "Confidence: {synthesis_response.confidence}\n",
        "Key Factors: {', '.join(synthesis_response.key_factors)}\n",
        "\n",
        "Identify any issues, biases, or missing elements.\"\"\"\n",
        "\n",
        "            llm_response, token_usage = self.call_llm(self.system_prompt, user_message)\n",
        "\n",
        "            try:\n",
        "                result = json.loads(llm_response)\n",
        "                quality_score = result.get('quality_score', 0.7)\n",
        "                issues = result.get('issues_found', [])\n",
        "                suggestions = result.get('suggestions', [])\n",
        "                adjusted_confidence_raw = result.get('adjusted_confidence', synthesis_response.confidence)\n",
        "\n",
        "                try:\n",
        "                    adjusted_confidence = max(0.0, min(1.0, float(adjusted_confidence_raw)))\n",
        "                except (ValueError, TypeError):\n",
        "                    adjusted_confidence = synthesis_response.confidence\n",
        "\n",
        "                analysis = f\"Quality Score: {quality_score}\\n\"\n",
        "                if issues:\n",
        "                    analysis += f\"Issues Found: {', '.join(issues)}\\n\"\n",
        "                if suggestions:\n",
        "                    analysis += f\"Suggestions: {', '.join(suggestions)}\"\n",
        "\n",
        "                key_factors = issues if issues else [\"No major issues found\"]\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                logger.warning(f\"{self.agent_name}: Failed to parse JSON response\")\n",
        "                quality_score = 0.7\n",
        "                analysis = llm_response\n",
        "                adjusted_confidence = synthesis_response.confidence\n",
        "                key_factors = [\"Unable to parse structured response\"]\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            self.add_to_memory({\n",
        "                'input': {'synthesis_confidence': synthesis_response.confidence},\n",
        "                'output': analysis,\n",
        "                'metadata': {'quality_score': quality_score, 'adjusted_confidence': adjusted_confidence}\n",
        "            })\n",
        "\n",
        "            self.share_insight('quality_score', quality_score)\n",
        "            self.share_insight('adjusted_confidence', adjusted_confidence)\n",
        "\n",
        "            self.update_metrics(success=True, execution_time=execution_time)\n",
        "\n",
        "            return AgentResponse(\n",
        "                agent_name=self.agent_name,\n",
        "                analysis=analysis,\n",
        "                score=float(quality_score),\n",
        "                confidence=float(adjusted_confidence),\n",
        "                key_factors=key_factors,\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                execution_time=execution_time,\n",
        "                token_usage=token_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.update_metrics(success=False, execution_time=execution_time)\n",
        "            logger.error(f\"{self.agent_name} error: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "FRwtWWV-xqyu"
      },
      "execution_count": 10,
      "outputs": [],
      "id": "FRwtWWV-xqyu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def print_agent_metrics():\n",
        "    \"\"\"Print performance metrics for all agents\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"AGENT PERFORMANCE METRICS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    agents = [\n",
        "        NewsAnalysisAgent(),\n",
        "        EarningsAnalysisAgent(),\n",
        "        MarketSignalsAgent(),\n",
        "        RiskAssessmentAgent(),\n",
        "        SynthesisAgent(),\n",
        "        CritiqueAgent()\n",
        "    ]\n",
        "\n",
        "    for agent in agents:\n",
        "        metrics = agent.get_metrics()\n",
        "        print(f\"\\n{metrics.agent_name}:\")\n",
        "        print(f\"  Total Calls: {metrics.total_calls}\")\n",
        "        print(f\"  Success Rate: {metrics.successful_calls}/{metrics.total_calls}\")\n",
        "        print(f\"  Avg Execution Time: {metrics.average_execution_time:.2f}s\")\n",
        "        print(f\"  Total Tokens Used: {metrics.total_tokens_used}\")\n",
        "\n",
        "\n",
        "def print_shared_insights():\n",
        "    \"\"\"Print all shared insights between agents\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SHARED INSIGHTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    insights = shared_context.get_all_insights()\n",
        "    for key, data in insights.items():\n",
        "        print(f\"\\n{key}: {data['value']}\")\n",
        "        print(f\"  From: {data['from_agent']}\")\n",
        "        print(f\"  Time: {data['timestamp']}\")"
      ],
      "metadata": {
        "id": "PwVXOoCOx3s7"
      },
      "execution_count": 11,
      "outputs": [],
      "id": "PwVXOoCOx3s7"
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"notebooks/01_data_ingestion.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1J_syHGmJHhA8XcAtxw7nQ0KPWMJbnSkd\n",
        "\"\"\"\n",
        "\n",
        "#import os\n",
        "os.environ[\"ALPHAVANTAGE_API_KEY\"] = \"BVGUKZR1MHVS0T6B\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datasets import Dataset\n",
        "\n",
        "# ------------------------------\n",
        "# Helper: Convert Pandas → Hugging Face Dataset\n",
        "# ------------------------------\n",
        "def to_hf(df, schema=None):\n",
        "    \"\"\"Convert a pandas DataFrame to a Hugging Face Dataset. Handles empty gracefully.\"\"\"\n",
        "    if df is None or getattr(df, \"empty\", True):\n",
        "        if schema:\n",
        "            return Dataset.from_dict({c: [] for c in schema})\n",
        "        return Dataset.from_dict({})\n",
        "    if schema:\n",
        "        df = df[[c for c in schema if c in df.columns]].copy()\n",
        "    return Dataset.from_pandas(df.reset_index(drop=True), preserve_index=False)\n",
        "\n",
        "# ------------------------------\n",
        "# Alpha Vantage Connector (for news + indicators only)\n",
        "# ------------------------------\n",
        "class AlphaConnector:\n",
        "    def __init__(self, api_key=None):\n",
        "        # Pick up API key from os.environ if not passed directly\n",
        "        self.api_key = api_key or os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"Alpha Vantage API key not found. Set os.environ['ALPHAVANTAGE_API_KEY'].\")\n",
        "\n",
        "        self.base_url = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "    def fetch_news(self, symbol):\n",
        "        \"\"\"Fetch company news & sentiment (Alpha Vantage).\"\"\"\n",
        "        params = {\n",
        "            \"function\": \"NEWS_SENTIMENT\",\n",
        "            \"tickers\": symbol,\n",
        "            \"apikey\": self.api_key\n",
        "        }\n",
        "        r = requests.get(self.base_url, params=params)\n",
        "        data = r.json()\n",
        "\n",
        "        if \"feed\" not in data:\n",
        "            print(\"No news data:\", data)\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        rows = []\n",
        "        for item in data[\"feed\"]:\n",
        "            rows.append({\n",
        "                \"published_at\": item.get(\"time_published\"),\n",
        "                \"source\": item.get(\"source\"),\n",
        "                \"title\": item.get(\"title\"),\n",
        "                \"summary\": item.get(\"summary\"),\n",
        "                \"url\": item.get(\"url\"),\n",
        "                \"overall_sentiment\": item.get(\"overall_sentiment_label\"),\n",
        "                # ** Added By Ali **\n",
        "                \"overall_sentiment_score\": item.get(\"overall_sentiment_score\") # both label and score so later agents (NewsAnalysisAgent, SynthesisAgent, etc.) can use either\n",
        "            })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def fetch_indicator(self, symbol, indicator, interval=\"daily\", time_period=14, series_type=\"close\"):\n",
        "        \"\"\"Generic technical indicator fetch (SMA, RSI, MACD).\"\"\"\n",
        "        params = {\n",
        "            \"function\": indicator,\n",
        "            \"symbol\": symbol,\n",
        "            \"interval\": interval,\n",
        "            \"time_period\": time_period,\n",
        "            \"series_type\": series_type,\n",
        "            \"apikey\": self.api_key\n",
        "        }\n",
        "        r = requests.get(self.base_url, params=params)\n",
        "        data = r.json()\n",
        "\n",
        "        key_map = {\n",
        "            \"SMA\": \"Technical Analysis: SMA\",\n",
        "            \"RSI\": \"Technical Analysis: RSI\",\n",
        "            \"MACD\": \"Technical Analysis: MACD\"\n",
        "        }\n",
        "        key = key_map.get(indicator)\n",
        "        if key not in data:\n",
        "            print(f\"{indicator} fetch failed:\", data)\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df.reset_index(inplace=True)\n",
        "        df = df.rename(columns={\"index\": \"date\"})\n",
        "\n",
        "        # Cast numeric values\n",
        "        for col in df.columns:\n",
        "            if col != \"date\":\n",
        "                df[col] = df[col].astype(float)\n",
        "\n",
        "        return df\n",
        "\n",
        "# ------------------------------\n",
        "# Data Ingestion Manager\n",
        "# ------------------------------\n",
        "class DataIngestionManager:\n",
        "    def __init__(self, api_key=None):\n",
        "        self.alpha = AlphaConnector(api_key)\n",
        "\n",
        "    def fetch_all(self, symbol, start=None, end=None):\n",
        "        \"\"\"Fetch prices (Yahoo), news (Alpha Vantage), SMA, RSI (Alpha Vantage).\"\"\"\n",
        "        datasets = {}\n",
        "\n",
        "        # Prices from Yahoo Finance (unlimited)\n",
        "        try:\n",
        "            df_prices = yf.download(symbol, start=start, end=end, progress=False)\n",
        "\n",
        "            # Flatten MultiIndex columns if necessary\n",
        "            if isinstance(df_prices.columns, pd.MultiIndex):\n",
        "                df_prices.columns = [c[0].lower() for c in df_prices.columns]\n",
        "\n",
        "            df_prices = df_prices.reset_index().rename(columns={\n",
        "                \"Date\": \"date\",\n",
        "                \"open\": \"open\",\n",
        "                \"high\": \"high\",\n",
        "                \"low\": \"low\",\n",
        "                \"close\": \"close\",\n",
        "                \"adj close\": \"adj_close\",\n",
        "                \"volume\": \"volume\"\n",
        "            })\n",
        "            df_prices[\"date\"] = df_prices[\"date\"].astype(str)\n",
        "\n",
        "            datasets[\"prices\"] = to_hf(\n",
        "                df_prices, schema=[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(\"Yahoo Finance fetch failed:\", e)\n",
        "            datasets[\"prices\"] = to_hf(pd.DataFrame(), schema=[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"])\n",
        "\n",
        "        # News from Alpha Vantage\n",
        "        datasets[\"news\"] = to_hf(\n",
        "            self.alpha.fetch_news(symbol),\n",
        "            schema=[\"published_at\",\"source\",\"title\",\"summary\",\"url\",\"overall_sentiment\"]\n",
        "        )\n",
        "\n",
        "        # Technical Indicators from Alpha Vantage\n",
        "        datasets[\"sma\"] = to_hf(\n",
        "            self.alpha.fetch_indicator(symbol, \"SMA\", time_period=20),\n",
        "            schema=[\"date\",\"SMA\"]\n",
        "        )\n",
        "        datasets[\"rsi\"] = to_hf(\n",
        "            self.alpha.fetch_indicator(symbol, \"RSI\", time_period=14),\n",
        "            schema=[\"date\",\"RSI\"]\n",
        "        )\n",
        "\n",
        "        # Removed MACD to avoid premium-only error\n",
        "        return datasets\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "mgr = DataIngestionManager()  # will pick up the key from os.environ\n",
        "symbol = \"AAPL\"\n",
        "start = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "end   = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "datasets = mgr.fetch_all(symbol, start, end)\n",
        "\n",
        "print(\"Prices sample:\")\n",
        "print(datasets[\"prices\"].to_pandas().head())\n",
        "\n",
        "print(\"News sample:\")\n",
        "print(datasets[\"news\"].to_pandas().head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvXsw-Yb3VUn",
        "outputId": "2aacb3d9-74c0-4265-80b7-546a2c062c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4032339001.py:120: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df_prices = yf.download(symbol, start=start, end=end, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No news data: {'Information': 'We have detected your API key as BVGUKZR1MHVS0T6B and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "SMA fetch failed: {'Information': 'We have detected your API key as BVGUKZR1MHVS0T6B and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "RSI fetch failed: {'Information': 'We have detected your API key as BVGUKZR1MHVS0T6B and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "Prices sample:\n",
            "         date        open        high         low       close     volume\n",
            "0  2025-09-18  239.970001  241.199997  236.649994  237.880005   44249600\n",
            "1  2025-09-19  241.229996  246.300003  240.210007  245.500000  163741300\n",
            "2  2025-09-22  248.300003  256.640015  248.119995  256.079987  105517400\n",
            "3  2025-09-23  255.880005  257.339996  253.580002  254.429993   60275200\n",
            "4  2025-09-24  255.220001  255.740005  251.039993  252.309998   42303700\n",
            "News sample:\n",
            "Empty DataFrame\n",
            "Columns: [published_at, source, title, summary, url, overall_sentiment]\n",
            "Index: []\n"
          ]
        }
      ],
      "id": "ZvXsw-Yb3VUn"
    },
    {
      "cell_type": "markdown",
      "id": "9f913beb",
      "metadata": {
        "id": "9f913beb"
      },
      "source": [
        "## ui/gradio_app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346a0a53",
      "metadata": {
        "id": "346a0a53",
        "outputId": "ff7a3604-9283-4f27-8663-12e26a8743f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\azizi\\anaconda3\\envs\\a520\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\USD\\Projects\\a520\\agentic-finance\\src\\data_io\\prices.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(symbol, start=start, end=end, progress=False)\n",
            "d:\\USD\\Projects\\a520\\agentic-finance\\src\\data_io\\prices.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(symbol, start=start, end=end, progress=False)\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Works in both scripts and notebooks:\n",
        "HERE = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
        "ROOT = (HERE / \"..\").resolve()\n",
        "sys.path.append(str(ROOT))\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "from src.system.orchestrator import run_pipeline\n",
        "\n",
        "\n",
        "def run(symbol, days_back, required_tags_csv):\n",
        "    import json\n",
        "    import pandas as pd\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    def as_text(x):\n",
        "        if x is None:\n",
        "            return \"\"\n",
        "        if isinstance(x, str):\n",
        "            return x\n",
        "        if isinstance(x, (dict, list)):\n",
        "            # pretty JSON for readability / stable comparisons\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2, sort_keys=True)\n",
        "        return str(x)\n",
        "\n",
        "    def clean(s: str) -> str:\n",
        "        # normalize whitespace and strip code fences if any slipped through\n",
        "        if not isinstance(s, str):\n",
        "            s = as_text(s)\n",
        "        s = s.strip()\n",
        "        if s.startswith(\"```\"):\n",
        "            s = s.strip(\"`\").strip()\n",
        "        return s\n",
        "\n",
        "    def synth_to_prose(obj):\n",
        "        # If it's already text, just return normalized text\n",
        "        if not isinstance(obj, dict):\n",
        "            return clean(as_text(obj))\n",
        "\n",
        "        parts = []\n",
        "\n",
        "        ms = obj.get(\"market_signals\") or {}\n",
        "        if ms:\n",
        "            ms_bits = []\n",
        "            cp = ms.get(\"current_price\")\n",
        "            if isinstance(cp, (int, float)):\n",
        "                ms_bits.append(f\"price ${cp:,.2f}\")\n",
        "            ma = ms.get(\"moving_averages\") or {}\n",
        "            ma50 = ma.get(\"50_day\")\n",
        "            ma200 = ma.get(\"200_day\")\n",
        "            if ma50 is not None or ma200 is not None:\n",
        "                ms_bits.append(f\"vs 50D {ma50}, 200D {ma200}\")\n",
        "            rsi = ms.get(\"RSI\")\n",
        "            if rsi is not None:\n",
        "                ms_bits.append(f\"RSI {rsi}\")\n",
        "            trend = ms.get(\"trend\")\n",
        "            if trend:\n",
        "                ms_bits.append(trend)\n",
        "            vol = ms.get(\"volume\") or {}\n",
        "            vcur, vavg = vol.get(\"current\"), vol.get(\"average\")\n",
        "            if vcur is not None and vavg is not None:\n",
        "                ms_bits.append(f\"volume {vcur:,} vs avg {vavg:,}\")\n",
        "            if ms_bits:\n",
        "                parts.append(\"Technicals: \" + \", \".join(str(x) for x in ms_bits if x))\n",
        "\n",
        "        news = obj.get(\"news\") or {}\n",
        "        if news:\n",
        "            # keep order stable-ish\n",
        "            news_bits = []\n",
        "            for k in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                if k in news:\n",
        "                    news_bits.append(f\"{k}: {news[k]}\")\n",
        "            # include any other keys if present\n",
        "            for k, v in news.items():\n",
        "                if k not in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                    news_bits.append(f\"{k}: {v}\")\n",
        "            parts.append(\"News: \" + \"; \".join(news_bits))\n",
        "\n",
        "        risk = obj.get(\"risk_assessment\") or {}\n",
        "        if risk:\n",
        "            risk_bits = []\n",
        "            for k in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                if k in risk:\n",
        "                    risk_bits.append(f\"{k}: {v}\")\n",
        "            for k, v in risk.items():\n",
        "                if k not in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                    risk_bits.append(f\"{k}: {v}\")\n",
        "            parts.append(\"Risk: \" + \"; \".join(risk_bits))\n",
        "\n",
        "        return \"\\n\".join(parts).strip()\n",
        "\n",
        "    def to_df(x):\n",
        "        if isinstance(x, pd.DataFrame):\n",
        "            return x\n",
        "        if x is None:\n",
        "            return pd.DataFrame()\n",
        "        try:\n",
        "            return pd.DataFrame(x)\n",
        "        except Exception:\n",
        "            return pd.DataFrame()\n",
        "    # ---------- /helpers ----------\n",
        "\n",
        "    start = (date.today() - timedelta(days=int(days_back))).isoformat()\n",
        "    end = date.today().isoformat()\n",
        "    tags = [t.strip() for t in required_tags_csv.split(\",\")] if required_tags_csv else None\n",
        "\n",
        "    res = run_pipeline(symbol.strip().upper(), start, end, required_tags=tags)\n",
        "\n",
        "    # Detect if optimizer re-synthesis ran (compare Initial vs Final on normalized JSON text)\n",
        "    optimizer_ran = False\n",
        "    init = next((a for a in res.agent_outputs if a.agent_name in {\"Initial Synthesis\", \"Research Synthesis Agent\", \"SynthesisAgent\"}), None)\n",
        "    if init is not None:\n",
        "        init_txt = clean(as_text(init.analysis))\n",
        "        final_txt_norm = clean(as_text(res.final.analysis))\n",
        "        optimizer_ran = (init_txt != final_txt_norm)\n",
        "\n",
        "    plan = \"\\n\".join([f\"• {step}\" for step in res.plan])\n",
        "\n",
        "    # Agents panel: Synthesis agents shown as prose; others as text\n",
        "    agents_txt = \"\\n\\n\".join([\n",
        "        (\n",
        "            f\"[{a.agent_name}] score={a.score:.2f} conf={a.confidence:.2f}\\n\"\n",
        "            f\"{synth_to_prose(a.analysis) if ('synthesis' in a.agent_name.lower()) else clean(as_text(a.analysis))}\"\n",
        "        )\n",
        "        for a in res.agent_outputs\n",
        "    ])\n",
        "\n",
        "    # Evidence tables (force DataFrame)\n",
        "    news_rows = to_df(res.evidence.get(\"top_news\", []))\n",
        "    prices_rows = to_df(res.evidence.get(\"prices_tail\", []))\n",
        "\n",
        "    # Helpful note when news evidence is empty\n",
        "    if news_rows.empty:\n",
        "        agents_txt += \"\\n\\n[Note] No news items matched filters or API limits were hit today.\"\n",
        "\n",
        "    # ---- Critique FIRST (so it appears above Final in the UI) ----\n",
        "    crit_txt = (\n",
        "        f\"[Critique]\\n\"\n",
        "        f\"score={res.critique.score:.2f} adj_conf={res.critique.confidence:.2f}\\n\"\n",
        "        f\"{clean(as_text(res.critique.analysis))}\"\n",
        "    )\n",
        "\n",
        "    # ---- Final AFTER Critique ----\n",
        "    headline = \"FINAL (After Critique)\"\n",
        "    opt_line = \"[Optimizer ran: YES]\" if optimizer_ran else \"[Optimizer ran: NO]\"\n",
        "    final_txt = (\n",
        "        f\"{headline}\\n{opt_line}\\n\"\n",
        "        f\"score={res.final.score:.2f} conf={res.final.confidence:.2f}\\n\"\n",
        "        f\"{synth_to_prose(res.final.analysis)}\\n\\nKey: {', '.join(res.final.key_factors)}\"\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: return order matches component outputs order (crit BEFORE final)\n",
        "    return plan, agents_txt, crit_txt, final_txt, news_rows, prices_rows\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Agentic Finance\") as demo:\n",
        "    gr.Markdown(\"# Agentic Finance — Interactive Tester\")\n",
        "\n",
        "    with gr.Row():\n",
        "        symbol = gr.Textbox(label=\"Ticker\", value=\"AAPL\")\n",
        "        days_back = gr.Slider(7, 120, value=30, step=1, label=\"Days Back\")\n",
        "        tags = gr.Textbox(label=\"Required Tags (optional, comma-sep)\", placeholder=\"earnings, product\")\n",
        "    run_btn = gr.Button(\"Run\")\n",
        "\n",
        "    plan = gr.Textbox(label=\"Plan\", lines=6)\n",
        "    agents = gr.Textbox(label=\"Agent Outputs\", lines=14)\n",
        "\n",
        "    #  Critique ABOVE Final\n",
        "    crit = gr.Textbox(label=\"Critique\", lines=8)\n",
        "    final = gr.Textbox(label=\"Final Recommendation\", lines=10)\n",
        "\n",
        "    news_tbl = gr.Dataframe(\n",
        "        headers=[\"published_at\",\"source\",\"title\",\"summary\",\"url\",\"overall_sentiment\",\"tags\",\"numbers\"],\n",
        "        label=\"Top News (evidence)\",\n",
        "        wrap=True\n",
        "    )\n",
        "    prices_tbl = gr.Dataframe(label=\"Recent Prices (evidence)\")\n",
        "\n",
        "    # Outputs order: plan, agents, CRITIQUE, FINAL, news, prices\n",
        "    run_btn.click(\n",
        "        run,\n",
        "        inputs=[symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "a520",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}