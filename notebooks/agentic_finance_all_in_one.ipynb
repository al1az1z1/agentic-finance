{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e8e8972",
      "metadata": {
        "id": "8e8e8972"
      },
      "source": [
        "# Agentic Finance — All-in-One Notebook\n",
        "\n",
        "**This notebook is a demonstration notebook showing the full workflow and plan of our project.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;It summarizes the key components we developed across the Agentic Finance system, including data ingestion, analysis, orchestration, and user interface layers\n",
        "\n",
        "**To run the full interactive application, please use the Gradio app located at:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;/src/ui/gradio_app.py\n",
        "\n",
        "**Run it from the project root with:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;python -m ui.gradio_app\n",
        "\n",
        "**This will test the agents and see live results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kl2KAh90u7JO",
      "metadata": {
        "id": "kl2KAh90u7JO"
      },
      "source": [
        "# Market Research Agent — Price, Indicators, and News Pipeline\n",
        "\n",
        "This project builds a lightweight research workflow for equities: we fetch historical prices, compute common technical indicators (SMA, RSI), and pull headline summaries with basic sentiment. A small JSON cache keeps runs fast and reproducible while avoiding API rate limits. The notebook(s) walk through data loading, quick EDA, feature prep, and simple evaluation.\n",
        "\n",
        "**Course:** MSAAI 520-02 — Group 5  \n",
        "**Date:** October 18, 2025\n",
        "\n",
        "## Team\n",
        "- Ali Azizi  \n",
        "- Sunitha Kosireddy  \n",
        "- Victor Salcedo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f452ac",
      "metadata": {
        "id": "21f452ac",
        "outputId": "d7c2cc1b-c923-483a-dff6-b46bd4327c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: \\mnt\\data\\agentic-finance\n"
          ]
        }
      ],
      "source": [
        "# Setup: create package tree and write source files (no changes to your code)\n",
        "import pathlib, sys\n",
        "ROOT = pathlib.Path(\"/mnt/data/agentic-finance\")\n",
        "SRC = ROOT / \"src\"\n",
        "UI = ROOT / \"ui\"\n",
        "sys.path.insert(0, str(ROOT))\n",
        "print(\"Project root:\", ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac59b6",
      "metadata": {
        "id": "1dac59b6"
      },
      "source": [
        "### Add repo root to sys.path and ensure packages exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2be1c7",
      "metadata": {
        "id": "ee2be1c7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Find repo root (folder containing \"src\")\n",
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"src\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "sys.path.insert(0, str(ROOT))  # make \"src\" importable\n",
        "\n",
        "# Ensure packages (empty __init__.py files)\n",
        "for p in [\n",
        "    ROOT / \"src\",\n",
        "    ROOT / \"src\" / \"config\",\n",
        "    ROOT / \"src\" / \"data_io\",\n",
        "    ROOT / \"src\" / \"system\",\n",
        "    ROOT / \"src\" / \"analysis\",\n",
        "\n",
        "\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    (p / \"__init__.py\").touch(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54efe4b1",
      "metadata": {
        "id": "54efe4b1"
      },
      "source": [
        "## src/analysis/features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "383f0b0b",
      "metadata": {
        "id": "383f0b0b"
      },
      "outputs": [],
      "source": [
        "# src/analysis/features.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_sma(prices: pd.DataFrame, window: int) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    return prices[\"close\"].rolling(window=window).mean()\n",
        "\n",
        "def compute_rsi(prices: pd.DataFrame, window: int = 14) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    delta = prices[\"close\"].diff()\n",
        "    gain = np.where(delta > 0, delta, 0.0)\n",
        "    loss = np.where(delta < 0, -delta, 0.0)\n",
        "    gain_s = pd.Series(gain, index=prices.index)\n",
        "    loss_s = pd.Series(loss, index=prices.index)\n",
        "    avg_gain = gain_s.rolling(window=window).mean()\n",
        "    avg_loss = loss_s.rolling(window=window).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99347f63",
      "metadata": {
        "id": "99347f63"
      },
      "source": [
        "## src/analysis/text.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc78099",
      "metadata": {
        "id": "fdc78099"
      },
      "outputs": [],
      "source": [
        "# Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "# from config.settings import SETTINGS\n",
        "\n",
        "# -----------------------------\n",
        "# Existing tagging / preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "TAG_RULES = {\n",
        "    \"earnings\": [\"earnings\", \"eps\", \"guidance\", \"outlook\", \"quarter\", \"revenue\"],\n",
        "    \"product\":  [\"launch\", \"iphone\", \"chip\", \"feature\", \"service\"],\n",
        "    \"legal\":    [\"lawsuit\", \"regulator\", \"antitrust\", \"fine\", \"settlement\"],\n",
        "    \"macro\":    [\"inflation\", \"rates\", \"fed\", \"recession\", \"gdp\"]\n",
        "}\n",
        "\n",
        "def preprocess_news(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"published_at\",\"source\",\"title\",\"summary\",\"url\",\n",
        "            \"overall_sentiment\",\"tags\",\"numbers\"\n",
        "        ])\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Alpha Vantage format is like \"20251017T200143\"\n",
        "    # Parse with explicit format; keep timezone-aware for safety\n",
        "    df[\"published_at\"] = pd.to_datetime(\n",
        "        df[\"published_at\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\", utc=True\n",
        "    )\n",
        "\n",
        "    # Drop rows with no title/url; keep others (don’t drop NaT here — the date filter happens later)\n",
        "    df = df.dropna(subset=[\"title\",\"url\"]).drop_duplicates(subset=[\"url\"])\n",
        "    df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def classify_tags(text: str) -> list[str]:\n",
        "    text_l = text.lower()\n",
        "    tags = [k for k, kws in TAG_RULES.items() if any(kw in text_l for kw in kws)]\n",
        "    return tags or [\"general\"]\n",
        "\n",
        "NUM_RE = re.compile(r'(\\$?\\b\\d+(?:\\.\\d+)?%?)')\n",
        "\n",
        "def extract_numbers(text: str) -> list[str]:\n",
        "    return NUM_RE.findall(text or \"\")[:6]\n",
        "\n",
        "def add_tags_and_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    df[\"tags\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(classify_tags)\n",
        "    df[\"numbers\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(extract_numbers)\n",
        "    return df\n",
        "\n",
        "def recent_topk(df: pd.DataFrame, topk: int, days: int, required_tags: list[str] | None = None) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Make an aware UTC cutoff; df['published_at'] is already UTC-aware\n",
        "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n",
        "    f = df[df[\"published_at\"] >= cutoff]\n",
        "\n",
        "    if required_tags:\n",
        "        want = [t.strip().lower() for t in required_tags]\n",
        "        f_tags = f[f[\"tags\"].apply(lambda ts: any(t in [x.lower() for x in ts] for t in want))]\n",
        "        f = f_tags if not f_tags.empty else f\n",
        "\n",
        "    return f.sort_values(\"published_at\", ascending=False).head(topk)\n",
        "\n",
        "# -----------------------------\n",
        "# NEW: shared agent utilities\n",
        "# -----------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "def strip_code_fences(s: str) -> str:\n",
        "    \"\"\"Remove leading/trailing ``` blocks (optionally ```json).\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.IGNORECASE)\n",
        "\n",
        "def to_float(x, default: float = 0.0) -> float:\n",
        "    \"\"\"Best-effort conversion of model outputs or strings to float.\"\"\"\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            xs = x.strip().lower()\n",
        "            # map common words to numeric anchors\n",
        "            if xs in (\"high\", \"strong\", \"bullish\", \"overbought\"):\n",
        "                return 0.8\n",
        "            if xs in (\"medium\", \"moderate\", \"neutral\"):\n",
        "                return 0.5\n",
        "            if xs in (\"low\", \"weak\", \"bearish\", \"oversold\"):\n",
        "                return 0.2\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def clamp(x: float, lo: float, hi: float) -> float:\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "def normalize_score(v: float) -> float:\n",
        "    \"\"\"\n",
        "    Normalize arbitrary score ranges to [-1, 1].\n",
        "    Heuristics:\n",
        "      - If already in [-1,1], keep.\n",
        "      - If in [0,1], map to [-1,1] via (v-0.5)*2.\n",
        "      - If in (1,100], treat as percent.\n",
        "      - If in (1,10], treat as 0-10 and map.\n",
        "      - Else, clamp.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        v = float(v)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    if -1.0 <= v <= 1.0:\n",
        "        return v\n",
        "    if 0.0 <= v <= 1.0:\n",
        "        return (v - 0.5) * 2.0\n",
        "    if 1.0 < v <= 100.0:\n",
        "        v01 = v / 100.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    if 1.0 < v <= 10.0:\n",
        "        v01 = v / 10.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    return clamp(v, -1.0, 1.0)\n",
        "\n",
        "def normalize_conf(v) -> float:\n",
        "    \"\"\"Normalize any confidence-like value to [0,1].\"\"\"\n",
        "    f = to_float(v, 0.7)\n",
        "    if 1.0 < f <= 100.0:\n",
        "        f = f / 100.0\n",
        "    return clamp(f, 0.0, 1.0)\n",
        "\n",
        "# Optional: helpers to render structured dicts into strings (for external tools)\n",
        "def pretty_json_block(obj: dict, max_chars: int = 4000) -> str:\n",
        "    \"\"\"Return a fenced JSON markdown block, truncated for UI safety.\"\"\"\n",
        "    try:\n",
        "        js = json.dumps(obj, ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        js = str(obj)\n",
        "    if len(js) > max_chars:\n",
        "        js = js[: max_chars - 20] + \"\\n... (truncated)\"\n",
        "    return f\"```json\\n{js}\\n```\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a110bf",
      "metadata": {
        "id": "a2a110bf"
      },
      "source": [
        "## src/config/settings.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74137240",
      "metadata": {
        "id": "74137240"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _find_project_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Walk upward to find the repo root heuristically.\n",
        "    Treat a folder containing both 'src' and 'data' as the root.\n",
        "    Fallback to the starting directory if not found.\n",
        "    \"\"\"\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# project root = repo root\n",
        "if \"__file__\" in globals():\n",
        "    ROOT = Path(__file__).resolve().parents[2]\n",
        "else:\n",
        "    # Notebook / REPL: start from CWD and auto-detect root\n",
        "    ROOT = _find_project_root(Path.cwd())\n",
        "\n",
        "load_dotenv(ROOT / \".env\", override=False)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Settings:\n",
        "    data_dir: Path = ROOT / \"data\"\n",
        "    cache_dir: Path = ROOT / \"data\" / \"cache\"\n",
        "    runs_dir: Path = ROOT / \"data\" / \"runs\"\n",
        "    alpha_api_key: str = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"\")\n",
        "    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "    news_window_days: int = 14\n",
        "    topk_news: int = 5\n",
        "    cache_ttl_minutes: int = 60\n",
        "\n",
        "SETTINGS = Settings()\n",
        "SETTINGS.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "SETTINGS.runs_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581069c6",
      "metadata": {
        "id": "581069c6"
      },
      "source": [
        "## src/data_io/cache.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XIKxbGY5r1ce",
      "metadata": {
        "id": "XIKxbGY5r1ce"
      },
      "source": [
        "### Simple JSON file cache (TTL + atomic writes)\n",
        "I use a tiny JSON-based cache so repeated runs don’t redo the same work. Each key maps to a file on disk, with a timestamp to support a time-to-live (TTL). Saving is done via a temp file + replace so partial writes don’t corrupt the cache.\n",
        "\n",
        "**Inputs:** `key`, optional `ttl_minutes`, arbitrary `data`  \n",
        "**Key choices:** per-key JSON files under `SETTINGS.cache_dir`, ISO-8601 for dates, atomic write on save  \n",
        "**Output:** `load_cache` returns cached payload or `None`; `save_cache` writes `{\"_ts\": ..., \"data\": ...}` to disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a42293c",
      "metadata": {
        "id": "6a42293c"
      },
      "outputs": [],
      "source": [
        "# Purpose: lightweight disk cache with TTL and atomic writes\n",
        "# Context: used by data fetchers (e.g., price downloads) to avoid repeat network calls\n",
        "# Notes: filenames derive from key under SETTINGS.cache_dir; payload stored as JSON\n",
        "\n",
        "# cache.py\n",
        "from __future__ import annotations\n",
        "import json, time\n",
        "from datetime import date, datetime\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def _cache_path(key: str) -> Path:\n",
        "    return SETTINGS.cache_dir / f\"{key}.json\"\n",
        "\n",
        "def _json_default(o: Any):\n",
        "    # datetime & pandas.Timestamp (subclass of datetime) → ISO 8601\n",
        "    if isinstance(o, (datetime, date)):\n",
        "        return o.isoformat()\n",
        "    # Fallback: make a best-effort string (covers Decimal, Path, Enum, etc.)\n",
        "    try:\n",
        "        return str(o)\n",
        "    except Exception:\n",
        "        return repr(o)\n",
        "\n",
        "def load_cache(key: str, ttl_minutes: int | None = None) -> Any | None:\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        if ttl_minutes is None:\n",
        "            return obj.get(\"data\")  # consistent: always return payload\n",
        "        if (time.time() - obj.get(\"_ts\", 0)) <= ttl_minutes * 60:\n",
        "            return obj.get(\"data\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def save_cache(key: str, data: Any) -> None:\n",
        "    p = _cache_path(key)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
        "    payload = {\"_ts\": time.time(), \"data\": data}\n",
        "    tmp.write_text(json.dumps(payload, ensure_ascii=False, default=_json_default), encoding=\"utf-8\")\n",
        "    tmp.replace(p)  # atomic on most OS/filesystems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ee3409",
      "metadata": {
        "id": "78ee3409"
      },
      "source": [
        "## src/data_io/prices.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vIV91U3JrnJa",
      "metadata": {
        "id": "vIV91U3JrnJa"
      },
      "source": [
        "### Fetch historical prices (with simple caching)\n",
        "I pull OHLCV data from Yahoo Finance and return a clean DataFrame. I use a short-lived cache so repeated runs don’t keep hitting the API. If data is already cached, I return it immediately. I also flatten any MultiIndex columns and standardize names so downstream code stays consistent.\n",
        "\n",
        "**Inputs:** `symbol`, `start`, `end`  \n",
        "**Key choices:** use `yfinance.download`, cache key includes date range, flatten MultiIndex, keep both `close` and `adj_close`  \n",
        "**Output:** DataFrame with `date, open, high, low, close, adj_close, volume`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6014f6",
      "metadata": {
        "id": "fb6014f6"
      },
      "outputs": [],
      "source": [
        "# Purpose: download OHLCV from Yahoo Finance and return a normalized DataFrame with caching\n",
        "# Context: called by data prep steps before features/EDA; avoids repeated network calls\n",
        "# Notes: flattens MultiIndex cols, standardizes names, stores json-serializable cache\n",
        "\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_prices(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:\n",
        "    cache_key = f\"prices_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "    df = yf.download(symbol, start=start, end=end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0].lower() for c in df.columns]\n",
        "    df = df.reset_index().rename(columns={\n",
        "        \"Date\": \"date\", \"open\":\"open\",\"high\":\"high\",\"low\":\"low\",\"close\":\"close\",\"adj close\":\"adj_close\",\"volume\":\"volume\"\n",
        "    })\n",
        "    df[\"date\"] = df[\"date\"].astype(str)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a58f9b6",
      "metadata": {
        "id": "7a58f9b6"
      },
      "source": [
        "## src/data_io/indicators.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTEo0-OksEFN",
      "metadata": {
        "id": "UTEo0-OksEFN"
      },
      "source": [
        "### Fetch technical indicators (Alpha Vantage with local fallback)\n",
        "I get daily SMA/RSI for a symbol. I try Alpha Vantage first and fall back to computing the indicator locally from our price history when the key is missing or the API is rate-limited. Results are cached so repeated calls are fast.\n",
        "\n",
        "**Inputs:** `symbol`, `indicator` (`\"SMA\"` or `\"RSI\"`), `time_period`  \n",
        "**Key choices:** Alpha Vantage params (daily interval, series_type=close), JSON parsing with key map, local fallback uses `compute_sma`/`compute_rsi` on `fetch_prices`  \n",
        "**Output:** DataFrame with `date` and indicator column(s), sorted ascending\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752d66ae",
      "metadata": {
        "id": "752d66ae"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch SMA/RSI via Alpha Vantage with a cached local-compute fallback\n",
        "# Context: used by feature pipelines that need daily indicators\n",
        "# Notes: caches by (symbol, indicator, time_period); normalizes dates and numeric types\n",
        "\n",
        "# src/data_io/indicators.py\n",
        "from __future__ import annotations\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.analysis.features import compute_sma, compute_rsi\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "KEYS = {\"SMA\": \"Technical Analysis: SMA\", \"RSI\": \"Technical Analysis: RSI\"}\n",
        "\n",
        "\n",
        "# If AV isn’t available (no key/limit), our code falls back to computing indicators locally from prices using our compute_sma / compute_rsi.\n",
        "def _fallback_from_prices(symbol: str, indicator: str, time_period: int) -> pd.DataFrame:\n",
        "    prices = fetch_prices(symbol, None, None)\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if indicator == \"SMA\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"SMA\": compute_sma(prices, window=time_period)})\n",
        "    elif indicator == \"RSI\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"RSI\": compute_rsi(prices, window=time_period)})\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna().sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def fetch_indicator(symbol: str, indicator: str, time_period: int = 14) -> pd.DataFrame:\n",
        "    key = KEYS.get(indicator)\n",
        "\n",
        "    # Try cache first\n",
        "    cache_key = f\"indicator_{symbol}_{indicator}_{time_period}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    if not SETTINGS.alpha_api_key or key is None:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    params = {\n",
        "        \"function\": indicator,\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": \"daily\",\n",
        "        \"time_period\": time_period,\n",
        "        \"series_type\": \"close\",\n",
        "        \"apikey\": SETTINGS.alpha_api_key,\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(BASE, params=params, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        # Alpha Vantage quota message handling:\n",
        "        if (not data or key not in data or not data[key] or \"Note\" in data or \"Information\" in data or \"Error Message\" in data):\n",
        "            df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "            save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "            return df\n",
        "    except Exception:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "    df.reset_index(inplace=True)\n",
        "    df = df.rename(columns={\"index\": \"date\"})\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30738873",
      "metadata": {
        "id": "30738873"
      },
      "source": [
        "## src/data_io/news.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iJxmkUXGsR3s",
      "metadata": {
        "id": "iJxmkUXGsR3s"
      },
      "source": [
        "### Fetch news (Alpha Vantage, symbol-filtered, cached)\n",
        "I pull recent news from Alpha Vantage’s `NEWS_SENTIMENT` endpoint and keep only items that explicitly mention my ticker with decent relevance. I cache the result so I don’t burn quota on repeat runs.\n",
        "\n",
        "**Inputs:** `symbol`  \n",
        "**Key choices:** `relevance_score >= 0.30`, require explicit ticker match, cache per symbol  \n",
        "**Output:** DataFrame with `published_at, source, title, summary, url, overall_sentiment`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9bb3f1",
      "metadata": {
        "id": "fc9bb3f1"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch and cache symbol-specific news via Alpha Vantage, filtered by relevance\n",
        "# Context: called by downstream reporting/EDA to attach headlines and sentiment\n",
        "# Notes: filters to items where ticker matches and relevance >= 0.30; caches by symbol\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, requests, pandas as pd\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "\n",
        "def fetch_news(symbol: str) -> pd.DataFrame:\n",
        "    if not SETTINGS.alpha_api_key:\n",
        "        return pd.DataFrame()  # safe fail\n",
        "    cache_key = f\"news_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    params = {\"function\":\"NEWS_SENTIMENT\",\"tickers\":symbol,\"apikey\":SETTINGS.alpha_api_key}\n",
        "    r = requests.get(BASE, params=params, timeout=30)\n",
        "    data = r.json()\n",
        "    if \"feed\" not in data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for item in data.get(\"feed\", []):\n",
        "        tickers = item.get(\"ticker_sentiment\", []) or []\n",
        "        # keep only if our symbol is explicitly mentioned\n",
        "        keep = any(t.get(\"ticker\", \"\").upper() == symbol.upper() and float(t.get(\"relevance_score\", 0) or 0) >= 0.30\n",
        "                   for t in tickers)\n",
        "        if not keep:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"published_at\": item.get(\"time_published\"),\n",
        "            \"source\": item.get(\"source\"),\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"summary\": item.get(\"summary\"),\n",
        "            \"url\": item.get(\"url\"),\n",
        "            \"overall_sentiment\": item.get(\"overall_sentiment_label\")\n",
        "        })\n",
        "\n",
        "    # ====== Forth APPROACH =====\n",
        "    df = pd.DataFrame(rows)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc9c201",
      "metadata": {
        "id": "6cc9c201"
      },
      "source": [
        "## src/system/router.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1200bf3",
      "metadata": {
        "id": "f1200bf3"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "def choose_agents(has_news: bool, has_prices: bool, has_technicals: bool) -> list[str]:\n",
        "    agents = []\n",
        "    if has_news: agents.append(\"news\")\n",
        "    # earnings optional if you add a financials fetch later\n",
        "    if has_technicals and has_prices: agents.append(\"technical\")\n",
        "    agents.append(\"risk\")\n",
        "    return agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa0a509",
      "metadata": {
        "id": "afa0a509"
      },
      "source": [
        "## src/system/memory.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b260d5e1",
      "metadata": {
        "id": "b260d5e1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "MEM_PATH = SETTINGS.runs_dir / \"run_notes.jsonl\"\n",
        "\n",
        "def append_memory(record: dict[str, Any]) -> None:\n",
        "    MEM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with MEM_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cba7d18",
      "metadata": {
        "id": "2cba7d18"
      },
      "source": [
        "## src/agents.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdbc08b",
      "metadata": {
        "id": "9fdbc08b"
      },
      "outputs": [],
      "source": [
        "# === Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Import shared helpers from analysis.text\n",
        "from src.analysis.text import (\n",
        "    strip_code_fences,\n",
        "    to_float,\n",
        "    clamp,\n",
        "    normalize_score,\n",
        "    normalize_conf,\n",
        ")\n",
        "\n",
        "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if OPENAI_KEY:\n",
        "    from openai import OpenAI\n",
        "    _client = OpenAI(api_key=OPENAI_KEY)\n",
        "else:\n",
        "    _client = None  # mock mode\n",
        "\n",
        "@dataclass\n",
        "class AgentResponse:\n",
        "    agent_name: str\n",
        "    analysis: str\n",
        "    score: float\n",
        "    confidence: float\n",
        "    key_factors: List[str]\n",
        "    timestamp: str\n",
        "\n",
        "class BaseAgent:\n",
        "    def __init__(self, agent_name: str, model: str = \"gpt-4o-mini\"):\n",
        "        self.agent_name = agent_name\n",
        "        self.model = model\n",
        "\n",
        "    def call_llm(self, system_prompt: str, user_message: str) -> str:\n",
        "        if _client is None:  # mock\n",
        "            return json.dumps({\n",
        "                \"analysis\": f\"MOCK: {self.agent_name} processed.\",\n",
        "                \"score\": 0.0,\n",
        "                \"key_factors\": [\"mock\"],\n",
        "                \"confidence\": 0.7\n",
        "            })\n",
        "        try:\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=800\n",
        "            )\n",
        "            return resp.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return json.dumps({\n",
        "                \"analysis\": f\"Error: {e}\",\n",
        "                \"score\": 0.0,\n",
        "                \"key_factors\": [\"error\"],\n",
        "                \"confidence\": 0.3\n",
        "            })\n",
        "\n",
        "# -----------------------------\n",
        "# News\n",
        "# -----------------------------\n",
        "\n",
        "class NewsAnalysisAgent(BaseAgent):\n",
        "    \"\"\"Analyzes financial news sentiment and impact\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"News Analysis Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a financial news analyst specializing in sentiment analysis.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze news articles objectively\n",
        "2. Consider both positive and negative aspects\n",
        "3. Provide a sentiment score from -1 (very negative) to +1 (very positive)\n",
        "4. Identify key factors driving the sentiment\n",
        "5. Assess potential stock price impact\n",
        "https://chatgpt.com/gpts\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"sentiment_score\": 0.75,\n",
        "  \"analysis\": \"Strong positive sentiment driven by earnings beat and product launch\",\n",
        "  \"key_factors\": [\"Earnings exceeded expectations\", \"New product well-received\"],\n",
        "  \"confidence\": 0.85\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: sentiment_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'AAPL')\n",
        "        news_articles = data.get('news', [])\n",
        "\n",
        "        news_summary = \"\\n\".join([\n",
        "            f\"- {a.get('title','')}: {a.get('description') or a.get('summary','')}\"\n",
        "            for a in news_articles[:5]\n",
        "        ])\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following recent news about {ticker}:\n",
        "\n",
        "{news_summary}\n",
        "\n",
        "Provide sentiment analysis and impact assessment.\"\"\"\n",
        "\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js  = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get('sentiment_score', 0), 0.0))\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# Technicals\n",
        "# -----------------------------\n",
        "\n",
        "class MarketSignalsAgent(BaseAgent):\n",
        "    \"\"\"Performs technical analysis on market data\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"Market Signals Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a technical analyst specializing in market signals and price patterns.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze technical indicators objectively\n",
        "2. Assess technical strength from -1 (very bearish) to +1 (very bullish)\n",
        "3. Identify support/resistance levels\n",
        "4. Evaluate trend direction and momentum\n",
        "5. Consider volume patterns\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"technical_score\": 0.65,\n",
        "  \"analysis\": \"Bullish technical setup with price above key moving averages\",\n",
        "  \"key_factors\": [\"Price above 50-day MA\", \"RSI indicates strength\", \"Volume confirming uptrend\"],\n",
        "  \"confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: technical_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'UNKNOWN')\n",
        "        technicals = data.get('technicals', {})\n",
        "\n",
        "        technical_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Current Price: ${technicals.get('current_price', 'N/A')}\n",
        "50-day MA: ${technicals.get('ma_50', 'N/A')}\n",
        "200-day MA: ${technicals.get('ma_200', 'N/A')}\n",
        "RSI: {technicals.get('rsi', 'N/A')}\n",
        "MACD: {technicals.get('macd', 'N/A')}\n",
        "Volume: {technicals.get('volume', 'N/A')} (Avg: {technicals.get('avg_volume', 'N/A')})\n",
        "Support: ${technicals.get('support', 'N/A')}\n",
        "Resistance: ${technicals.get('resistance', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following technical data for {ticker}:\n",
        "\n",
        "{technical_summary}\n",
        "\n",
        "Assess technical strength and price momentum. Provide only the JSON object described above.\"\"\"\n",
        "\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js  = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get('technical_score', 0), 0.0))\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# Risk\n",
        "# -----------------------------\n",
        "\n",
        "class RiskAssessmentAgent(BaseAgent):\n",
        "    \"\"\"Assesses investment risk and portfolio fit\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"Risk Assessment Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a risk management analyst specializing in portfolio risk assessment.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze risk metrics objectively\n",
        "2. Provide risk level score from 0 (very low risk) to 1 (very high risk)\n",
        "3. Identify key risk factors\n",
        "4. Assess portfolio diversification implications\n",
        "5. Evaluate risk-adjusted returns\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"risk_score\": 0.45,\n",
        "  \"analysis\": \"Moderate risk profile with acceptable volatility and strong Sharpe ratio\",\n",
        "  \"key_factors\": [\"Beta of 1.15 indicates moderate volatility\", \"Strong Sharpe ratio\", \"Manageable drawdown\"],\n",
        "  \"confidence\": 0.82\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: risk_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'UNKNOWN')\n",
        "        risk_data = data.get('risk_metrics', {})\n",
        "\n",
        "        risk_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Beta: {risk_data.get('beta', 'N/A')}\n",
        "Volatility (30-day): {risk_data.get('volatility', 'N/A')}%\n",
        "Value at Risk (5%): ${risk_data.get('var_5', 'N/A')}\n",
        "Sharpe Ratio: {risk_data.get('sharpe_ratio', 'N/A')}\n",
        "Max Drawdown: {risk_data.get('max_drawdown', 'N/A')}%\n",
        "Sector Correlation: {risk_data.get('sector_correlation', 'N/A')}\n",
        "P/E Ratio: {risk_data.get('pe_ratio', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following risk metrics for {ticker}:\n",
        "\n",
        "{risk_summary}\n",
        "\n",
        "Assess overall investment risk and portfolio implications.\"\"\"\n",
        "\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js  = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            # Keep 0..1 semantics but normalize/clamp\n",
        "            risk01 = to_float(result.get('risk_score', 0.5), 0.5)\n",
        "            if 1.0 < risk01 <= 100.0:\n",
        "                risk01 = risk01 / 100.0\n",
        "            elif 1.0 < risk01 <= 10.0:\n",
        "                risk01 = risk01 / 10.0\n",
        "            risk01 = clamp(risk01, 0.0, 1.0)\n",
        "\n",
        "            score = risk01\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.8))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.5\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# Synthesis\n",
        "# -----------------------------\n",
        "\n",
        "class SynthesisAgent(BaseAgent):\n",
        "    \"\"\"Combines insights from all agents into final recommendation\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"Research Synthesis Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a senior investment analyst who synthesizes multiple analyses into actionable recommendations.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review all agent analyses objectively\n",
        "2. Weigh different factors appropriately\n",
        "3. Provide clear investment recommendation (STRONG BUY, BUY, HOLD, SELL, STRONG SELL)\n",
        "4. State confidence level (0 to 1)\n",
        "5. Summarize key reasoning\n",
        "6. Note important risks\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"recommendation\": \"BUY\",\n",
        "  \"confidence\": 0.78,\n",
        "  \"analysis\": \"Strong fundamentals and positive technical signals support a buy recommendation despite moderate risk\",\n",
        "  \"key_points\": [\"Earnings beat expectations\", \"Technical breakout\", \"Acceptable risk profile\"],\n",
        "  \"risks\": [\"Market volatility\", \"Sector headwinds\"]\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: recommendation, confidence, analysis, key_points, risks\"\"\"\n",
        "\n",
        "    def process(self, agent_responses: List[AgentResponse]) -> AgentResponse:\n",
        "        analyses_summary = \"\\n\\n\".join([\n",
        "            f\"{resp.agent_name}:\\n\"\n",
        "            f\"Score: {resp.score}\\n\"\n",
        "            f\"Analysis: {resp.analysis}\\n\"\n",
        "            f\"Key Factors: {', '.join(resp.key_factors)}\"\n",
        "            for resp in agent_responses\n",
        "        ])\n",
        "\n",
        "        user_message = f\"\"\"Synthesize the following analyses into a final investment recommendation:\n",
        "\n",
        "{analyses_summary}\n",
        "\n",
        "Provide comprehensive investment recommendation with supporting reasoning.\"\"\"\n",
        "\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js  = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            recommendation = str(result.get('recommendation', 'HOLD')).upper()\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_points', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "\n",
        "            rec_to_score = {\n",
        "                'STRONG BUY': 1.0,\n",
        "                'BUY': 0.6,\n",
        "                'HOLD': 0.0,\n",
        "                'SELL': -0.6,\n",
        "                'STRONG SELL': -1.0\n",
        "            }\n",
        "            score = rec_to_score.get(recommendation, 0.0)\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# Critique\n",
        "# -----------------------------\n",
        "\n",
        "class CritiqueAgent(BaseAgent):\n",
        "    \"\"\"Reviews and validates analysis quality\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"Critique & Validation Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a critique analyst who reviews investment recommendations for biases, logical errors, and completeness.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the synthesis objectively\n",
        "2. Identify logical inconsistencies\n",
        "3. Detect potential biases\n",
        "4. Note missing considerations\n",
        "5. Assess data quality\n",
        "6. Recommend confidence adjustments\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"quality_score\": 0.82,\n",
        "  \"issues_found\": [\"Limited macroeconomic analysis\"],\n",
        "  \"suggestions\": [\"Consider Federal Reserve policy impact\", \"Add sector comparison\"],\n",
        "  \"adjusted_confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: quality_score, issues_found, suggestions, adjusted_confidence\"\"\"\n",
        "\n",
        "    def process(self, synthesis_response: AgentResponse) -> AgentResponse:\n",
        "        user_message = f\"\"\"Review this investment analysis for quality and completeness:\n",
        "\n",
        "Recommendation: {synthesis_response.analysis}\n",
        "Confidence: {synthesis_response.confidence}\n",
        "Key Factors: {', '.join(synthesis_response.key_factors)}\n",
        "\n",
        "Identify any issues, biases, or missing elements.\"\"\"\n",
        "\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js  = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            quality_score = to_float(result.get('quality_score', 0.7), 0.7)\n",
        "            # normalize 0..10 or 0..100 to 0..1 (display-style)\n",
        "            if 1.0 < quality_score <= 10.0:\n",
        "                quality_score = quality_score / 10.0\n",
        "            elif 10.0 < quality_score <= 100.0:\n",
        "                quality_score = quality_score / 100.0\n",
        "            quality_score = clamp(quality_score, 0.0, 1.0)\n",
        "\n",
        "            issues = result.get('issues_found', [])\n",
        "            suggestions = result.get('suggestions', [])\n",
        "            adjusted_confidence = normalize_conf(\n",
        "                result.get('adjusted_confidence', synthesis_response.confidence)\n",
        "            )\n",
        "\n",
        "            analysis = f\"Quality Score: {quality_score}\\n\"\n",
        "            if issues:\n",
        "                analysis += f\"Issues Found: {', '.join(issues)}\\n\"\n",
        "            if suggestions:\n",
        "                analysis += f\"Suggestions: {', '.join(suggestions)}\"\n",
        "\n",
        "            key_factors = issues if issues else [\"No major issues found\"]\n",
        "        except json.JSONDecodeError:\n",
        "            quality_score = 0.7\n",
        "            analysis = raw\n",
        "            adjusted_confidence = synthesis_response.confidence\n",
        "            key_factors = [\"No major issues found\"]\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(quality_score),\n",
        "            confidence=float(adjusted_confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14963947",
      "metadata": {},
      "source": [
        "## src/system/orchestration.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4a539fc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ======= 3Rd version of the orchestrator ==========\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.data_io.news import fetch_news\n",
        "from src.data_io.indicators import fetch_indicator\n",
        "from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "from src.system.router import choose_agents\n",
        "from src.system.memory import append_memory\n",
        "from src.agents import (\n",
        "    NewsAnalysisAgent,\n",
        "    MarketSignalsAgent,\n",
        "    RiskAssessmentAgent,\n",
        "    SynthesisAgent,\n",
        "    CritiqueAgent,\n",
        "    AgentResponse,\n",
        ")\n",
        "\n",
        "# from ..analysis.signals import normalize_technicals  # add this import\n",
        "\n",
        "import json\n",
        "\n",
        "def _as_text(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _as_list_of_text(x):\n",
        "    if isinstance(x, list):\n",
        "        return [_as_text(i) for i in x]\n",
        "    if x is None:\n",
        "        return []\n",
        "    # if a single str/dict was returned, wrap it\n",
        "    return [_as_text(x)]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OrchestratorResult:\n",
        "    plan: List[str]\n",
        "    evidence: Dict[str, DataFrame]\n",
        "    agent_outputs: List[AgentResponse]\n",
        "    final: AgentResponse\n",
        "    critique: AgentResponse\n",
        "\n",
        "\n",
        "def run_pipeline(symbol: str, start: str | None, end: str | None,\n",
        "                 required_tags: list[str] | None = None) -> OrchestratorResult:\n",
        "    plan = [\n",
        "        \"fetch_prices\", \"fetch_news\", \"preprocess\", \"classify_extract\",\n",
        "        \"retrieve_topk\", \"route\", \"run_agents\", \"synthesize\", \"critique\", \"save_memory\"\n",
        "    ]\n",
        "\n",
        "    # 1) fetch\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    news   = fetch_news(symbol)\n",
        "\n",
        "    # 2) preprocess (clean + tags + lightweight numeric extraction)\n",
        "    news_pp = add_tags_and_numbers(preprocess_news(news))\n",
        "\n",
        "    # 3) retrieval (deterministic)\n",
        "    top_news = recent_topk(\n",
        "        news_pp,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags\n",
        "    )\n",
        "\n",
        "    # 4) route\n",
        "    has_news   = not top_news.empty\n",
        "    has_prices = not prices.empty\n",
        "\n",
        "    # Technical indicators (fetch what we can; leave the rest as None)\n",
        "    rsi = fetch_indicator(symbol, \"RSI\", 14)         # may be empty\n",
        "    sma20 = fetch_indicator(symbol, \"SMA\", 20)       # may be empty\n",
        "    # Optional (if your fetch_indicator supports these; otherwise they'll be empty DataFrames)\n",
        "    sma50 = fetch_indicator(symbol, \"SMA\", 50)\n",
        "    sma200 = fetch_indicator(symbol, \"SMA\", 200)\n",
        "\n",
        "    has_technicals = (not rsi.empty) or (not sma20.empty) or (not sma50.empty) or (not sma200.empty)\n",
        "    lanes = choose_agents(has_news, has_prices, has_technicals)\n",
        "\n",
        "    # 5) run agents\n",
        "    outputs: List[AgentResponse] = []\n",
        "\n",
        "    # NEWS (delegate summarization to the agent)\n",
        "    if \"news\" in lanes and has_news:\n",
        "        # Teammate agent expects `description`; remap our `summary` to `description`\n",
        "        news_payload_records = (\n",
        "            top_news\n",
        "            .rename(columns={\"summary\": \"description\"})\n",
        "            .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "            .to_dict(orient=\"records\")\n",
        "        )\n",
        "        news_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"news\": news_payload_records\n",
        "        }\n",
        "        outputs.append(NewsAnalysisAgent().process(news_payload))\n",
        "\n",
        "    # TECHNICALS (build dict the agent expects; missing fields are fine)\n",
        "    if \"technical\" in lanes and (has_technicals or has_prices):\n",
        "        current_price = float(prices[\"close\"].iloc[-1]) if has_prices else None\n",
        "        volume = int(prices[\"volume\"].iloc[-1]) if has_prices else None\n",
        "        avg_volume = int(prices[\"volume\"].tail(20).mean()) if has_prices else None\n",
        "\n",
        "        technicals = {\n",
        "            \"current_price\": current_price,\n",
        "            \"rsi\": (float(rsi[\"RSI\"].iloc[-1]) if not rsi.empty else None),\n",
        "            # Map to teammate’s keys; if we don’t have these, leave None.\n",
        "            \"ma_50\": (float(sma50[\"SMA\"].iloc[-1]) if not sma50.empty else (float(sma20[\"SMA\"].iloc[-1]) if not sma20.empty else None)),\n",
        "            \"ma_200\": (float(sma200[\"SMA\"].iloc[-1]) if not sma200.empty else None),\n",
        "            \"macd\": None,                # not computed here\n",
        "            \"volume\": volume,\n",
        "            \"avg_volume\": avg_volume,\n",
        "            \"support\": None,\n",
        "            \"resistance\": None,\n",
        "        }\n",
        "        tech_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"technicals\": technicals\n",
        "        }\n",
        "        outputs.append(MarketSignalsAgent().process(tech_payload))\n",
        "\n",
        "\n",
        "    # RISK (simple proxy; fill what you have)\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if has_prices else None\n",
        "    risk_payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\": None,\n",
        "            \"volatility\": vol_30d,      # teammate agent expects 'volatility' (%)\n",
        "            \"var_5\": None,\n",
        "            \"sharpe_ratio\": None,\n",
        "            \"max_drawdown\": None,\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\": None\n",
        "        }\n",
        "    }\n",
        "    outputs.append(RiskAssessmentAgent().process(risk_payload))\n",
        "\n",
        "    # 6) synthesize + critique (gated second pass)\n",
        "    synth_v1 = SynthesisAgent().process(outputs)\n",
        "    crit     = CritiqueAgent().process(synth_v1)\n",
        "\n",
        "    # Decide if we need a second pass.\n",
        "    # Rule: re-run if critique quality score < 0.90 OR if it mentions \"data quality\".\n",
        "    needs_rerun = (crit.score < 0.90) or (\"data quality\" in \" \".join(crit.key_factors).lower())\n",
        "\n",
        "    synth_final = synth_v1\n",
        "    if needs_rerun:\n",
        "        # Turn critique into feedback for the optimizer pass\n",
        "        critique_feedback = AgentResponse(\n",
        "            agent_name=\"Critique Feedback\",\n",
        "            analysis=_as_text(synth_v1.analysis) + \"\\n\\n[CRITIQUE]\\n\" + _as_text(crit.analysis),\n",
        "            score=crit.score,\n",
        "            confidence=crit.confidence,\n",
        "            key_factors=_as_list_of_text(crit.key_factors),\n",
        "            timestamp=datetime.utcnow().isoformat()\n",
        "            \n",
        "        )\n",
        "\n",
        "        synth_v2_inputs = outputs + [critique_feedback]\n",
        "        synth_v2 = SynthesisAgent().process(synth_v2_inputs)\n",
        "        synth_final = synth_v2\n",
        "\n",
        "    # 7) memory (store both passes where applicable)\n",
        "    append_memory({\n",
        "        \"ticker\": symbol,\n",
        "        \"lanes\": lanes,\n",
        "        \"issues\": crit.key_factors,\n",
        "        \"final_confidence_v1\": synth_v1.confidence,\n",
        "        \"final_confidence_v2\": synth_final.confidence if needs_rerun else None,\n",
        "        \"optimizer_triggered\": bool(needs_rerun),\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    })\n",
        "\n",
        "    # 8) evidence for UI\n",
        "    evidence = {\n",
        "        \"top_news\": top_news,\n",
        "        \"prices_tail\": prices.tail(5)\n",
        "    }\n",
        "\n",
        "    # Show the first synthesis alongside other agents for transparency\n",
        "\n",
        "    outputs.append(AgentResponse(\n",
        "        agent_name=\"Initial Synthesis\",\n",
        "        analysis=_as_text(synth_v1.analysis),\n",
        "        score=float(synth_v1.score),\n",
        "        confidence=float(synth_v1.confidence),\n",
        "        key_factors=_as_list_of_text(synth_v1.key_factors),\n",
        "        timestamp=synth_v1.timestamp\n",
        "))\n",
        "\n",
        "    return OrchestratorResult(plan, evidence, outputs, synth_final, crit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f913beb",
      "metadata": {
        "id": "9f913beb"
      },
      "source": [
        "## ui/gradio_app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "346a0a53",
      "metadata": {
        "id": "346a0a53",
        "outputId": "ff7a3604-9283-4f27-8663-12e26a8743f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\USD\\Projects\\a520\\agentic-finance\\src\\data_io\\prices.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(symbol, start=start, end=end, progress=False)\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Works in both scripts and notebooks:\n",
        "HERE = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
        "ROOT = (HERE / \"..\").resolve()\n",
        "sys.path.append(str(ROOT))\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "from src.system.orchestrator import run_pipeline\n",
        "\n",
        "\n",
        "def run(symbol, days_back, required_tags_csv):\n",
        "    import json\n",
        "    import pandas as pd\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    def as_text(x):\n",
        "        if x is None:\n",
        "            return \"\"\n",
        "        if isinstance(x, str):\n",
        "            return x\n",
        "        if isinstance(x, (dict, list)):\n",
        "            # pretty JSON for readability / stable comparisons\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2, sort_keys=True)\n",
        "        return str(x)\n",
        "\n",
        "    def clean(s: str) -> str:\n",
        "        # normalize whitespace and strip code fences if any slipped through\n",
        "        if not isinstance(s, str):\n",
        "            s = as_text(s)\n",
        "        s = s.strip()\n",
        "        if s.startswith(\"```\"):\n",
        "            s = s.strip(\"`\").strip()\n",
        "        return s\n",
        "\n",
        "    def synth_to_prose(obj):\n",
        "        # If it's already text, just return normalized text\n",
        "        if not isinstance(obj, dict):\n",
        "            return clean(as_text(obj))\n",
        "\n",
        "        parts = []\n",
        "\n",
        "        ms = obj.get(\"market_signals\") or {}\n",
        "        if ms:\n",
        "            ms_bits = []\n",
        "            cp = ms.get(\"current_price\")\n",
        "            if isinstance(cp, (int, float)):\n",
        "                ms_bits.append(f\"price ${cp:,.2f}\")\n",
        "            ma = ms.get(\"moving_averages\") or {}\n",
        "            ma50 = ma.get(\"50_day\")\n",
        "            ma200 = ma.get(\"200_day\")\n",
        "            if ma50 is not None or ma200 is not None:\n",
        "                ms_bits.append(f\"vs 50D {ma50}, 200D {ma200}\")\n",
        "            rsi = ms.get(\"RSI\")\n",
        "            if rsi is not None:\n",
        "                ms_bits.append(f\"RSI {rsi}\")\n",
        "            trend = ms.get(\"trend\")\n",
        "            if trend:\n",
        "                ms_bits.append(trend)\n",
        "            vol = ms.get(\"volume\") or {}\n",
        "            vcur, vavg = vol.get(\"current\"), vol.get(\"average\")\n",
        "            if vcur is not None and vavg is not None:\n",
        "                ms_bits.append(f\"volume {vcur:,} vs avg {vavg:,}\")\n",
        "            if ms_bits:\n",
        "                parts.append(\"Technicals: \" + \", \".join(str(x) for x in ms_bits if x))\n",
        "\n",
        "        news = obj.get(\"news\") or {}\n",
        "        if news:\n",
        "            # keep order stable-ish\n",
        "            news_bits = []\n",
        "            for k in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                if k in news:\n",
        "                    news_bits.append(f\"{k}: {news[k]}\")\n",
        "            # include any other keys if present\n",
        "            for k, v in news.items():\n",
        "                if k not in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                    news_bits.append(f\"{k}: {v}\")\n",
        "            parts.append(\"News: \" + \"; \".join(news_bits))\n",
        "\n",
        "        risk = obj.get(\"risk_assessment\") or {}\n",
        "        if risk:\n",
        "            risk_bits = []\n",
        "            for k in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                if k in risk:\n",
        "                    risk_bits.append(f\"{k}: {v}\")\n",
        "            for k, v in risk.items():\n",
        "                if k not in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                    risk_bits.append(f\"{k}: {v}\")\n",
        "            parts.append(\"Risk: \" + \"; \".join(risk_bits))\n",
        "\n",
        "        return \"\\n\".join(parts).strip()\n",
        "\n",
        "    def to_df(x):\n",
        "        if isinstance(x, pd.DataFrame):\n",
        "            return x\n",
        "        if x is None:\n",
        "            return pd.DataFrame()\n",
        "        try:\n",
        "            return pd.DataFrame(x)\n",
        "        except Exception:\n",
        "            return pd.DataFrame()\n",
        "    # ---------- /helpers ----------\n",
        "\n",
        "    start = (date.today() - timedelta(days=int(days_back))).isoformat()\n",
        "    end = date.today().isoformat()\n",
        "    tags = [t.strip() for t in required_tags_csv.split(\",\")] if required_tags_csv else None\n",
        "\n",
        "    res = run_pipeline(symbol.strip().upper(), start, end, required_tags=tags)\n",
        "\n",
        "    # Detect if optimizer re-synthesis ran (compare Initial vs Final on normalized JSON text)\n",
        "    optimizer_ran = False\n",
        "    init = next((a for a in res.agent_outputs if a.agent_name in {\"Initial Synthesis\", \"Research Synthesis Agent\", \"SynthesisAgent\"}), None)\n",
        "    if init is not None:\n",
        "        init_txt = clean(as_text(init.analysis))\n",
        "        final_txt_norm = clean(as_text(res.final.analysis))\n",
        "        optimizer_ran = (init_txt != final_txt_norm)\n",
        "\n",
        "    plan = \"\\n\".join([f\"• {step}\" for step in res.plan])\n",
        "\n",
        "    # Agents panel: Synthesis agents shown as prose; others as text\n",
        "    agents_txt = \"\\n\\n\".join([\n",
        "        (\n",
        "            f\"[{a.agent_name}] score={a.score:.2f} conf={a.confidence:.2f}\\n\"\n",
        "            f\"{synth_to_prose(a.analysis) if ('synthesis' in a.agent_name.lower()) else clean(as_text(a.analysis))}\"\n",
        "        )\n",
        "        for a in res.agent_outputs\n",
        "    ])\n",
        "\n",
        "    # Evidence tables (force DataFrame)\n",
        "    news_rows = to_df(res.evidence.get(\"top_news\", []))\n",
        "    prices_rows = to_df(res.evidence.get(\"prices_tail\", []))\n",
        "\n",
        "    # Helpful note when news evidence is empty\n",
        "    if news_rows.empty:\n",
        "        agents_txt += \"\\n\\n[Note] No news items matched filters or API limits were hit today.\"\n",
        "\n",
        "    # ---- Critique FIRST (so it appears above Final in the UI) ----\n",
        "    crit_txt = (\n",
        "        f\"[Critique]\\n\"\n",
        "        f\"score={res.critique.score:.2f} adj_conf={res.critique.confidence:.2f}\\n\"\n",
        "        f\"{clean(as_text(res.critique.analysis))}\"\n",
        "    )\n",
        "\n",
        "    # ---- Final AFTER Critique ----\n",
        "    headline = \"FINAL (After Critique)\"\n",
        "    opt_line = \"[Optimizer ran: YES]\" if optimizer_ran else \"[Optimizer ran: NO]\"\n",
        "    final_txt = (\n",
        "        f\"{headline}\\n{opt_line}\\n\"\n",
        "        f\"score={res.final.score:.2f} conf={res.final.confidence:.2f}\\n\"\n",
        "        f\"{synth_to_prose(res.final.analysis)}\\n\\nKey: {', '.join(res.final.key_factors)}\"\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: return order matches component outputs order (crit BEFORE final)\n",
        "    return plan, agents_txt, crit_txt, final_txt, news_rows, prices_rows\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Agentic Finance\") as demo:\n",
        "    gr.Markdown(\"# Agentic Finance — Interactive Tester\")\n",
        "\n",
        "    with gr.Row():\n",
        "        symbol = gr.Textbox(label=\"Ticker\", value=\"AAPL\")\n",
        "        days_back = gr.Slider(7, 120, value=30, step=1, label=\"Days Back\")\n",
        "        tags = gr.Textbox(label=\"Required Tags (optional, comma-sep)\", placeholder=\"earnings, product\")\n",
        "    run_btn = gr.Button(\"Run\")\n",
        "\n",
        "    plan = gr.Textbox(label=\"Plan\", lines=6)\n",
        "    agents = gr.Textbox(label=\"Agent Outputs\", lines=14)\n",
        "\n",
        "    #  Critique ABOVE Final\n",
        "    crit = gr.Textbox(label=\"Critique\", lines=8)\n",
        "    final = gr.Textbox(label=\"Final Recommendation\", lines=10)\n",
        "\n",
        "    news_tbl = gr.Dataframe(\n",
        "        headers=[\"published_at\",\"source\",\"title\",\"summary\",\"url\",\"overall_sentiment\",\"tags\",\"numbers\"],\n",
        "        label=\"Top News (evidence)\",\n",
        "        wrap=True\n",
        "    )\n",
        "    prices_tbl = gr.Dataframe(label=\"Recent Prices (evidence)\")\n",
        "\n",
        "    # Outputs order: plan, agents, CRITIQUE, FINAL, news, prices\n",
        "    run_btn.click(\n",
        "        run,\n",
        "        inputs=[symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "a520",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
