{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e8e8972",
      "metadata": {
        "id": "8e8e8972"
      },
      "source": [
        "# Agentic Finance — All-in-One Notebook\n",
        "\n",
        "**This notebook is a demonstration notebook showing the full workflow and plan of our project.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;It summarizes the key components we developed across the Agentic Finance system, including data ingestion, analysis, orchestration, and user interface layers\n",
        "\n",
        "3 explicit workflow patterns\n",
        "-  5 working AI agents\n",
        "- Full data pipeline\n",
        "- Interactive UI\n",
        "- Professional code structure\n",
        "\n",
        "**To run the full interactive application, please use the Gradio app located at:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;/src/ui/gradio_app.py\n",
        "\n",
        "**Run it from the project root with:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;python -m ui.gradio_app\n",
        "\n",
        "**This will test the agents and see live results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kl2KAh90u7JO",
      "metadata": {
        "id": "kl2KAh90u7JO"
      },
      "source": [
        "# Market Research Agent — Price, Indicators, and News Pipeline\n",
        "\n",
        "This project builds a lightweight research workflow for equities: we fetch historical prices, compute common technical indicators (SMA, RSI), and pull headline summaries with basic sentiment. A small JSON cache keeps runs fast and reproducible while avoiding API rate limits. The notebook(s) walk through data loading, quick EDA, feature prep, and simple evaluation.\n",
        "\n",
        "**Course:** MSAAI 520-02 — Group 5  \n",
        "**Date:** October 18, 2025\n",
        "\n",
        "## Team\n",
        "- Ali Azizi  \n",
        "- Sunitha Kosireddy  \n",
        "- Victor Salcedo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "21f452ac",
      "metadata": {
        "id": "21f452ac",
        "outputId": "128d0883-843e-402b-9f2a-db342ace4189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root: /mnt/data/agentic-finance\n"
          ]
        }
      ],
      "source": [
        "# Setup: create package tree and write source files (no changes to your code)\n",
        "import pathlib, sys\n",
        "ROOT = pathlib.Path(\"/mnt/data/agentic-finance\")\n",
        "SRC = ROOT / \"src\"\n",
        "UI = ROOT / \"ui\"\n",
        "sys.path.insert(0, str(ROOT))\n",
        "print(\"Project root:\", ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bQuv9IKH90Q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb9ea1d-8314-4cbf-ab10-048a6ef8bcb9"
      },
      "id": "bQuv9IKH90Q_",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac59b6",
      "metadata": {
        "id": "1dac59b6"
      },
      "source": [
        "### Add repo root to sys.path and ensure packages exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ee2be1c7",
      "metadata": {
        "id": "ee2be1c7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Find repo root (folder containing \"src\")\n",
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"src\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "sys.path.insert(0, str(ROOT))  # make \"src\" importable\n",
        "\n",
        "# Ensure packages (empty __init__.py files)\n",
        "for p in [\n",
        "    ROOT / \"src\",\n",
        "    ROOT / \"src\" / \"config\",\n",
        "    ROOT / \"src\" / \"data_io\",\n",
        "    ROOT / \"src\" / \"system\",\n",
        "    ROOT / \"src\" / \"analysis\",\n",
        "\n",
        "\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    (p / \"__init__.py\").touch(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54efe4b1",
      "metadata": {
        "id": "54efe4b1"
      },
      "source": [
        "## src/analysis/features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "383f0b0b",
      "metadata": {
        "id": "383f0b0b"
      },
      "outputs": [],
      "source": [
        "# src/analysis/features.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_sma(prices: pd.DataFrame, window: int) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    return prices[\"close\"].rolling(window=window).mean()\n",
        "\n",
        "def compute_rsi(prices: pd.DataFrame, window: int = 14) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    delta = prices[\"close\"].diff()\n",
        "    gain = np.where(delta > 0, delta, 0.0)\n",
        "    loss = np.where(delta < 0, -delta, 0.0)\n",
        "    gain_s = pd.Series(gain, index=prices.index)\n",
        "    loss_s = pd.Series(loss, index=prices.index)\n",
        "    avg_gain = gain_s.rolling(window=window).mean()\n",
        "    avg_loss = loss_s.rolling(window=window).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99347f63",
      "metadata": {
        "id": "99347f63"
      },
      "source": [
        "## src/analysis/text.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "fdc78099",
      "metadata": {
        "id": "fdc78099"
      },
      "outputs": [],
      "source": [
        "# Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "# from config.settings import SETTINGS\n",
        "\n",
        "# -----------------------------\n",
        "# Existing tagging / preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "TAG_RULES = {\n",
        "    \"earnings\": [\"earnings\", \"eps\", \"guidance\", \"outlook\", \"quarter\", \"revenue\"],\n",
        "    \"product\":  [\"launch\", \"iphone\", \"chip\", \"feature\", \"service\"],\n",
        "    \"legal\":    [\"lawsuit\", \"regulator\", \"antitrust\", \"fine\", \"settlement\"],\n",
        "    \"macro\":    [\"inflation\", \"rates\", \"fed\", \"recession\", \"gdp\"]\n",
        "}\n",
        "\n",
        "def preprocess_news(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"published_at\",\"source\",\"title\",\"summary\",\"url\",\n",
        "            \"overall_sentiment\",\"tags\",\"numbers\"\n",
        "        ])\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Alpha Vantage format is like \"20251017T200143\"\n",
        "    # Parse with explicit format; keep timezone-aware for safety\n",
        "    df[\"published_at\"] = pd.to_datetime(\n",
        "        df[\"published_at\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\", utc=True\n",
        "    )\n",
        "\n",
        "    # Drop rows with no title/url; keep others (don’t drop NaT here — the date filter happens later)\n",
        "    df = df.dropna(subset=[\"title\",\"url\"]).drop_duplicates(subset=[\"url\"])\n",
        "    df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def classify_tags(text: str) -> list[str]:\n",
        "    text_l = text.lower()\n",
        "    tags = [k for k, kws in TAG_RULES.items() if any(kw in text_l for kw in kws)]\n",
        "    return tags or [\"general\"]\n",
        "\n",
        "NUM_RE = re.compile(r'(\\$?\\b\\d+(?:\\.\\d+)?%?)')\n",
        "\n",
        "def extract_numbers(text: str) -> list[str]:\n",
        "    return NUM_RE.findall(text or \"\")[:6]\n",
        "\n",
        "def add_tags_and_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    df[\"tags\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(classify_tags)\n",
        "    df[\"numbers\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(extract_numbers)\n",
        "    return df\n",
        "\n",
        "def recent_topk(df: pd.DataFrame, topk: int, days: int, required_tags: list[str] | None = None) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Make an aware UTC cutoff; df['published_at'] is already UTC-aware\n",
        "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n",
        "    f = df[df[\"published_at\"] >= cutoff]\n",
        "\n",
        "    if required_tags:\n",
        "        want = [t.strip().lower() for t in required_tags]\n",
        "        f_tags = f[f[\"tags\"].apply(lambda ts: any(t in [x.lower() for x in ts] for t in want))]\n",
        "        f = f_tags if not f_tags.empty else f\n",
        "\n",
        "    return f.sort_values(\"published_at\", ascending=False).head(topk)\n",
        "\n",
        "# -----------------------------\n",
        "# NEW: shared agent utilities\n",
        "# -----------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "def strip_code_fences(s: str) -> str:\n",
        "    \"\"\"Remove leading/trailing ``` blocks (optionally ```json).\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.IGNORECASE)\n",
        "\n",
        "def to_float(x, default: float = 0.0) -> float:\n",
        "    \"\"\"Best-effort conversion of model outputs or strings to float.\"\"\"\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            xs = x.strip().lower()\n",
        "            # map common words to numeric anchors\n",
        "            if xs in (\"high\", \"strong\", \"bullish\", \"overbought\"):\n",
        "                return 0.8\n",
        "            if xs in (\"medium\", \"moderate\", \"neutral\"):\n",
        "                return 0.5\n",
        "            if xs in (\"low\", \"weak\", \"bearish\", \"oversold\"):\n",
        "                return 0.2\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def clamp(x: float, lo: float, hi: float) -> float:\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "def normalize_score(v: float) -> float:\n",
        "    \"\"\"\n",
        "    Normalize arbitrary score ranges to [-1, 1].\n",
        "    Heuristics:\n",
        "      - If already in [-1,1], keep.\n",
        "      - If in [0,1], map to [-1,1] via (v-0.5)*2.\n",
        "      - If in (1,100], treat as percent.\n",
        "      - If in (1,10], treat as 0-10 and map.\n",
        "      - Else, clamp.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        v = float(v)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    if -1.0 <= v <= 1.0:\n",
        "        return v\n",
        "    if 0.0 <= v <= 1.0:\n",
        "        return (v - 0.5) * 2.0\n",
        "    if 1.0 < v <= 100.0:\n",
        "        v01 = v / 100.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    if 1.0 < v <= 10.0:\n",
        "        v01 = v / 10.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    return clamp(v, -1.0, 1.0)\n",
        "\n",
        "def normalize_conf(v) -> float:\n",
        "    \"\"\"Normalize any confidence-like value to [0,1].\"\"\"\n",
        "    f = to_float(v, 0.7)\n",
        "    if 1.0 < f <= 100.0:\n",
        "        f = f / 100.0\n",
        "    return clamp(f, 0.0, 1.0)\n",
        "\n",
        "# Optional: helpers to render structured dicts into strings (for external tools)\n",
        "def pretty_json_block(obj: dict, max_chars: int = 4000) -> str:\n",
        "    \"\"\"Return a fenced JSON markdown block, truncated for UI safety.\"\"\"\n",
        "    try:\n",
        "        js = json.dumps(obj, ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        js = str(obj)\n",
        "    if len(js) > max_chars:\n",
        "        js = js[: max_chars - 20] + \"\\n... (truncated)\"\n",
        "    return f\"```json\\n{js}\\n```\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a110bf",
      "metadata": {
        "id": "a2a110bf"
      },
      "source": [
        "## src/config/settings.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "74137240",
      "metadata": {
        "id": "74137240"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _find_project_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Walk upward to find the repo root heuristically.\n",
        "    Treat a folder containing both 'src' and 'data' as the root.\n",
        "    Fallback to the starting directory if not found.\n",
        "    \"\"\"\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# project root = repo root\n",
        "if \"__file__\" in globals():\n",
        "    ROOT = Path(__file__).resolve().parents[2]\n",
        "else:\n",
        "    # Notebook / REPL: start from CWD and auto-detect root\n",
        "    ROOT = _find_project_root(Path.cwd())\n",
        "\n",
        "load_dotenv(ROOT / \".env\", override=False)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Settings:\n",
        "    data_dir: Path = ROOT / \"data\"\n",
        "    cache_dir: Path = ROOT / \"data\" / \"cache\"\n",
        "    runs_dir: Path = ROOT / \"data\" / \"runs\"\n",
        "    alpha_api_key: str = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"BVGUKZR1MHVS0T6B\")\n",
        "    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"sk-proj-\")\n",
        "    news_window_days: int = 14\n",
        "    topk_news: int = 5\n",
        "    cache_ttl_minutes: int = 60\n",
        "\n",
        "SETTINGS = Settings()\n",
        "SETTINGS.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "SETTINGS.runs_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581069c6",
      "metadata": {
        "id": "581069c6"
      },
      "source": [
        "## src/data_io/cache.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XIKxbGY5r1ce",
      "metadata": {
        "id": "XIKxbGY5r1ce"
      },
      "source": [
        "### Simple JSON file cache (TTL + atomic writes)\n",
        "I use a tiny JSON-based cache so repeated runs don’t redo the same work. Each key maps to a file on disk, with a timestamp to support a time-to-live (TTL). Saving is done via a temp file + replace so partial writes don’t corrupt the cache.\n",
        "\n",
        "**Inputs:** `key`, optional `ttl_minutes`, arbitrary `data`  \n",
        "**Key choices:** per-key JSON files under `SETTINGS.cache_dir`, ISO-8601 for dates, atomic write on save  \n",
        "**Output:** `load_cache` returns cached payload or `None`; `save_cache` writes `{\"_ts\": ..., \"data\": ...}` to disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "6a42293c",
      "metadata": {
        "id": "6a42293c"
      },
      "outputs": [],
      "source": [
        "# Purpose: lightweight disk cache with TTL and atomic writes\n",
        "# Context: used by data fetchers (e.g., price downloads) to avoid repeat network calls\n",
        "# Notes: filenames derive from key under SETTINGS.cache_dir; payload stored as JSON\n",
        "\n",
        "# cache.py\n",
        "from __future__ import annotations\n",
        "import json, time\n",
        "from datetime import date, datetime\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def _cache_path(key: str) -> Path:\n",
        "    return SETTINGS.cache_dir / f\"{key}.json\"\n",
        "\n",
        "def _json_default(o: Any):\n",
        "    # datetime & pandas.Timestamp (subclass of datetime) → ISO 8601\n",
        "    if isinstance(o, (datetime, date)):\n",
        "        return o.isoformat()\n",
        "    # Fallback: make a best-effort string (covers Decimal, Path, Enum, etc.)\n",
        "    try:\n",
        "        return str(o)\n",
        "    except Exception:\n",
        "        return repr(o)\n",
        "\n",
        "def load_cache(key: str, ttl_minutes: int | None = None) -> Any | None:\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        if ttl_minutes is None:\n",
        "            return obj.get(\"data\")  # consistent: always return payload\n",
        "        if (time.time() - obj.get(\"_ts\", 0)) <= ttl_minutes * 60:\n",
        "            return obj.get(\"data\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def save_cache(key: str, data: Any) -> None:\n",
        "    p = _cache_path(key)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
        "    payload = {\"_ts\": time.time(), \"data\": data}\n",
        "    tmp.write_text(json.dumps(payload, ensure_ascii=False, default=_json_default), encoding=\"utf-8\")\n",
        "    tmp.replace(p)  # atomic on most OS/filesystems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ee3409",
      "metadata": {
        "id": "78ee3409"
      },
      "source": [
        "## src/data_io/prices.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vIV91U3JrnJa",
      "metadata": {
        "id": "vIV91U3JrnJa"
      },
      "source": [
        "### Fetch historical prices (with simple caching)\n",
        "I pull OHLCV data from Yahoo Finance and return a clean DataFrame. I use a short-lived cache so repeated runs don’t keep hitting the API. If data is already cached, I return it immediately. I also flatten any MultiIndex columns and standardize names so downstream code stays consistent.\n",
        "\n",
        "**Inputs:** `symbol`, `start`, `end`  \n",
        "**Key choices:** use `yfinance.download`, cache key includes date range, flatten MultiIndex, keep both `close` and `adj_close`  \n",
        "**Output:** DataFrame with `date, open, high, low, close, adj_close, volume`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "fb6014f6",
      "metadata": {
        "id": "fb6014f6"
      },
      "outputs": [],
      "source": [
        "# Purpose: download OHLCV from Yahoo Finance and return a normalized DataFrame with caching\n",
        "# Context: called by data prep steps before features/EDA; avoids repeated network calls\n",
        "# Notes: flattens MultiIndex cols, standardizes names, stores json-serializable cache\n",
        "\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_prices(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:\n",
        "    cache_key = f\"prices_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "    df = yf.download(symbol, start=start, end=end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0].lower() for c in df.columns]\n",
        "    df = df.reset_index().rename(columns={\n",
        "        \"Date\": \"date\", \"open\":\"open\",\"high\":\"high\",\"low\":\"low\",\"close\":\"close\",\"adj close\":\"adj_close\",\"volume\":\"volume\"\n",
        "    })\n",
        "    df[\"date\"] = df[\"date\"].astype(str)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a58f9b6",
      "metadata": {
        "id": "7a58f9b6"
      },
      "source": [
        "## src/data_io/indicators.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTEo0-OksEFN",
      "metadata": {
        "id": "UTEo0-OksEFN"
      },
      "source": [
        "### Fetch technical indicators (Alpha Vantage with local fallback)\n",
        "I get daily SMA/RSI for a symbol. I try Alpha Vantage first and fall back to computing the indicator locally from our price history when the key is missing or the API is rate-limited. Results are cached so repeated calls are fast.\n",
        "\n",
        "**Inputs:** `symbol`, `indicator` (`\"SMA\"` or `\"RSI\"`), `time_period`  \n",
        "**Key choices:** Alpha Vantage params (daily interval, series_type=close), JSON parsing with key map, local fallback uses `compute_sma`/`compute_rsi` on `fetch_prices`  \n",
        "**Output:** DataFrame with `date` and indicator column(s), sorted ascending\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "752d66ae",
      "metadata": {
        "id": "752d66ae"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch SMA/RSI via Alpha Vantage with a cached local-compute fallback\n",
        "# Context: used by feature pipelines that need daily indicators\n",
        "# Notes: caches by (symbol, indicator, time_period); normalizes dates and numeric types\n",
        "\n",
        "# src/data_io/indicators.py\n",
        "from __future__ import annotations\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.analysis.features import compute_sma, compute_rsi\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "KEYS = {\"SMA\": \"Technical Analysis: SMA\", \"RSI\": \"Technical Analysis: RSI\"}\n",
        "\n",
        "\n",
        "# If AV isn’t available (no key/limit), our code falls back to computing indicators locally from prices using our compute_sma / compute_rsi.\n",
        "def _fallback_from_prices(symbol: str, indicator: str, time_period: int) -> pd.DataFrame:\n",
        "    prices = fetch_prices(symbol, None, None)\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if indicator == \"SMA\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"SMA\": compute_sma(prices, window=time_period)})\n",
        "    elif indicator == \"RSI\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"RSI\": compute_rsi(prices, window=time_period)})\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna().sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def fetch_indicator(symbol: str, indicator: str, time_period: int = 14) -> pd.DataFrame:\n",
        "    key = KEYS.get(indicator)\n",
        "\n",
        "    # Try cache first\n",
        "    cache_key = f\"indicator_{symbol}_{indicator}_{time_period}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    if not SETTINGS.alpha_api_key or key is None:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    params = {\n",
        "        \"function\": indicator,\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": \"daily\",\n",
        "        \"time_period\": time_period,\n",
        "        \"series_type\": \"close\",\n",
        "        \"apikey\": SETTINGS.alpha_api_key,\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(BASE, params=params, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        # Alpha Vantage quota message handling:\n",
        "        if (not data or key not in data or not data[key] or \"Note\" in data or \"Information\" in data or \"Error Message\" in data):\n",
        "            df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "            save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "            return df\n",
        "    except Exception:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "    df.reset_index(inplace=True)\n",
        "    df = df.rename(columns={\"index\": \"date\"})\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30738873",
      "metadata": {
        "id": "30738873"
      },
      "source": [
        "## src/data_io/news.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iJxmkUXGsR3s",
      "metadata": {
        "id": "iJxmkUXGsR3s"
      },
      "source": [
        "### Fetch news (Alpha Vantage, symbol-filtered, cached)\n",
        "I pull recent news from Alpha Vantage’s `NEWS_SENTIMENT` endpoint and keep only items that explicitly mention my ticker with decent relevance. I cache the result so I don’t burn quota on repeat runs.\n",
        "\n",
        "**Inputs:** `symbol`  \n",
        "**Key choices:** `relevance_score >= 0.30`, require explicit ticker match, cache per symbol  \n",
        "**Output:** DataFrame with `published_at, source, title, summary, url, overall_sentiment`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "fc9bb3f1",
      "metadata": {
        "id": "fc9bb3f1"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch and cache symbol-specific news via Alpha Vantage, filtered by relevance\n",
        "# Context: called by downstream reporting/EDA to attach headlines and sentiment\n",
        "# Notes: filters to items where ticker matches and relevance >= 0.30; caches by symbol\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, requests, pandas as pd\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "\n",
        "def fetch_news(symbol: str) -> pd.DataFrame:\n",
        "    if not SETTINGS.alpha_api_key:\n",
        "        return pd.DataFrame()  # safe fail\n",
        "    cache_key = f\"news_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    params = {\"function\":\"NEWS_SENTIMENT\",\"tickers\":symbol,\"apikey\":SETTINGS.alpha_api_key}\n",
        "    r = requests.get(BASE, params=params, timeout=30)\n",
        "    data = r.json()\n",
        "    if \"feed\" not in data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for item in data.get(\"feed\", []):\n",
        "        tickers = item.get(\"ticker_sentiment\", []) or []\n",
        "        # keep only if our symbol is explicitly mentioned\n",
        "        keep = any(t.get(\"ticker\", \"\").upper() == symbol.upper() and float(t.get(\"relevance_score\", 0) or 0) >= 0.30\n",
        "                   for t in tickers)\n",
        "        if not keep:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"published_at\": item.get(\"time_published\"),\n",
        "            \"source\": item.get(\"source\"),\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"summary\": item.get(\"summary\"),\n",
        "            \"url\": item.get(\"url\"),\n",
        "            \"overall_sentiment\": item.get(\"overall_sentiment_label\")\n",
        "        })\n",
        "\n",
        "    # ====== Forth APPROACH =====\n",
        "    df = pd.DataFrame(rows)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc9c201",
      "metadata": {
        "id": "6cc9c201"
      },
      "source": [
        "## src/system/router.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "f1200bf3",
      "metadata": {
        "id": "f1200bf3"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "def choose_agents(has_news: bool, has_prices: bool, has_technicals: bool) -> list[str]:\n",
        "    agents = []\n",
        "    if has_news: agents.append(\"news\")\n",
        "    # earnings optional if you add a financials fetch later\n",
        "    if has_technicals and has_prices: agents.append(\"technical\")\n",
        "    agents.append(\"risk\")\n",
        "    return agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa0a509",
      "metadata": {
        "id": "afa0a509"
      },
      "source": [
        "## src/system/memory.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "b260d5e1",
      "metadata": {
        "id": "b260d5e1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "MEM_PATH = SETTINGS.runs_dir / \"run_notes.jsonl\"\n",
        "\n",
        "def append_memory(record: dict[str, Any]) -> None:\n",
        "    MEM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with MEM_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cba7d18",
      "metadata": {
        "id": "2cba7d18"
      },
      "source": [
        "## src/agents.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14963947",
      "metadata": {
        "id": "14963947"
      },
      "source": [
        "## src/system/orchestration.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "4a539fc5",
      "metadata": {
        "id": "4a539fc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======= 3Rd version of the orchestrator ==========\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.data_io.news import fetch_news\n",
        "from src.data_io.indicators import fetch_indicator\n",
        "from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "from src.system.router import choose_agents\n",
        "from src.system.memory import append_memory\n",
        "from src.agents import (\n",
        "    NewsAnalysisAgent,\n",
        "    MarketSignalsAgent,\n",
        "    RiskAssessmentAgent,\n",
        "    SynthesisAgent,\n",
        "    CritiqueAgent,\n",
        "    AgentResponse,\n",
        ")\n",
        "\n",
        "# from ..analysis.signals import normalize_technicals  # add this import\n",
        "\n",
        "import json\n",
        "\n",
        "def _as_text(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _as_list_of_text(x):\n",
        "    if isinstance(x, list):\n",
        "        return [_as_text(i) for i in x]\n",
        "    if x is None:\n",
        "        return []\n",
        "    # if a single str/dict was returned, wrap it\n",
        "    return [_as_text(x)]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OrchestratorResult:\n",
        "    plan: List[str]\n",
        "    evidence: Dict[str, DataFrame]\n",
        "    agent_outputs: List[AgentResponse]\n",
        "    final: AgentResponse\n",
        "    critique: AgentResponse\n",
        "\n",
        "\n",
        "def run_pipeline(symbol: str, start: str | None, end: str | None,\n",
        "                 required_tags: list[str] | None = None) -> OrchestratorResult:\n",
        "    plan = [\n",
        "        \"fetch_prices\", \"fetch_news\", \"preprocess\", \"classify_extract\",\n",
        "        \"retrieve_topk\", \"route\", \"run_agents\", \"synthesize\", \"critique\", \"save_memory\"\n",
        "    ]\n",
        "\n",
        "    # 1) fetch\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    news   = fetch_news(symbol)\n",
        "\n",
        "    # 2) preprocess (clean + tags + lightweight numeric extraction)\n",
        "    news_pp = add_tags_and_numbers(preprocess_news(news))\n",
        "\n",
        "    # 3) retrieval (deterministic)\n",
        "    top_news = recent_topk(\n",
        "        news_pp,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags\n",
        "    )\n",
        "\n",
        "    # 4) route\n",
        "    has_news   = not top_news.empty\n",
        "    has_prices = not prices.empty\n",
        "\n",
        "    # Technical indicators (fetch what we can; leave the rest as None)\n",
        "    rsi = fetch_indicator(symbol, \"RSI\", 14)         # may be empty\n",
        "    sma20 = fetch_indicator(symbol, \"SMA\", 20)       # may be empty\n",
        "    # Optional (if your fetch_indicator supports these; otherwise they'll be empty DataFrames)\n",
        "    sma50 = fetch_indicator(symbol, \"SMA\", 50)\n",
        "    sma200 = fetch_indicator(symbol, \"SMA\", 200)\n",
        "\n",
        "    has_technicals = (not rsi.empty) or (not sma20.empty) or (not sma50.empty) or (not sma200.empty)\n",
        "    lanes = choose_agents(has_news, has_prices, has_technicals)\n",
        "\n",
        "    # 5) run agents\n",
        "    outputs: List[AgentResponse] = []\n",
        "\n",
        "    # NEWS (delegate summarization to the agent)\n",
        "    if \"news\" in lanes and has_news:\n",
        "        # Teammate agent expects `description`; remap our `summary` to `description`\n",
        "        news_payload_records = (\n",
        "            top_news\n",
        "            .rename(columns={\"summary\": \"description\"})\n",
        "            .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "            .to_dict(orient=\"records\")\n",
        "        )\n",
        "        news_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"news\": news_payload_records\n",
        "        }\n",
        "        outputs.append(NewsAnalysisAgent().process(news_payload))\n",
        "\n",
        "    # TECHNICALS (build dict the agent expects; missing fields are fine)\n",
        "    if \"technical\" in lanes and (has_technicals or has_prices):\n",
        "        current_price = float(prices[\"close\"].iloc[-1]) if has_prices else None\n",
        "        volume = int(prices[\"volume\"].iloc[-1]) if has_prices else None\n",
        "        avg_volume = int(prices[\"volume\"].tail(20).mean()) if has_prices else None\n",
        "\n",
        "        technicals = {\n",
        "            \"current_price\": current_price,\n",
        "            \"rsi\": (float(rsi[\"RSI\"].iloc[-1]) if not rsi.empty else None),\n",
        "            # Map to teammate’s keys; if we don’t have these, leave None.\n",
        "            \"ma_50\": (float(sma50[\"SMA\"].iloc[-1]) if not sma50.empty else (float(sma20[\"SMA\"].iloc[-1]) if not sma20.empty else None)),\n",
        "            \"ma_200\": (float(sma200[\"SMA\"].iloc[-1]) if not sma200.empty else None),\n",
        "            \"macd\": None,                # not computed here\n",
        "            \"volume\": volume,\n",
        "            \"avg_volume\": avg_volume,\n",
        "            \"support\": None,\n",
        "            \"resistance\": None,\n",
        "        }\n",
        "        tech_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"technicals\": technicals\n",
        "        }\n",
        "        outputs.append(MarketSignalsAgent().process(tech_payload))\n",
        "\n",
        "\n",
        "    # RISK (simple proxy; fill what you have)\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if has_prices else None\n",
        "    risk_payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\": None,\n",
        "            \"volatility\": vol_30d,      # teammate agent expects 'volatility' (%)\n",
        "            \"var_5\": None,\n",
        "            \"sharpe_ratio\": None,\n",
        "            \"max_drawdown\": None,\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\": None\n",
        "        }\n",
        "    }\n",
        "    outputs.append(RiskAssessmentAgent().process(risk_payload))\n",
        "\n",
        "    # 6) synthesize + critique (gated second pass)\n",
        "    synth_v1 = SynthesisAgent().process(outputs)\n",
        "    crit     = CritiqueAgent().process(synth_v1)\n",
        "\n",
        "    # Decide if we need a second pass.\n",
        "    # Rule: re-run if critique quality score < 0.90 OR if it mentions \"data quality\".\n",
        "    needs_rerun = (crit.score < 0.90) or (\"data quality\" in \" \".join(crit.key_factors).lower())\n",
        "\n",
        "    synth_final = synth_v1\n",
        "\n",
        "    if needs_rerun:\n",
        "        # Turn critique into feedback for the optimizer pass\n",
        "        critique_feedback = AgentResponse(\n",
        "            agent_name=\"Critique Feedback\",\n",
        "            analysis=_as_text(synth_v1.analysis) + \"\\n\\n[CRITIQUE]\\n\" + _as_text(crit.analysis),\n",
        "            score=crit.score,\n",
        "            confidence=crit.confidence,\n",
        "            key_factors=_as_list_of_text(crit.key_factors),\n",
        "            timestamp=datetime.now().isoformat()  # ← FIXED\n",
        "        )\n",
        "\n",
        "        synth_v2_inputs = outputs + [critique_feedback]\n",
        "        synth_v2 = SynthesisAgent().process(synth_v2_inputs)\n",
        "        synth_final = synth_v2\n",
        "\n",
        "    # 7) memory (store both passes where applicable)\n",
        "    append_memory({\n",
        "        \"ticker\": symbol,\n",
        "        \"lanes\": lanes,\n",
        "        \"issues\": crit.key_factors,\n",
        "        \"final_confidence_v1\": synth_v1.confidence,\n",
        "        \"final_confidence_v2\": synth_final.confidence if needs_rerun else None,\n",
        "        \"optimizer_triggered\": bool(needs_rerun),\n",
        "        \"timestamp\": datetime.now().isoformat()  # ← FIXED\n",
        "})\n",
        "\n",
        "    # 8) evidence for UI\n",
        "    evidence = {\n",
        "        \"top_news\": top_news,\n",
        "        \"prices_tail\": prices.tail(5)\n",
        "    }\n",
        "\n",
        "    # Show the first synthesis alongside other agents for transparency\n",
        "\n",
        "    outputs.append(AgentResponse(\n",
        "        agent_name=\"Initial Synthesis\",\n",
        "        analysis=_as_text(synth_v1.analysis),\n",
        "        score=float(synth_v1.score),\n",
        "        confidence=float(synth_v1.confidence),\n",
        "        key_factors=_as_list_of_text(synth_v1.key_factors),\n",
        "        timestamp=synth_v1.timestamp\n",
        "))\n",
        "\n",
        "    return OrchestratorResult(plan, evidence, outputs, synth_final, crit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute in parallel\n",
        "# ======= 3Rd version of the orchestrator ==========\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.data_io.news import fetch_news\n",
        "from src.data_io.indicators import fetch_indicator\n",
        "from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "from src.system.router import choose_agents\n",
        "from src.system.memory import append_memory\n",
        "from src.agents import (\n",
        "    NewsAnalysisAgent,\n",
        "    MarketSignalsAgent,\n",
        "    RiskAssessmentAgent,\n",
        "    SynthesisAgent,\n",
        "    CritiqueAgent,\n",
        "    AgentResponse,\n",
        ")\n",
        "# from ..analysis.signals import normalize_technicals  # add this import\n",
        "\n",
        "import json\n",
        "\n",
        "def _as_text(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _as_list_of_text(x):\n",
        "    if isinstance(x, list):\n",
        "        return [_as_text(i) for i in x]\n",
        "    if x is None:\n",
        "        return []\n",
        "    # if a single str/dict was returned, wrap it\n",
        "    return [_as_text(x)]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OrchestratorResult:\n",
        "    plan: List[str]\n",
        "    evidence: Dict[str, DataFrame]\n",
        "    agent_outputs: List[AgentResponse]\n",
        "    final: AgentResponse\n",
        "    critique: AgentResponse\n",
        "\n",
        "\n",
        "def run_pipeline(symbol: str, start: str | None, end: str | None,\n",
        "                 required_tags: list[str] | None = None) -> OrchestratorResult:\n",
        "    \"\"\"\n",
        "    Main orchestration pipeline for financial analysis.\n",
        "\n",
        "    Coordinates multiple specialized agents to analyze a stock through:\n",
        "    1. Data ingestion (prices, news, technical indicators)\n",
        "    2. Preprocessing and classification\n",
        "    3. Routing to appropriate agents\n",
        "    4. Synthesis of findings\n",
        "    5. Quality critique and optional re-synthesis\n",
        "\n",
        "    This function implements the Evaluator-Optimizer pattern internally.\n",
        "    \"\"\"\n",
        "    plan = [\n",
        "        \"fetch_prices\", \"fetch_news\", \"preprocess\", \"classify_extract\",\n",
        "        \"retrieve_topk\", \"route\", \"run_agents\", \"synthesize\", \"critique\", \"save_memory\"\n",
        "    ]\n",
        "\n",
        "    # 1) fetch\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    news   = fetch_news(symbol)\n",
        "\n",
        "    # 2) preprocess (clean + tags + lightweight numeric extraction)\n",
        "    news_pp = add_tags_and_numbers(preprocess_news(news))\n",
        "\n",
        "    # 3) retrieval (deterministic)\n",
        "    top_news = recent_topk(\n",
        "        news_pp,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags\n",
        "    )\n",
        "\n",
        "    # 4) route\n",
        "    has_news   = not top_news.empty\n",
        "    has_prices = not prices.empty\n",
        "\n",
        "    # Technical indicators (fetch what we can; leave the rest as None)\n",
        "    rsi = fetch_indicator(symbol, \"RSI\", 14)         # may be empty\n",
        "    sma20 = fetch_indicator(symbol, \"SMA\", 20)       # may be empty\n",
        "    # Optional (if your fetch_indicator supports these; otherwise they'll be empty DataFrames)\n",
        "    sma50 = fetch_indicator(symbol, \"SMA\", 50)\n",
        "    sma200 = fetch_indicator(symbol, \"SMA\", 200)\n",
        "\n",
        "    has_technicals = (not rsi.empty) or (not sma20.empty) or (not sma50.empty) or (not sma200.empty)\n",
        "    lanes = choose_agents(has_news, has_prices, has_technicals)\n",
        "\n",
        "    # 5) run agents\n",
        "    outputs: List[AgentResponse] = []\n",
        "\n",
        "    # NEWS (delegate summarization to the agent)\n",
        "    if \"news\" in lanes and has_news:\n",
        "        # Teammate agent expects `description`; remap our `summary` to `description`\n",
        "        news_payload_records = (\n",
        "            top_news\n",
        "            .rename(columns={\"summary\": \"description\"})\n",
        "            .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "            .to_dict(orient=\"records\")\n",
        "        )\n",
        "        news_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"news\": news_payload_records\n",
        "        }\n",
        "        outputs.append(NewsAnalysisAgent().process(news_payload))\n",
        "\n",
        "    # TECHNICALS (build dict the agent expects; missing fields are fine)\n",
        "    if \"technical\" in lanes and (has_technicals or has_prices):\n",
        "        current_price = float(prices[\"close\"].iloc[-1]) if has_prices else None\n",
        "        volume = int(prices[\"volume\"].iloc[-1]) if has_prices else None\n",
        "        avg_volume = int(prices[\"volume\"].tail(20).mean()) if has_prices else None\n",
        "\n",
        "        technicals = {\n",
        "            \"current_price\": current_price,\n",
        "            \"rsi\": (float(rsi[\"RSI\"].iloc[-1]) if not rsi.empty else None),\n",
        "            # Map to teammate's keys; if we don't have these, leave None.\n",
        "            \"ma_50\": (float(sma50[\"SMA\"].iloc[-1]) if not sma50.empty else (float(sma20[\"SMA\"].iloc[-1]) if not sma20.empty else None)),\n",
        "            \"ma_200\": (float(sma200[\"SMA\"].iloc[-1]) if not sma200.empty else None),\n",
        "            \"macd\": None,                # not computed here\n",
        "            \"volume\": volume,\n",
        "            \"avg_volume\": avg_volume,\n",
        "            \"support\": None,\n",
        "            \"resistance\": None,\n",
        "        }\n",
        "        tech_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"technicals\": technicals\n",
        "        }\n",
        "        outputs.append(MarketSignalsAgent().process(tech_payload))\n",
        "\n",
        "\n",
        "    # RISK (simple proxy; fill what you have)\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if has_prices else None\n",
        "    risk_payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\": None,\n",
        "            \"volatility\": vol_30d,      # teammate agent expects 'volatility' (%)\n",
        "            \"var_5\": None,\n",
        "            \"sharpe_ratio\": None,\n",
        "            \"max_drawdown\": None,\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\": None\n",
        "        }\n",
        "    }\n",
        "    outputs.append(RiskAssessmentAgent().process(risk_payload))\n",
        "\n",
        "    # 6) synthesize + critique (gated second pass)\n",
        "    synth_v1 = SynthesisAgent().process(outputs)\n",
        "    crit     = CritiqueAgent().process(synth_v1)\n",
        "\n",
        "    # Decide if we need a second pass.\n",
        "    # Rule: re-run if critique quality score < 0.90 OR if it mentions \"data quality\".\n",
        "    needs_rerun = (crit.score < 0.90) or (\"data quality\" in \" \".join(crit.key_factors).lower())\n",
        "\n",
        "    synth_final = synth_v1\n",
        "    if needs_rerun:\n",
        "        # Turn critique into feedback for the optimizer pass\n",
        "        critique_feedback = AgentResponse(\n",
        "            agent_name=\"Critique Feedback\",\n",
        "            analysis=_as_text(synth_v1.analysis) + \"\\n\\n[CRITIQUE]\\n\" + _as_text(crit.analysis),\n",
        "            score=crit.score,\n",
        "            confidence=crit.confidence,\n",
        "            key_factors=_as_list_of_text(crit.key_factors),\n",
        "            timestamp=datetime.now().isoformat()\n",
        "\n",
        "        )\n",
        "\n",
        "        synth_v2_inputs = outputs + [critique_feedback]\n",
        "        synth_v2 = SynthesisAgent().process(synth_v2_inputs)\n",
        "        synth_final = synth_v2\n",
        "\n",
        "    # 7) memory (store both passes where applicable)\n",
        "    append_memory({\n",
        "        \"ticker\": symbol,\n",
        "        \"lanes\": lanes,\n",
        "        \"issues\": crit.key_factors,\n",
        "        \"final_confidence_v1\": synth_v1.confidence,\n",
        "        \"final_confidence_v2\": synth_final.confidence if needs_rerun else None,\n",
        "        \"optimizer_triggered\": bool(needs_rerun),\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    })\n",
        "\n",
        "    # 8) evidence for UI\n",
        "    evidence = {\n",
        "        \"top_news\": top_news,\n",
        "        \"prices_tail\": prices.tail(5)\n",
        "    }\n",
        "\n",
        "    # Show the first synthesis alongside other agents for transparency\n",
        "    outputs.append(AgentResponse(\n",
        "        agent_name=\"Initial Synthesis\",\n",
        "        analysis=_as_text(synth_v1.analysis),\n",
        "        score=float(synth_v1.score),\n",
        "        confidence=float(synth_v1.confidence),\n",
        "        key_factors=_as_list_of_text(synth_v1.key_factors),\n",
        "        timestamp=synth_v1.timestamp\n",
        "    ))\n",
        "\n",
        "    return OrchestratorResult(plan, evidence, outputs, synth_final, crit)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# WORKFLOW PATTERN DEMONSTRATIONS\n",
        "# ============================================================================\n",
        "\n",
        "def run_prompt_chaining_workflow(symbol: str, start: str, end: str,\n",
        "                                  required_tags: list[str] | None = None) -> AgentResponse:\n",
        "    \"\"\"\n",
        "    WORKFLOW PATTERN 1: PROMPT CHAINING\n",
        "\n",
        "    Demonstrates sequential processing where each step's output becomes\n",
        "    the input to the next step, creating a progressive refinement chain.\n",
        "\n",
        "    Pipeline: Ingest → Preprocess → Classify → Extract → Summarize\n",
        "\n",
        "    Args:\n",
        "        symbol: Stock ticker (e.g., 'AAPL')\n",
        "        start: Start date (YYYY-MM-DD)\n",
        "        end: End date (YYYY-MM-DD)\n",
        "        required_tags: Optional tag filter\n",
        "\n",
        "    Returns:\n",
        "        AgentResponse with sentiment analysis\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WORKFLOW PATTERN 1: PROMPT CHAINING\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} to {end}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: INGEST\n",
        "    print(\"\\n┌─ STEP 1/5: INGEST ─────────────────────────────────────────────┐\")\n",
        "    print(\"│ Fetching raw news from Alpha Vantage API...                    │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    raw_news = fetch_news(symbol)\n",
        "\n",
        "    if raw_news.empty:\n",
        "        print(\"  No news data available\")\n",
        "        return AgentResponse(\n",
        "            agent_name=\"News Analysis Agent\",\n",
        "            analysis=\"No news data available\",\n",
        "            score=0.0,\n",
        "            confidence=0.0,\n",
        "            key_factors=[\"No data\"],\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    print(f\"  ✓ Fetched: {len(raw_news)} articles\")\n",
        "\n",
        "    # Step 2: PREPROCESS\n",
        "    print(\"\\n┌─ STEP 2/5: PREPROCESS ─────────────────────────────────────────┐\")\n",
        "    print(\"│ Cleaning, parsing dates, removing duplicates...                │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    clean_news = preprocess_news(raw_news)\n",
        "    print(f\"  ✓ Cleaned: {len(clean_news)} articles\")\n",
        "\n",
        "    # Step 3: CLASSIFY\n",
        "    print(\"\\n┌─ STEP 3/5: CLASSIFY ───────────────────────────────────────────┐\")\n",
        "    print(\"│ Tagging by topic, extracting numbers...                        │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    tagged_news = add_tags_and_numbers(clean_news)\n",
        "    print(f\"  ✓ Tagged: {len(tagged_news)} articles\")\n",
        "\n",
        "    # Step 4: EXTRACT\n",
        "    print(\"\\n┌─ STEP 4/5: EXTRACT ────────────────────────────────────────────┐\")\n",
        "    print(\"│ Filtering to top-K most relevant...                            │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    top_news = recent_topk(tagged_news, topk=SETTINGS.topk_news, days=SETTINGS.news_window_days, required_tags=required_tags)\n",
        "    print(f\"  ✓ Extracted: {len(top_news)} top articles\")\n",
        "\n",
        "    # Step 5: SUMMARIZE\n",
        "    print(\"\\n┌─ STEP 5/5: SUMMARIZE ──────────────────────────────────────────┐\")\n",
        "    print(\"│ LLM agent analyzing sentiment...                               │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    news_payload = {\n",
        "        'ticker': symbol,\n",
        "        'news': top_news.rename(columns={\"summary\": \"description\"}).to_dict('records') if not top_news.empty else []\n",
        "    }\n",
        "    result = NewsAnalysisAgent().process(news_payload)\n",
        "    print(f\"  ✓ Score: {result.score:+.2f} | Confidence: {result.confidence:.0%}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PROMPT CHAINING COMPLETE\")\n",
        "    print(\"Pattern: Raw Data → Clean → Tagged → Filtered → Analysis\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_parallel_workflow(symbol: str, start: str, end: str) -> List[AgentResponse]:\n",
        "    \"\"\"\n",
        "    WORKFLOW PATTERN 2: PARALLEL EXECUTION\n",
        "\n",
        "    Demonstrates concurrent execution of multiple agents for improved performance.\n",
        "    Instead of sequential (A→B→C), runs simultaneously (A+B+C).\n",
        "\n",
        "    Args:\n",
        "        symbol: Stock ticker\n",
        "        start: Start date\n",
        "        end: End date\n",
        "\n",
        "    Returns:\n",
        "        List of AgentResponses from concurrent execution\n",
        "    \"\"\"\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "    import time\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WORKFLOW PATTERN 2: PARALLEL EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Analyzing: {symbol}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\n[Preparation] Fetching data...\")\n",
        "    news = fetch_news(symbol)\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "\n",
        "    # Prepare inputs\n",
        "    news_input = {\n",
        "        'ticker': symbol,\n",
        "        'news': news.head(5).rename(columns={\"summary\": \"description\"}).to_dict('records') if not news.empty else []\n",
        "    }\n",
        "\n",
        "    tech_input = {\n",
        "        'ticker': symbol,\n",
        "        'technicals': {\n",
        "            'current_price': float(prices['close'].iloc[-1]) if not prices.empty else None,\n",
        "            'volume': int(prices['volume'].iloc[-1]) if not prices.empty else None,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    risk_input = {\n",
        "        'ticker': symbol,\n",
        "        'risk_metrics': {\n",
        "            'volatility': float(prices[\"close\"].pct_change().tail(30).std() * 100) if not prices.empty else None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n[Parallel] Running 3 agents concurrently...\")\n",
        "    print(\"  Sequential would be: Agent1 → Agent2 → Agent3\")\n",
        "    print(\"  Parallel runs: Agent1 + Agent2 + Agent3 simultaneously\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Execute in parallel\n",
        "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        futures = {\n",
        "            'news': executor.submit(NewsAnalysisAgent().process, news_input),\n",
        "            'technical': executor.submit(MarketSignalsAgent().process, tech_input),\n",
        "            'risk': executor.submit(RiskAssessmentAgent().process, risk_input)\n",
        "        }\n",
        "\n",
        "        results = {}\n",
        "        for name, future in futures.items():\n",
        "            results[name] = future.result()\n",
        "            print(f\"   {name.capitalize()}: Score={results[name].score:+.2f}\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"PARALLEL EXECUTION COMPLETE ({elapsed:.2f}s)\")\n",
        "    print(f\"Speedup: ~2-3x faster than sequential execution\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return list(results.values())\n",
        "\n",
        "\n",
        "def run_evaluator_optimizer_workflow(symbol: str, start: str, end: str,\n",
        "                                      required_tags: list[str] | None = None) -> OrchestratorResult:\n",
        "    \"\"\"\n",
        "    WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER (Self-Improving)\n",
        "\n",
        "    Demonstrates adaptive refinement through Generate → Evaluate → Optimize loop.\n",
        "    System critiques its own output and re-generates if quality is below threshold.\n",
        "\n",
        "    This wraps the existing run_pipeline() to explicitly demonstrate the pattern.\n",
        "\n",
        "    Args:\n",
        "        symbol: Stock ticker\n",
        "        start: Start date\n",
        "        end: End date\n",
        "        required_tags: Optional tag filter\n",
        "\n",
        "    Returns:\n",
        "        OrchestratorResult with potentially optimized synthesis\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Analyzing: {symbol}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # GENERATE\n",
        "    print(\"\\n[Phase 1] GENERATE: Running initial analysis...\")\n",
        "    result = run_pipeline(symbol, start, end, required_tags)\n",
        "\n",
        "    initial_synthesis = next(\n",
        "        (a for a in result.agent_outputs if \"Initial Synthesis\" in a.agent_name),\n",
        "        None\n",
        "    )\n",
        "\n",
        "    if initial_synthesis:\n",
        "        print(f\"  Initial: Score={initial_synthesis.score:+.2f}, Conf={initial_synthesis.confidence:.0%}\")\n",
        "\n",
        "    # EVALUATE\n",
        "    print(f\"\\n[Phase 2] EVALUATE: Critiquing quality...\")\n",
        "    print(f\"  Quality Score: {result.critique.score:.2f}\")\n",
        "    print(f\"  Issues Found: {len(result.critique.key_factors)}\")\n",
        "\n",
        "    # Check if optimizer ran\n",
        "    optimizer_triggered = initial_synthesis and (initial_synthesis.analysis != result.final.analysis)\n",
        "\n",
        "    # OPTIMIZE\n",
        "    if optimizer_triggered:\n",
        "        print(f\"\\n[Phase 3] OPTIMIZE: Re-synthesizing with feedback...\")\n",
        "        print(f\"  Final: Score={result.final.score:+.2f}, Conf={result.final.confidence:.0%}\")\n",
        "\n",
        "        conf_change = result.final.confidence - initial_synthesis.confidence\n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(f\"OPTIMIZATION SUCCESSFUL\")\n",
        "        print(f\"Confidence Change: {conf_change:+.0%} | Optimizer: YES\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "    else:\n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(f\"INITIAL QUALITY ACCEPTABLE\")\n",
        "        print(f\"Quality: {result.critique.score:.2f} | Optimizer: NO\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DEMONSTRATION RUNNER\n",
        "# ============================================================================\n",
        "\n",
        "def demo_all_workflows(symbol: str = \"AAPL\"):\n",
        "    \"\"\"\n",
        "    Run all three workflow patterns for demonstration purposes.\n",
        "\n",
        "    This function is designed to be called from a notebook or script\n",
        "    to show how the system implements different orchestration strategies.\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    start = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "    end = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    print(\"\\n\" + \"*\"*40)\n",
        "    print(\"  DEMONSTRATING 3 AGENTIC WORKFLOW PATTERNS\")\n",
        "    print(\"*\"*40)\n",
        "    print(f\"\\nTicker: {symbol}\")\n",
        "    print(f\"Date Range: {start} to {end}\\n\")\n",
        "\n",
        "    # Pattern 1\n",
        "    result1 = run_prompt_chaining_workflow(symbol, start, end)\n",
        "\n",
        "    # Pattern 2\n",
        "    result2 = run_parallel_workflow(symbol, start, end)\n",
        "\n",
        "    # Pattern 3\n",
        "    result3 = run_evaluator_optimizer_workflow(symbol, start, end)\n",
        "\n",
        "    print(\"\\n\" + \"#\"*40)\n",
        "    print(\"  ALL 3 WORKFLOW PATTERNS DEMONSTRATED\")\n",
        "    print(\"#\"*40 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        'prompt_chaining': result1,\n",
        "        'parallel': result2,\n",
        "        'evaluator_optimizer': result3\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run demonstration when file is executed directly\n",
        "    demo_all_workflows(\"AAPL\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6DRp1gSHKSd",
        "outputId": "dcd877fb-5167-4c31-f822-dd7f35cf0be5"
      },
      "id": "_6DRp1gSHKSd",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "****************************************\n",
            "  DEMONSTRATING 3 AGENTIC WORKFLOW PATTERNS\n",
            "****************************************\n",
            "\n",
            "Ticker: AAPL\n",
            "Date Range: 2025-09-19 to 2025-10-19\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 1: PROMPT CHAINING\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-19 to 2025-10-19\n",
            "================================================================================\n",
            "\n",
            "┌─ STEP 1/5: INGEST ─────────────────────────────────────────────┐\n",
            "│ Fetching raw news from Alpha Vantage API...                    │\n",
            "└─────────────────────────────────────────────────────────────────┘\n",
            "  No news data available\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 2: PARALLEL EXECUTION\n",
            "================================================================================\n",
            "Analyzing: AAPL\n",
            "================================================================================\n",
            "\n",
            "[Preparation] Fetching data...\n",
            "\n",
            "[Parallel] Running 3 agents concurrently...\n",
            "  Sequential would be: Agent1 → Agent2 → Agent3\n",
            "  Parallel runs: Agent1 + Agent2 + Agent3 simultaneously\n",
            "   News: Score=+0.00\n",
            "   Technical: Score=+0.00\n",
            "   Risk: Score=+0.50\n",
            "\n",
            "================================================================================\n",
            "PARALLEL EXECUTION COMPLETE (0.00s)\n",
            "Speedup: ~2-3x faster than sequential execution\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER\n",
            "================================================================================\n",
            "Analyzing: AAPL\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] GENERATE: Running initial analysis...\n",
            "  Initial: Score=+0.00, Conf=70%\n",
            "\n",
            "[Phase 2] EVALUATE: Critiquing quality...\n",
            "  Quality Score: 0.70\n",
            "  Issues Found: 1\n",
            "\n",
            "================================================================================\n",
            "INITIAL QUALITY ACCEPTABLE\n",
            "Quality: 0.70 | Optimizer: NO\n",
            "================================================================================\n",
            "\n",
            "\n",
            "########################################\n",
            "  ALL 3 WORKFLOW PATTERNS DEMONSTRATED\n",
            "########################################\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f913beb",
      "metadata": {
        "id": "9f913beb"
      },
      "source": [
        "## ui/gradio_app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "346a0a53",
      "metadata": {
        "id": "346a0a53",
        "outputId": "afa5b852-c99d-46c5-c189-30b931f2e8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7dc5ab8312851ff8d5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7dc5ab8312851ff8d5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Works in both scripts and notebooks:\n",
        "HERE = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
        "ROOT = (HERE / \"..\").resolve()\n",
        "sys.path.append(str(ROOT))\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "from src.system.orchestrator import run_pipeline\n",
        "\n",
        "\n",
        "def run(symbol, days_back, required_tags_csv):\n",
        "    import json\n",
        "    import pandas as pd\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    def as_text(x):\n",
        "        if x is None:\n",
        "            return \"\"\n",
        "        if isinstance(x, str):\n",
        "            return x\n",
        "        if isinstance(x, (dict, list)):\n",
        "            # pretty JSON for readability / stable comparisons\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2, sort_keys=True)\n",
        "        return str(x)\n",
        "\n",
        "    def clean(s: str) -> str:\n",
        "        # normalize whitespace and strip code fences if any slipped through\n",
        "        if not isinstance(s, str):\n",
        "            s = as_text(s)\n",
        "        s = s.strip()\n",
        "        if s.startswith(\"```\"):\n",
        "            s = s.strip(\"`\").strip()\n",
        "        return s\n",
        "\n",
        "    def synth_to_prose(obj):\n",
        "        # If it's already text, just return normalized text\n",
        "        if not isinstance(obj, dict):\n",
        "            return clean(as_text(obj))\n",
        "\n",
        "        parts = []\n",
        "\n",
        "        ms = obj.get(\"market_signals\") or {}\n",
        "        if ms:\n",
        "            ms_bits = []\n",
        "            cp = ms.get(\"current_price\")\n",
        "            if isinstance(cp, (int, float)):\n",
        "                ms_bits.append(f\"price ${cp:,.2f}\")\n",
        "            ma = ms.get(\"moving_averages\") or {}\n",
        "            ma50 = ma.get(\"50_day\")\n",
        "            ma200 = ma.get(\"200_day\")\n",
        "            if ma50 is not None or ma200 is not None:\n",
        "                ms_bits.append(f\"vs 50D {ma50}, 200D {ma200}\")\n",
        "            rsi = ms.get(\"RSI\")\n",
        "            if rsi is not None:\n",
        "                ms_bits.append(f\"RSI {rsi}\")\n",
        "            trend = ms.get(\"trend\")\n",
        "            if trend:\n",
        "                ms_bits.append(trend)\n",
        "            vol = ms.get(\"volume\") or {}\n",
        "            vcur, vavg = vol.get(\"current\"), vol.get(\"average\")\n",
        "            if vcur is not None and vavg is not None:\n",
        "                ms_bits.append(f\"volume {vcur:,} vs avg {vavg:,}\")\n",
        "            if ms_bits:\n",
        "                parts.append(\"Technicals: \" + \", \".join(str(x) for x in ms_bits if x))\n",
        "\n",
        "        news = obj.get(\"news\") or {}\n",
        "        if news:\n",
        "            # keep order stable-ish\n",
        "            news_bits = []\n",
        "            for k in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                if k in news:\n",
        "                    news_bits.append(f\"{k}: {news[k]}\")\n",
        "            # include any other keys if present\n",
        "            for k, v in news.items():\n",
        "                if k not in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                    news_bits.append(f\"{k}: {v}\")\n",
        "            parts.append(\"News: \" + \"; \".join(news_bits))\n",
        "\n",
        "        risk = obj.get(\"risk_assessment\") or {}\n",
        "        if risk:\n",
        "            risk_bits = []\n",
        "            for k in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                if k in risk:\n",
        "                    risk_bits.append(f\"{k}: {v}\")\n",
        "            for k, v in risk.items():\n",
        "                if k not in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                    risk_bits.append(f\"{k}: {v}\")\n",
        "            parts.append(\"Risk: \" + \"; \".join(risk_bits))\n",
        "\n",
        "        return \"\\n\".join(parts).strip()\n",
        "\n",
        "    def to_df(x):\n",
        "        if isinstance(x, pd.DataFrame):\n",
        "            return x\n",
        "        if x is None:\n",
        "            return pd.DataFrame()\n",
        "        try:\n",
        "            return pd.DataFrame(x)\n",
        "        except Exception:\n",
        "            return pd.DataFrame()\n",
        "    # ---------- /helpers ----------\n",
        "\n",
        "    start = (date.today() - timedelta(days=int(days_back))).isoformat()\n",
        "    end = date.today().isoformat()\n",
        "    tags = [t.strip() for t in required_tags_csv.split(\",\")] if required_tags_csv else None\n",
        "\n",
        "    res = run_pipeline(symbol.strip().upper(), start, end, required_tags=tags)\n",
        "\n",
        "    # Detect if optimizer re-synthesis ran (compare Initial vs Final on normalized JSON text)\n",
        "    optimizer_ran = False\n",
        "    init = next((a for a in res.agent_outputs if a.agent_name in {\"Initial Synthesis\", \"Research Synthesis Agent\", \"SynthesisAgent\"}), None)\n",
        "    if init is not None:\n",
        "        init_txt = clean(as_text(init.analysis))\n",
        "        final_txt_norm = clean(as_text(res.final.analysis))\n",
        "        optimizer_ran = (init_txt != final_txt_norm)\n",
        "\n",
        "    plan = \"\\n\".join([f\"• {step}\" for step in res.plan])\n",
        "\n",
        "    # Agents panel: Synthesis agents shown as prose; others as text\n",
        "    agents_txt = \"\\n\\n\".join([\n",
        "        (\n",
        "            f\"[{a.agent_name}] score={a.score:.2f} conf={a.confidence:.2f}\\n\"\n",
        "            f\"{synth_to_prose(a.analysis) if ('synthesis' in a.agent_name.lower()) else clean(as_text(a.analysis))}\"\n",
        "        )\n",
        "        for a in res.agent_outputs\n",
        "    ])\n",
        "\n",
        "    # Evidence tables (force DataFrame)\n",
        "    news_rows = to_df(res.evidence.get(\"top_news\", []))\n",
        "    prices_rows = to_df(res.evidence.get(\"prices_tail\", []))\n",
        "\n",
        "    # Helpful note when news evidence is empty\n",
        "    if news_rows.empty:\n",
        "        agents_txt += \"\\n\\n[Note] No news items matched filters or API limits were hit today.\"\n",
        "\n",
        "    # ---- Critique FIRST (so it appears above Final in the UI) ----\n",
        "    crit_txt = (\n",
        "        f\"[Critique]\\n\"\n",
        "        f\"score={res.critique.score:.2f} adj_conf={res.critique.confidence:.2f}\\n\"\n",
        "        f\"{clean(as_text(res.critique.analysis))}\"\n",
        "    )\n",
        "\n",
        "    # ---- Final AFTER Critique ----\n",
        "    headline = \"FINAL (After Critique)\"\n",
        "    opt_line = \"[Optimizer ran: YES]\" if optimizer_ran else \"[Optimizer ran: NO]\"\n",
        "    final_txt = (\n",
        "        f\"{headline}\\n{opt_line}\\n\"\n",
        "        f\"score={res.final.score:.2f} conf={res.final.confidence:.2f}\\n\"\n",
        "        f\"{synth_to_prose(res.final.analysis)}\\n\\nKey: {', '.join(res.final.key_factors)}\"\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: return order matches component outputs order (crit BEFORE final)\n",
        "    return plan, agents_txt, crit_txt, final_txt, news_rows, prices_rows\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Agentic Finance\") as demo:\n",
        "    gr.Markdown(\"# Agentic Finance — Interactive Tester\")\n",
        "\n",
        "    with gr.Row():\n",
        "        symbol = gr.Textbox(label=\"Ticker\", value=\"AAPL\")\n",
        "        days_back = gr.Slider(7, 120, value=30, step=1, label=\"Days Back\")\n",
        "        tags = gr.Textbox(label=\"Required Tags (optional, comma-sep)\", placeholder=\"earnings, product\")\n",
        "    run_btn = gr.Button(\"Run\")\n",
        "\n",
        "    plan = gr.Textbox(label=\"Plan\", lines=6)\n",
        "    agents = gr.Textbox(label=\"Agent Outputs\", lines=14)\n",
        "\n",
        "    #  Critique ABOVE Final\n",
        "    crit = gr.Textbox(label=\"Critique\", lines=8)\n",
        "    final = gr.Textbox(label=\"Final Recommendation\", lines=10)\n",
        "\n",
        "    news_tbl = gr.Dataframe(\n",
        "        headers=[\"published_at\",\"source\",\"title\",\"summary\",\"url\",\"overall_sentiment\",\"tags\",\"numbers\"],\n",
        "        label=\"Top News (evidence)\",\n",
        "        wrap=True\n",
        "    )\n",
        "    prices_tbl = gr.Dataframe(label=\"Recent Prices (evidence)\")\n",
        "\n",
        "    # Outputs order: plan, agents, CRITIQUE, FINAL, news, prices\n",
        "    run_btn.click(\n",
        "        run,\n",
        "        inputs=[symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "a520",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}