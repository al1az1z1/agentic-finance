{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "kl2KAh90u7JO",
      "metadata": {
        "id": "kl2KAh90u7JO"
      },
      "source": [
        "# Market Research Agent — Price, Indicators, and News Pipeline\n",
        "\n",
        "This project builds a lightweight research workflow for equities: we fetch historical prices, compute common technical indicators (SMA, RSI), and pull headline summaries with basic sentiment. A small JSON cache keeps runs fast and reproducible while avoiding API rate limits. The notebook(s) walk through data loading, quick EDA, feature prep, and simple evaluation.\n",
        "\n",
        "**Course:** MSAAI 520-02 — Group 5  \n",
        "**Date:** October 18, 2025\n",
        "\n",
        "## Group 5 Members\n",
        "- Sunitha Kosireddy  \n",
        "- Victor Salcedo\n",
        "- Ali Azizi\n",
        "\n",
        "**Professor:** Dr. Kahila Mokhtari \n",
        "\n",
        "Our course instructor has been added as a **GitHub contributor** to this project repository\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8e8972",
      "metadata": {
        "id": "8e8e8972"
      },
      "source": [
        "# Agentic Finance — All-in-One Notebook\n",
        "\n",
        "**This notebook is a demonstration notebook showing the full workflow and plan of our project.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;It summarizes the key components we developed across the Agentic Finance system, including data ingestion, analysis, orchestration, and user interface layers\n",
        "\n",
        "3 explicit workflow patterns\n",
        "-  5 working AI agents\n",
        "- Full data pipeline\n",
        "- Interactive UI\n",
        "- Professional code structure\n",
        "\n",
        "**To run the full interactive application, please use the Gradio app located at:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;/src/ui/gradio_app.py\n",
        "\n",
        "**Run it from the project root with:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;python -m ui.gradio_app\n",
        "\n",
        "**This will test the agents and see live results.**\n",
        "\n",
        "**GitHub Repository**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;GitHub: https://github.com/al1az1z1/agentic-finance\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;To reproduce or run this notebook\n",
        "\n",
        "pandas>=1.5\n",
        "numpy>=1.23\n",
        "requests>=2.31\n",
        "yfinance>=0.2.44\n",
        "gradio>=4.44\n",
        "python-dotenv>=1.0\n",
        "openai>=1.40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac59b6",
      "metadata": {
        "id": "1dac59b6"
      },
      "source": [
        "# Project Path Setup (for Imports)\n",
        "\n",
        "**Purpose**  \n",
        "\n",
        "- **This cell just sets the main project folder and makes sure our src/ code can be imported properly inside the notebook.**\n",
        "\n",
        "- **It adds the project path to Python’s import list so we can use things like**\n",
        "\n",
        "- **from src.data_io.cache import load_cache without errors.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ee2be1c7",
      "metadata": {
        "id": "ee2be1c7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Discover the repo root by walking upward until we find a folder containing \"src\".\n",
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"src\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "# Expose the repository root on sys.path so from src... works in this session.\n",
        "sys.path.insert(0, str(ROOT))  # make \"src\" importable\n",
        "\n",
        "# Ensure packages (empty __init__.py files)\n",
        "for p in [\n",
        "    ROOT / \"src\",\n",
        "    ROOT / \"src\" / \"config\",\n",
        "    ROOT / \"src\" / \"data_io\",\n",
        "    ROOT / \"src\" / \"system\",\n",
        "    ROOT / \"src\" / \"analysis\",\n",
        "\n",
        "\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    (p / \"__init__.py\").touch(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54efe4b1",
      "metadata": {
        "id": "54efe4b1"
      },
      "source": [
        "# Basic Feature Engineering — SMA and RSI Computation\n",
        "\n",
        "**Purpose**  \n",
        "- We fetch SMA/RSI from Alpha Vantage when possible\n",
        "- **if the API key is missing or rate-limited, we compute the same indicators locally from prices using our compute_sma and compute_rsi.**\n",
        "- **We cache results so repeated calls are fast and consistent.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "383f0b0b",
      "metadata": {
        "id": "383f0b0b"
      },
      "outputs": [],
      "source": [
        "# src/analysis/features.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_sma(prices: pd.DataFrame, window: int) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    return prices[\"close\"].rolling(window=window).mean()\n",
        "\n",
        "def compute_rsi(prices: pd.DataFrame, window: int = 14) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    delta = prices[\"close\"].diff()\n",
        "    gain = np.where(delta > 0, delta, 0.0)\n",
        "    loss = np.where(delta < 0, -delta, 0.0)\n",
        "    gain_s = pd.Series(gain, index=prices.index)\n",
        "    loss_s = pd.Series(loss, index=prices.index)\n",
        "    avg_gain = gain_s.rolling(window=window).mean()\n",
        "    avg_loss = loss_s.rolling(window=window).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99347f63",
      "metadata": {
        "id": "99347f63"
      },
      "source": [
        "# News basic Preprocessing Prepare news for the pipeline\n",
        "# Make LLM I/O stable\n",
        "\n",
        "**How the orchestrator uses it**  \n",
        "\n",
        "&nbsp;**preprocess_news(df) -> Parse published_at to UTC, drop dupes by url, fill missing summary.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: we want consistent timestamps and no broken rows before routing\n",
        "\n",
        "&nbsp;**add_tags_and_numbers(df) -> Add tags from simple keyword rules and pull numeric tokens (e.g., $10, 3.5%).**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: tags help later filtering; numbers give agents concrete context.\n",
        "\n",
        "&nbsp;**recent_topk(df, topk, days, required_tags) -> Keep only recent items and (optionally) require certain tags.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: trim the context to timely, relevant news so agents stay focused.\n",
        "\n",
        "\n",
        "\n",
        "**How the agents use it**  \n",
        "\n",
        "&nbsp;**strip_code_fences(s) -> Remove /json wrappers before json.loads.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: models sometimes wrap JSON; this prevents parse errors.\n",
        "\n",
        "&nbsp;**to_float(x) -> Best-effort number parsing (also maps words like bullish/neutral to anchors).**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: agent prompts may yield words or percents—we standardize them.\n",
        "\n",
        "&nbsp;**normalize_score(v) -> Map various scales (−1..1, 0..1, %, 0..10) into −1..1.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: every agent's \"score\" becomes comparable for synthesis.\n",
        "\n",
        "&nbsp;**normalize_conf(v) -> Normalize any confidence-like value into 0..1 (clamped).**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: uniform confidence makes critique/synthesis logic simpler.\n",
        "\n",
        "&nbsp;**pretty_json_block(obj) -> Safe, truncated JSON block for UI.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;Why: readable agent outputs without overloading the notebook/app.\n",
        "\n",
        "\n",
        "#### **Result:** each agent returns consistent, parseable JSON with aligned scales, making synthesis and critique reliable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fdc78099",
      "metadata": {
        "id": "fdc78099"
      },
      "outputs": [],
      "source": [
        "# Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "# from config.settings import SETTINGS\n",
        "\n",
        "# -----------------------------\n",
        "# Existing tagging / preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "# Lightweight keyword rules for simple topic tagging\n",
        "TAG_RULES = {\n",
        "    \"earnings\": [\"earnings\", \"eps\", \"guidance\", \"outlook\", \"quarter\", \"revenue\"],\n",
        "    \"product\":  [\"launch\", \"iphone\", \"chip\", \"feature\", \"service\"],\n",
        "    \"legal\":    [\"lawsuit\", \"regulator\", \"antitrust\", \"fine\", \"settlement\"],\n",
        "    \"macro\":    [\"inflation\", \"rates\", \"fed\", \"recession\", \"gdp\"]\n",
        "}\n",
        "\n",
        "def preprocess_news(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"published_at\",\"source\",\"title\",\"summary\",\"url\",\n",
        "            \"overall_sentiment\",\"tags\",\"numbers\"\n",
        "        ])\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Alpha Vantage format is like \"20251017T200143\"\n",
        "    # Parse with explicit format; keep timezone-aware for safety\n",
        "    df[\"published_at\"] = pd.to_datetime(\n",
        "        df[\"published_at\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\", utc=True\n",
        "    )\n",
        "\n",
        "    # Drop rows with no title/url; keep others (don’t drop NaT here — the date filter happens later)\n",
        "    df = df.dropna(subset=[\"title\",\"url\"]).drop_duplicates(subset=[\"url\"])\n",
        "    df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def classify_tags(text: str) -> list[str]:\n",
        "    text_l = text.lower()\n",
        "    tags = [k for k, kws in TAG_RULES.items() if any(kw in text_l for kw in kws)]\n",
        "    return tags or [\"general\"]\n",
        "\n",
        "# Regex for lightweight number extraction:\n",
        "# integers/floats, optional leading '$', optional trailing '%'\n",
        "NUM_RE = re.compile(r'(\\$?\\b\\d+(?:\\.\\d+)?%?)')\n",
        "\n",
        "def extract_numbers(text: str) -> list[str]:\n",
        "    \n",
        "    return NUM_RE.findall(text or \"\")[:6] #Pull the first few numeric-looking tokens (e.g., '$10', '3.5%', '2025').\n",
        "\n",
        "def add_tags_and_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add 'tags' and 'numbers' columns using title+summary text.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    df[\"tags\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(classify_tags)\n",
        "    df[\"numbers\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(extract_numbers)\n",
        "    return df\n",
        "\n",
        "def recent_topk(df: pd.DataFrame, topk: int, days: int, required_tags: list[str] | None = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Keep only recent items (last `days`), optionally require at least one of `required_tags`,\n",
        "    then return the most recent `topk`.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Make an aware UTC cutoff; df['published_at'] is already UTC-aware\n",
        "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n",
        "    f = df[df[\"published_at\"] >= cutoff]\n",
        "\n",
        "    # If tag constraints are provided, we prefer rows that match them.\n",
        "    if required_tags:\n",
        "        want = [t.strip().lower() for t in required_tags]\n",
        "        f_tags = f[f[\"tags\"].apply(lambda ts: any(t in [x.lower() for x in ts] for t in want))]\n",
        "        f = f_tags if not f_tags.empty else f\n",
        "\n",
        "    return f.sort_values(\"published_at\", ascending=False).head(topk)\n",
        "\n",
        "# -----------------------------\n",
        "# NEW: shared agent utilities\n",
        "# -----------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "def strip_code_fences(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove leading/trailing triple backticks (``` or ```json) from model text.\n",
        "    This helps when models wrap JSON in code fences.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.IGNORECASE)\n",
        "\n",
        "def to_float(x, default: float = 0.0) -> float:\n",
        "    \"\"\"Best-effort conversion of model outputs or strings to float\n",
        "        - Maps common qualitative words to representative numeric levels.\n",
        "        - Falls back to 'default' if parsing fails\n",
        "        \"\"\"\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            xs = x.strip().lower()\n",
        "            # map common words to numeric anchors\n",
        "            if xs in (\"high\", \"strong\", \"bullish\", \"overbought\"):\n",
        "                return 0.8\n",
        "            if xs in (\"medium\", \"moderate\", \"neutral\"):\n",
        "                return 0.5\n",
        "            if xs in (\"low\", \"weak\", \"bearish\", \"oversold\"):\n",
        "                return 0.2\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def clamp(x: float, lo: float, hi: float) -> float:\n",
        "    \"\"\"Keep a value within [lo, hi].\"\"\"\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "def normalize_score(v: float) -> float:\n",
        "    \"\"\"\n",
        "    Normalize arbitrary score ranges to [-1, 1].\n",
        "    Heuristics:\n",
        "      - If already in [-1,1], keep.\n",
        "      - If in [0,1], map to [-1,1] via (v-0.5)*2.\n",
        "      - If in (1,100], treat as percent.\n",
        "      - If in (1,10], treat as 0-10 and map.\n",
        "      - Else, clamp.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        v = float(v)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    if -1.0 <= v <= 1.0:\n",
        "        return v\n",
        "    if 0.0 <= v <= 1.0:\n",
        "        return (v - 0.5) * 2.0\n",
        "    if 1.0 < v <= 100.0:\n",
        "        v01 = v / 100.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    if 1.0 < v <= 10.0:\n",
        "        v01 = v / 10.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    return clamp(v, -1.0, 1.0)\n",
        "\n",
        "def normalize_conf(v) -> float:\n",
        "    \"\"\"Normalize any confidence-like value to [0,1].\n",
        "        - Defaults to 0.7 if parsing is uncertain.\n",
        "    \"\"\"\n",
        "    f = to_float(v, 0.7)\n",
        "    if 1.0 < f <= 100.0:\n",
        "        f = f / 100.0\n",
        "    return clamp(f, 0.0, 1.0)\n",
        "\n",
        "# Optional: helpers to render structured dicts into strings (for external tools)\n",
        "def pretty_json_block(obj: dict, max_chars: int = 4000) -> str:\n",
        "    \"\"\"Return a fenced JSON markdown block, truncated for UI safety.\"\"\"\n",
        "    try:\n",
        "        js = json.dumps(obj, ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        js = str(obj)\n",
        "    if len(js) > max_chars:\n",
        "        js = js[: max_chars - 20] + \"\\n... (truncated)\"\n",
        "    return f\"```json\\n{js}\\n```\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a110bf",
      "metadata": {
        "id": "a2a110bf"
      },
      "source": [
        "# Settings & .env: one clean place for keys and knob (safety)\n",
        "\n",
        "&nbsp;**We use a tiny Settings object (loaded from .env) so every module like prices, news, indicators, earnings, and agents—reads config from one spot.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;This keeps API keys out of code and Git, standardizes paths (data/, cache/, runs/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "74137240",
      "metadata": {
        "id": "74137240"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _find_project_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Walk upward to find the repo root heuristically.\n",
        "    Treat a folder containing both 'src' and 'data' as the root.\n",
        "    Fallback to the starting directory if not found.\n",
        "    \"\"\"\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# project root = repo root\n",
        "if \"__file__\" in globals():\n",
        "    ROOT = Path(__file__).resolve().parents[2]\n",
        "else:\n",
        "    # Notebook / REPL: start from CWD and auto-detect root\n",
        "    ROOT = _find_project_root(Path.cwd())\n",
        "\n",
        "load_dotenv(ROOT / \".env\", override=False)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Settings:\n",
        "    data_dir: Path = ROOT / \"data\"\n",
        "    cache_dir: Path = ROOT / \"data\" / \"cache\"\n",
        "    runs_dir: Path = ROOT / \"data\" / \"runs\"\n",
        "    alpha_api_key: str = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"BVGUKZR1MHVS0T6B\")\n",
        "    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"sk-proj-\")\n",
        "    news_window_days: int = 14\n",
        "    topk_news: int = 5\n",
        "    cache_ttl_minutes: int = 60\n",
        "\n",
        "SETTINGS = Settings()\n",
        "SETTINGS.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "SETTINGS.runs_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XIKxbGY5r1ce",
      "metadata": {
        "id": "XIKxbGY5r1ce"
      },
      "source": [
        "# Component — Simple JSON Cache (TTL + Atomic Writes)\n",
        "\n",
        "**Purpose**  \n",
        "Provide a lightweight on-disk cache to reduce redundant network calls and stabilize latency. Each cache entry is stored as a single JSON file with a timestamp for TTL-based expiry. Writes are atomic to avoid corruption.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by data fetchers (e.g., price/news downloads) prior to any external HTTP call. Implemented as simple functions for ease of reuse and testing.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `key: str` (file name stem), optional `ttl_minutes: int | None`, arbitrary JSON-serializable `data`  \n",
        "- **Outputs:** `load_cache` returns the stored payload (`data`) or `None`; `save_cache` returns `None`  \n",
        "- **Side Effects:** Creates/updates files under `SETTINGS.cache_dir` as `<key>.json`\n",
        "\n",
        "**Behavior**  \n",
        "- `load_cache(key, ttl_minutes=None)`  \n",
        "  - If file missing/corrupt → `None`  \n",
        "  - If `ttl_minutes is None` → returns the stored `data` (ignore age)  \n",
        "  - Else → returns `data` only if `(now - _ts) <= ttl_minutes * 60`  \n",
        "- `save_cache(key, data)`  \n",
        "  - Serializes as `{\"_ts\": epoch_seconds, \"data\": <payload>}`  \n",
        "  - Writes to a temp file then atomically replaces the target\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Invalid JSON / partial writes → treated as a cache miss (returns `None`)  \n",
        "- Non-serializable objects → coerced via `_json_default` (ISO-8601 for dates, `str()` fallback, `repr()` last resort)\n",
        "\n",
        "**Configuration & Tunables**  \n",
        "- `SETTINGS.cache_dir` controls storage location  \n",
        "- TTL per call via `ttl_minutes`; absence means “return whatever is present”\n",
        "\n",
        "**Security & Data Handling**  \n",
        "- Do not cache secrets/PII. Payload is plain JSON on disk.\n",
        "\n",
        "**Testability**  \n",
        "- Unit tests: miss→save→hit, TTL expiry path, corrupt file → miss, atomic replace behavior (temp file present)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6a42293c",
      "metadata": {
        "id": "6a42293c"
      },
      "outputs": [],
      "source": [
        "# Purpose: lightweight disk cache with TTL and atomic writes\n",
        "# Context: used by data fetchers (e.g., price downloads) to avoid repeat network calls\n",
        "# Notes: filenames derive from key under SETTINGS.cache_dir; payload stored as JSON\n",
        "\n",
        "# cache.py\n",
        "from __future__ import annotations\n",
        "import json, time\n",
        "from datetime import date, datetime\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def _cache_path(key: str) -> Path:\n",
        "    return SETTINGS.cache_dir / f\"{key}.json\"\n",
        "\n",
        "def _json_default(o: Any):\n",
        "    # datetime & pandas.Timestamp (subclass of datetime) → ISO 8601\n",
        "    if isinstance(o, (datetime, date)):\n",
        "        return o.isoformat()\n",
        "    # Fallback: make a best-effort string (covers Decimal, Path, Enum, etc.)\n",
        "    try:\n",
        "        return str(o)\n",
        "    except Exception:\n",
        "        return repr(o)\n",
        "\n",
        "def load_cache(key: str, ttl_minutes: int | None = None) -> Any | None:\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        if ttl_minutes is None:\n",
        "            return obj.get(\"data\")  # consistent: always return payload\n",
        "        if (time.time() - obj.get(\"_ts\", 0)) <= ttl_minutes * 60:\n",
        "            return obj.get(\"data\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def save_cache(key: str, data: Any) -> None:\n",
        "    p = _cache_path(key)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
        "    payload = {\"_ts\": time.time(), \"data\": data}\n",
        "    tmp.write_text(json.dumps(payload, ensure_ascii=False, default=_json_default), encoding=\"utf-8\")\n",
        "    tmp.replace(p)  # atomic on most OS/filesystems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "909a0729",
      "metadata": {
        "id": "909a0729"
      },
      "source": [
        "# Risk Metrics — Return/Volatility/Sharpe, Max Drawdown, VaR(5%), and Beta vs Benchmark\n",
        "\n",
        "**Purpose**  \n",
        "Compute a small, interpretable risk summary for a ticker: average daily return, volatility, Sharpe ratio, max drawdown, 5th-percentile Value-at-Risk, and beta versus a benchmark (default S&P 500, `^GSPC`). Results are cached per `(symbol, start, end)`.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by reporting and decision agents to contextualize recent performance and risk. Implemented as side-effect-free helpers plus a single orchestration function, `fetch_risk_metrics`.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:**  \n",
        "  - `symbol: str` — ticker (e.g., `\"AAPL\"`)  \n",
        "  - `start: Optional[str]`, `end: Optional[str]` — ISO dates or `None` for provider defaults  \n",
        "  - `benchmark: str` — comparison index for beta (default `^GSPC`)  \n",
        "- **Outputs:** `Dict[str, Any]` with keys:  \n",
        "  `avg_daily_return`, `volatility`, `sharpe_ratio`, `max_drawdown`, `var_5`, `beta`  \n",
        "- **Side Effects:** Reads/writes a JSON cache keyed by `(symbol, start, end)` via `load_cache`/`save_cache`.\n",
        "\n",
        "**Method Overview**  \n",
        "- **Daily returns:** `pct_change()` on close prices.  \n",
        "- **Volatility:** standard deviation of daily returns.  \n",
        "- **Sharpe (daily):** mean/volatility (no risk-free rate; downstream can annualize).  \n",
        "- **Max drawdown:** min of `(price / rolling_max) - 1`. Reported in percent (negative).  \n",
        "- **VaR(5%):** empirical 5th percentile of daily returns.  \n",
        "- **Beta:** covariance(asset, benchmark) / variance(benchmark), over the overlapping window.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Missing/empty price series → `{}` (empty metrics) cached and returned.  \n",
        "- Benchmark retrieval errors → `beta = NaN` while other metrics remain valid.  \n",
        "- Non-overlapping or constant benchmark returns → `beta = NaN`.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Cache stabilizes results for the TTL window defined in `SETTINGS`.  \n",
        "- Deterministic given inputs; explicit column normalization when downloading the benchmark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0747e75a",
      "metadata": {
        "id": "0747e75a"
      },
      "outputs": [],
      "source": [
        "# Implementation\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from typing import Dict, Any, Optional\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "\n",
        "# Compute daily returns from a price DataFrame.\n",
        "# Returns an empty float Series if input is None/empty or lacks 'close'\n",
        "def _daily_returns(df: pd.DataFrame) -> pd.Series:\n",
        "    if df is None or df.empty or \"close\" not in df:\n",
        "        return pd.Series(dtype=float)\n",
        "    return df[\"close\"].astype(float).pct_change()\n",
        "\n",
        "# Compute max drawdown as the minimum (price/rolling_peak - 1), returned as percent.\n",
        "# Returns NaN if input is invalid.\n",
        "def _max_drawdown_pct(prices: pd.DataFrame) -> float:\n",
        "    if prices is None or prices.empty or \"close\" not in prices:\n",
        "        return float(\"nan\")\n",
        "    series = prices[\"close\"].astype(float)\n",
        "    roll_max = series.cummax()\n",
        "    drawdown = (series / roll_max) - 1.0\n",
        "    mdd = drawdown.min()\n",
        "    return float(round(mdd * 100.0, 3))\n",
        "\n",
        "# Compute beta = cov(asset, bench) / var(bench) on aligned, non-null returns.\n",
        "# Returns NaN if insufficient data or zero variance.\n",
        "def _beta_vs_bench(asset_rets: pd.Series, bench_rets: pd.Series) -> float:\n",
        "    m = pd.concat([asset_rets, bench_rets], axis=1).dropna()\n",
        "    if m.empty:\n",
        "        return float(\"nan\")\n",
        "    cov = np.cov(m.iloc[:, 0], m.iloc[:, 1])[0, 1]\n",
        "    var = np.var(m.iloc[:, 1])\n",
        "    if var == 0:\n",
        "        return float(\"nan\")\n",
        "    return float(cov / var)\n",
        "\n",
        "# Compute a compact set of risk/return metrics for `symbol`, with caching.\n",
        "def fetch_risk_metrics(symbol: str, start: Optional[str], end: Optional[str], benchmark: str = \"^GSPC\") -> Dict[str, Any]:\n",
        "    cache_key = f\"risk_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return cached\n",
        "\n",
        "    # Fetch prices; bail early if not available.\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    if prices is None or prices.empty:\n",
        "        save_cache(cache_key, {})\n",
        "        return {}\n",
        "\n",
        "    # Compute returns; if no variance (e.g., single row), return {}.\n",
        "    rets = _daily_returns(prices).dropna()\n",
        "    if rets.empty:\n",
        "        save_cache(cache_key, {})\n",
        "        return {}\n",
        "\n",
        "    # Core metrics on asset returns (daily scale).\n",
        "    mean_ret = float(rets.mean())\n",
        "    vol = float(rets.std())\n",
        "    sharpe = float(mean_ret / vol) if vol > 0 else float(\"nan\")\n",
        "    mdd_pct = _max_drawdown_pct(prices)\n",
        "    var_5 = float(np.nanpercentile(rets.values, 5))\n",
        "\n",
        "    # Download benchmark with explicit auto_adjust to avoid FutureWarning\n",
        "    beta = float(\"nan\")\n",
        "    try:\n",
        "        bench = yf.download(\n",
        "            benchmark,\n",
        "            start=prices[\"date\"].min(),\n",
        "            end=prices[\"date\"].max(),\n",
        "            progress=False,\n",
        "            auto_adjust=False,   # <— key change\n",
        "            threads=False,\n",
        "        )\n",
        "        # Normalize possible MultiIndex/flat columns to lower-case names.\n",
        "        if isinstance(bench.columns, pd.MultiIndex):\n",
        "            bench.columns = [c[0].lower() for c in bench.columns]\n",
        "        else:\n",
        "            bench.columns = [c.lower() for c in bench.columns]\n",
        "        bench = bench.reset_index().rename(columns={\"Date\": \"date\"})\n",
        "        bench[\"date\"] = bench[\"date\"].astype(str)\n",
        "        bench_rets = bench[\"close\"].astype(float).pct_change().dropna()\n",
        "        # Align windows: use the overlapping tail of equal length.\n",
        "        n = min(len(rets), len(bench_rets))\n",
        "        beta = _beta_vs_bench(rets.tail(n).reset_index(drop=True), bench_rets.tail(n).reset_index(drop=True))\n",
        "    except Exception:\n",
        "        beta = float(\"nan\")\n",
        "\n",
        "    metrics = {\n",
        "        \"avg_daily_return\": round(mean_ret, 6),\n",
        "        \"volatility\": round(vol, 6),\n",
        "        \"sharpe_ratio\": round(sharpe, 3) if not np.isnan(sharpe) else float(\"nan\"),\n",
        "        \"max_drawdown\": mdd_pct,       # percent (negative)\n",
        "        \"var_5\": round(var_5, 6),\n",
        "        \"beta\": round(beta, 3) if not np.isnan(beta) else float(\"nan\"),\n",
        "    }\n",
        "    save_cache(cache_key, metrics)\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28824d0",
      "metadata": {
        "id": "c28824d0"
      },
      "source": [
        "# Earnings Ingestion — Yahoo Finance `earnings_dates` with On-Disk Caching\n",
        "\n",
        "**Purpose**  \n",
        "Fetch quarterly earnings data (EPS estimate, reported EPS, surprise %) from Yahoo Finance and normalize it for downstream analysis. Cache results per symbol to reduce repeated network calls.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by reporting/EDA to attach recent earnings context alongside prices/technicals. Implemented in `src/data_io/earnings.py`.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `symbol: str` (e.g., \"AAPL\")  \n",
        "- **Outputs:** `pd.DataFrame` with columns: `['date','EPS Estimate','Reported EPS','Surprise(%)']` (date as `YYYY-MM-DD`)  \n",
        "- **Side Effects:** Reads/writes JSON records in `SETTINGS.cache_dir` (key: `earnings_{symbol}`)\n",
        "\n",
        "**Behavior**  \n",
        "1. Try cache (TTL from `SETTINGS.cache_ttl_minutes`); return on hit.  \n",
        "2. On miss, call `yfinance.Ticker(symbol).earnings_dates`.  \n",
        "3. Normalize the date column to `date` and ensure required columns exist.  \n",
        "4. Coerce `date` to string `YYYY-MM-DD`, cache as records, and return.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Provider returns `None`/empty → return typed empty frame with required columns.  \n",
        "- Schema variations (date as index vs column) → handled via normalization branch.  \n",
        "- Exceptions during fetch/parse → return empty typed frame (safe fail) and cache it.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Cached list-of-dicts makes reruns deterministic for the TTL window.  \n",
        "- Stable schema enables straightforward joins and plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d776edf7",
      "metadata": {
        "id": "d776edf7"
      },
      "outputs": [],
      "source": [
        "# src/data_io/earnings.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from typing import Dict, Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_earnings(symbol: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Quarterly earnings with EPS actual/estimate/surprise.\n",
        "    Columns: ['date','EPS Estimate','Reported EPS','Surprise(%)']\n",
        "    \"\"\"\n",
        "    cache_key = f\"earnings_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    try:\n",
        "        tk = yf.Ticker(symbol)\n",
        "        df = tk.earnings_dates\n",
        "        if df is None or getattr(df, \"empty\", True):\n",
        "            df = pd.DataFrame(columns=[\"Earnings Date\",\"EPS Estimate\",\"Reported EPS\",\"Surprise(%)\"])\n",
        "\n",
        "        # Normalize column named 'Earnings Date' -> 'date'\n",
        "        if \"Earnings Date\" in df.columns:\n",
        "            df = df.reset_index(drop=True).rename(columns={\"Earnings Date\": \"date\"})\n",
        "        elif df.index.name == \"Earnings Date\":\n",
        "            df = df.reset_index().rename(columns={\"Earnings Date\": \"date\"})\n",
        "        else:\n",
        "            if \"date\" not in df.columns:\n",
        "                df = df.reset_index().rename(columns={\"index\": \"date\"})\n",
        "\n",
        "         # Ensure the required output columns exist, filling missing with None.\n",
        "        keep = [\"date\",\"EPS Estimate\",\"Reported EPS\",\"Surprise(%)\"]\n",
        "        for k in keep:\n",
        "            if k not in df.columns:\n",
        "                df[k] = None\n",
        "        df = df[keep].copy()\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    except Exception:\n",
        "        df = pd.DataFrame(columns=[\"date\",\"EPS Estimate\",\"Reported EPS\",\"Surprise(%)\"])\n",
        "\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vIV91U3JrnJa",
      "metadata": {
        "id": "vIV91U3JrnJa"
      },
      "source": [
        "# Price Ingestion — Yahoo Finance OHLCV with On-Disk Caching\n",
        "\n",
        "**Purpose**  \n",
        "Download OHLCV time series from Yahoo Finance (`yfinance`) and return a normalized DataFrame. Use a disk cache to avoid redundant network calls and smooth over API throttling.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Called by data preparation steps before feature engineering/EDA. Lives in `src/...` and is imported by notebooks and agents.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:**  \n",
        "  - `symbol: str` — e.g., `\"AAPL\"`  \n",
        "  - `start: str | None` — ISO-like date (e.g., `\"2020-01-01\"`) or `None`  \n",
        "  - `end: str | None` — ISO-like date or `None`  \n",
        "- **Outputs:** `pd.DataFrame` with columns: `date`, `open`, `high`, `low`, `close`, `adj_close`, `volume`  \n",
        "- **Side Effects:** Reads/writes JSON records under `SETTINGS.cache_dir` via `load_cache`/`save_cache`\n",
        "\n",
        "**Behavior**  \n",
        "- Compose a cache key from `(symbol, start, end)` and honor `SETTINGS.cache_ttl_minutes`.  \n",
        "- On cache hit, materialize a DataFrame from the cached JSON records.  \n",
        "- On miss, call `yfinance.download`, flatten a possible MultiIndex, standardize column names, coerce `date` to string (JSON-safe), and cache the result.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Network/throttle issues → function returns whatever `yfinance` yields (may be empty); subsequent calls can hit cache if a prior success exists.  \n",
        "- Unknown symbols or empty ranges → valid but empty DataFrame.  \n",
        "- Column shape variations (e.g., MultiIndex) → flattened defensively.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- The cached JSON (records orient) makes runs reproducible for a TTL window and simplifies inspection.  \n",
        "- Deterministic column naming aids downstream merging and plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "fb6014f6",
      "metadata": {
        "id": "fb6014f6"
      },
      "outputs": [],
      "source": [
        "# Purpose: download OHLCV from Yahoo Finance and return a normalized DataFrame with caching\n",
        "# Context: called by data prep steps before features/EDA; avoids repeated network calls\n",
        "# Notes: flattens MultiIndex cols, standardizes names, stores json-serializable cache\n",
        "\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_prices(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:\n",
        "    cache_key = f\"prices_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "    df = yf.download(symbol, start=start, end=end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0].lower() for c in df.columns]\n",
        "    df = df.reset_index().rename(columns={\n",
        "        \"Date\": \"date\", \"open\":\"open\",\"high\":\"high\",\"low\":\"low\",\"close\":\"close\",\"adj close\":\"adj_close\",\"volume\":\"volume\"\n",
        "    })\n",
        "    df[\"date\"] = df[\"date\"].astype(str)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTEo0-OksEFN",
      "metadata": {
        "id": "UTEo0-OksEFN"
      },
      "source": [
        "# Technical Indicators — Alpha Vantage SMA/RSI with Cached Local Fallback\n",
        "\n",
        "**Purpose**  \n",
        "Retrieve daily SMA/RSI time series using Alpha Vantage when available, with a deterministic local-compute fallback (from Yahoo Finance OHLCV) to maintain functionality under API limits or missing keys.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by feature pipelines that require daily technical indicators. Implemented in `src/data_io/indicators.py` and consumed by analysis/agent steps.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:**  \n",
        "  - `symbol: str` — ticker (e.g., `\"AAPL\"`)  \n",
        "  - `indicator: {\"SMA\",\"RSI\"}`  \n",
        "  - `time_period: int` — lookback window (default `14`)  \n",
        "- **Outputs:** `pd.DataFrame` with columns `date` and `SMA` or `RSI`, sorted ascending by `date`  \n",
        "- **Side Effects:** Caches JSON records under `SETTINGS.cache_dir` by `(symbol, indicator, time_period)`\n",
        "\n",
        "**Behavior**  \n",
        "1. Attempt cache → return on hit (honors `SETTINGS.cache_ttl_minutes`).  \n",
        "2. If Alpha Vantage is unavailable (no key/unknown indicator) or rate-limited/error, compute locally from `fetch_prices` using `compute_sma` or `compute_rsi`.  \n",
        "3. On successful API call, normalize Alpha Vantage payload to a tidy DataFrame (parsed dates, numeric columns), sort ascending, cache, and return.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Network errors / quota messages / malformed payload → fallback to local compute.  \n",
        "- Empty price data in fallback path → return empty DataFrame.  \n",
        "- Non-parsable dates or numeric fields → coerced with `errors=\"coerce\"` and dropped.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Cache persists list-of-dict records for deterministic reloads during the TTL window.  \n",
        "- Dates normalized to `datetime64[ns]`; output sorted for stable joins/plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "752d66ae",
      "metadata": {
        "id": "752d66ae"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch SMA/RSI via Alpha Vantage with a cached local-compute fallback\n",
        "# Context: used by feature pipelines that need daily indicators\n",
        "# Notes: caches by (symbol, indicator, time_period); normalizes dates and numeric types\n",
        "\n",
        "# src/data_io/indicators.py\n",
        "from __future__ import annotations\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.analysis.features import compute_sma, compute_rsi\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "KEYS = {\"SMA\": \"Technical Analysis: SMA\", \"RSI\": \"Technical Analysis: RSI\"}\n",
        "\n",
        "\n",
        "# If AV isn’t available (no key/limit), our code falls back to computing indicators locally from prices using our compute_sma / compute_rsi.\n",
        "def _fallback_from_prices(symbol: str, indicator: str, time_period: int) -> pd.DataFrame:\n",
        "    prices = fetch_prices(symbol, None, None)\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if indicator == \"SMA\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"SMA\": compute_sma(prices, window=time_period)})\n",
        "    elif indicator == \"RSI\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"RSI\": compute_rsi(prices, window=time_period)})\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna().sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def fetch_indicator(symbol: str, indicator: str, time_period: int = 14) -> pd.DataFrame:\n",
        "    key = KEYS.get(indicator)\n",
        "\n",
        "    # Try cache first\n",
        "    cache_key = f\"indicator_{symbol}_{indicator}_{time_period}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    if not SETTINGS.alpha_api_key or key is None:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    params = {\n",
        "        \"function\": indicator,\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": \"daily\",\n",
        "        \"time_period\": time_period,\n",
        "        \"series_type\": \"close\",\n",
        "        \"apikey\": SETTINGS.alpha_api_key,\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(BASE, params=params, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        # Alpha Vantage quota message handling:\n",
        "        if (not data or key not in data or not data[key] or \"Note\" in data or \"Information\" in data or \"Error Message\" in data):\n",
        "            df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "            save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "            return df\n",
        "    except Exception:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "    df.reset_index(inplace=True)\n",
        "    df = df.rename(columns={\"index\": \"date\"})\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iJxmkUXGsR3s",
      "metadata": {
        "id": "iJxmkUXGsR3s"
      },
      "source": [
        "# News Ingestion — Alpha Vantage Feed with Ticker/Relevance Filtering and Cache\n",
        "\n",
        "**Purpose**  \n",
        "Fetch symbol-specific headlines from Alpha Vantage’s News Sentiment API, filter to items that explicitly mention the target ticker with sufficient relevance, and cache the normalized rows to reduce redundant calls.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Called by downstream reporting/EDA steps to attach recent headlines and high-level sentiment to a ticker. Implemented as a single function for clarity and testability.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `symbol: str` (e.g., `\"AAPL\"`)  \n",
        "- **Outputs:** `pd.DataFrame` with columns: `published_at`, `source`, `title`, `summary`, `url`, `overall_sentiment`  \n",
        "- **Side Effects:**  \n",
        "  - Reads/writes JSON records under `SETTINGS.cache_dir` using `load_cache` / `save_cache`  \n",
        "  - Performs a network request to Alpha Vantage on cache miss\n",
        "\n",
        "**Behavior**  \n",
        "1. If no API key is configured, return an empty DataFrame (safe fail).  \n",
        "2. Check a per-symbol cache; return cached rows on hit.  \n",
        "3. On miss, call `NEWS_SENTIMENT` with the given ticker.  \n",
        "4. Keep only articles where the symbol appears in `ticker_sentiment` **and** `relevance_score ≥ 0.30`.  \n",
        "5. Normalize to a tidy DataFrame and cache as list-of-dict records.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Missing `feed` key or malformed payload → return empty DataFrame.  \n",
        "- Network errors throw from `requests.get` by default; callers can handle exceptions upstream if desired.  \n",
        "- Inconsistent per-item fields are handled with `.get(...)` defaults; missing values propagate as `None/NaN`.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Cached records (JSON) make runs deterministic for the TTL window configured in `SETTINGS`.  \n",
        "- Output schema is stable and designed for straightforward joins/plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fc9bb3f1",
      "metadata": {
        "id": "fc9bb3f1"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch and cache symbol-specific news via Alpha Vantage, filtered by relevance\n",
        "# Context: called by downstream reporting/EDA to attach headlines and sentiment\n",
        "# Notes: filters to items where ticker matches and relevance >= 0.30; caches by symbol\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, requests, pandas as pd\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "\n",
        "def fetch_news(symbol: str) -> pd.DataFrame:\n",
        "    if not SETTINGS.alpha_api_key:\n",
        "        return pd.DataFrame()  # safe fail\n",
        "    cache_key = f\"news_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    params = {\"function\":\"NEWS_SENTIMENT\",\"tickers\":symbol,\"apikey\":SETTINGS.alpha_api_key}\n",
        "    r = requests.get(BASE, params=params, timeout=30)\n",
        "    data = r.json()\n",
        "    if \"feed\" not in data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for item in data.get(\"feed\", []):\n",
        "        tickers = item.get(\"ticker_sentiment\", []) or []\n",
        "        # keep only if our symbol is explicitly mentioned\n",
        "        keep = any(t.get(\"ticker\", \"\").upper() == symbol.upper() and float(t.get(\"relevance_score\", 0) or 0) >= 0.30\n",
        "                   for t in tickers)\n",
        "        if not keep:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"published_at\": item.get(\"time_published\"),\n",
        "            \"source\": item.get(\"source\"),\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"summary\": item.get(\"summary\"),\n",
        "            \"url\": item.get(\"url\"),\n",
        "            \"overall_sentiment\": item.get(\"overall_sentiment_label\")\n",
        "        })\n",
        "\n",
        "    # ====== Forth APPROACH =====\n",
        "    df = pd.DataFrame(rows)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc9c201",
      "metadata": {
        "id": "6cc9c201"
      },
      "source": [
        "# pick which lanes to run\n",
        "\n",
        "**How it works**  \n",
        "\n",
        "&nbsp;**This tiny helper tells the orchestrator which agents to run based on what data we actually have.**\n",
        "\n",
        "\n",
        "&nbsp;**If there’s news, we add the news lane.**\n",
        "\n",
        "\n",
        "&nbsp;**If we have both prices and technicals, we add technical**\n",
        "\n",
        "\n",
        "&nbsp;**If earnings are available, we add earnings**\n",
        "\n",
        "\n",
        "&nbsp;**and we always run risk as a baseline check. It keeps the pipeline efficient—no agent runs without the inputs it needs.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f1200bf3",
      "metadata": {
        "id": "f1200bf3"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "def choose_agents(has_news: bool, has_prices: bool, has_technicals: bool, has_earnings: bool) -> list[str]:\n",
        "    agents = []\n",
        "    if has_news:\n",
        "        agents.append(\"news\")\n",
        "    if has_technicals and has_prices:\n",
        "        agents.append(\"technical\")\n",
        "    if has_earnings:\n",
        "        agents.append(\"earnings\")\n",
        "    agents.append(\"risk\")\n",
        "    return agents\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa0a509",
      "metadata": {
        "id": "afa0a509"
      },
      "source": [
        "# Run Memory (JSONL log for audits & debugging)\n",
        "\n",
        "**Purpose**  \n",
        "\n",
        "&nbsp;We keep a lightweight run log so the orchestrator can record what happened each run (lanes used, issues from critique, confidences, timestamp)\n",
        "\n",
        "&nbsp;append_memory() writes one JSON object per line to data/runs/run_notes.jsonl (auto-created via SETTINGS.runs_dir).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b260d5e1",
      "metadata": {
        "id": "b260d5e1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "MEM_PATH = SETTINGS.runs_dir / \"run_notes.jsonl\"\n",
        "\n",
        "def append_memory(record: dict[str, Any]) -> None:\n",
        "    MEM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with MEM_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cba7d18",
      "metadata": {
        "id": "2cba7d18"
      },
      "source": [
        "## agents.py\n",
        "## Notes\n",
        "This section defines the base for a multi-agent financial analysis framework used in the class project.\n",
        "It includes specialized agents:\n",
        "- **NewsAnalysisAgent** – interprets news sentiment and market tone.\n",
        "- **EarningsAnalysisAgent** – evaluates EPS trends and surprise ratios.\n",
        "- **MarketSignalsAgent** – performs basic technical analysis.\n",
        "- **RiskAssessmentAgent** – quantifies investment risk.\n",
        "- **SynthesisAgent** – merges all signals into a BUY/HOLD/SELL recommendation.\n",
        "- **CritiqueAgent** – reviews the synthesis for quality and bias.\n",
        "Each agent outputs structured JSON with keys like analysis, score, confidence, and key_factors, enabling easy comparison and visualization.\n",
        "The modular design supports testing, replacement, and future extension with additional agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "93d94922",
      "metadata": {
        "id": "93d94922",
        "outputId": "29c8b46e-b3b5-43a0-a443-da5fbed9ed8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY found: sk-pro***\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import os, json\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Import shared helpers from analysis.text\n",
        "from src.analysis.text import (\n",
        "    strip_code_fences,\n",
        "    to_float,\n",
        "    clamp,\n",
        "    normalize_score,\n",
        "    normalize_conf,\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# OpenAI client (safe stub for local/dev)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Use the standard env var name\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Optional: print a very short prefix to help you debug locally\n",
        "if api_key:\n",
        "    print(f\"OPENAI_API_KEY found: {api_key[:6]}***\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY NOT found! (running in MOCK mode)\")\n",
        "    # Don't set a fake key here; just run in mock.\n",
        "\n",
        "# Initialize client if possible; otherwise fall back to mock\n",
        "_client = None\n",
        "try:\n",
        "    # If you want to use the newer SDK:\n",
        "    # from openai import OpenAI\n",
        "    # _client = OpenAI()\n",
        "    #\n",
        "    # Or (legacy) openai.ChatCompletion API — but we'll stick to the new client interface:\n",
        "    from openai import OpenAI\n",
        "    if api_key:\n",
        "        _client = OpenAI()\n",
        "except Exception:\n",
        "    _client = None\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Shared response container\n",
        "# -----------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class AgentResponse:\n",
        "    agent_name: str\n",
        "    analysis: str\n",
        "    score: float\n",
        "    confidence: float\n",
        "    key_factors: List[str]\n",
        "    timestamp: str\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# BaseAgent\n",
        "# -----------------------------------------------------------------------------\n",
        "class BaseAgent:\n",
        "    def __init__(self, agent_name: str, model: str = \"gpt-4o\"):\n",
        "        self.agent_name = agent_name\n",
        "        self.model = model\n",
        "\n",
        "    def call_llm(self, system_prompt: str, user_message: str) -> str:\n",
        "        # Mock path (no API key / no client)\n",
        "        if _client is None:\n",
        "            return json.dumps({\n",
        "                \"analysis\": f\"MOCK: {self.agent_name} processed.\",\n",
        "                \"score\": 0.0,\n",
        "                \"key_factors\": [\"mock\"],\n",
        "                \"confidence\": 0.7\n",
        "            })\n",
        "        try:\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=0.5,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "            return resp.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return json.dumps({\n",
        "                \"analysis\": f\"Error: {e}\",\n",
        "                \"score\": 0.0,\n",
        "                \"key_factors\": [\"error\"],\n",
        "                \"confidence\": 0.3\n",
        "            })\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# News\n",
        "# -----------------------------------------------------------------------------\n",
        "class NewsAnalysisAgent(BaseAgent):\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"News Analysis Agent\", model)\n",
        "        # IMPORTANT: keep everything inside one triple-quoted string\n",
        "        self.system_prompt = \"\"\"You are a senior financial analyst with 15+ years of experience in equity research.\n",
        "\n",
        "Analyze the provided news articles with focus on:\n",
        "1. SENTIMENT: Quantify market sentiment from -1 (very negative) to +1 (very positive)\n",
        "2. MATERIALITY: How much will this impact stock price? (high/medium/low)\n",
        "3. CATALYSTS: Identify specific events that could move the stock\n",
        "4. RISKS: Note any red flags or concerns mentioned\n",
        "\n",
        "SCORING GUIDELINES:\n",
        "+0.8 to +1.0: Major positive catalyst (earnings beat, breakthrough product, strategic win)\n",
        "+0.4 to +0.7: Positive news (growth signals, analyst upgrades, market share gains)\n",
        "-0.3 to +0.3: Neutral or mixed signals\n",
        "-0.7 to -0.4: Negative news (missed targets, regulatory issues, competitive threats)\n",
        "-1.0 to -0.8: Major negative catalyst (fraud, bankruptcy risk, losing key customers)\n",
        "\n",
        "IMPORTANT:\n",
        "- Use actual numbers from articles (revenue, EPS, growth rates)\n",
        "- Compare to analyst expectations when mentioned\n",
        "- Note if news is company-specific vs industry-wide\n",
        "- Higher confidence when multiple sources agree\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze news articles objectively\n",
        "2. Consider both positive and negative aspects\n",
        "3. Provide a sentiment score from -1 (very negative) to +1 (very positive)\n",
        "4. Identify key factors driving the sentiment\n",
        "5. Assess potential stock price impact\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"sentiment_score\": 0.75,\n",
        "  \"analysis\": \"Strong positive sentiment driven by earnings beat and product launch\",\n",
        "  \"key_factors\": [\"Earnings exceeded expectations\", \"New product well-received\"],\n",
        "  \"confidence\": 0.85\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: sentiment_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'AAPL')\n",
        "        news_articles = data.get('news', [])\n",
        "\n",
        "        news_summary = \"\\n\".join([\n",
        "            f\"- {a.get('title','')}: {a.get('description') or a.get('summary','')}\"\n",
        "            for a in news_articles[:5]\n",
        "        ])\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following recent news about {ticker}:\n",
        "\n",
        "{news_summary}\n",
        "\n",
        "Provide sentiment analysis and impact assessment. Return only the JSON.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get('sentiment_score', 0), 0.0))\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Earnings  (COMPLETED)\n",
        "# ------------------------------------------------------------------------------\n",
        "class EarningsAnalysisAgent(BaseAgent):\n",
        "    \"\"\"Analyzes earnings reports and patterns (EPS actual vs estimate, surprise history).\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Earnings Analysis Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a financial analyst specializing in earnings and fundamental analysis.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze the earnings series objectively (EPS actual vs. estimates, surprises).\n",
        "2. Identify recent beats/misses, average surprise, and beat ratio.\n",
        "3. Provide a fundamental strength score from -1 (very weak) to +1 (very strong).\n",
        "4. List concise key factors that justify the score.\n",
        "5. Be specific with numbers when available.\n",
        "\n",
        "EXPECTED JSON SCHEMA:\n",
        "{\n",
        "  \"fundamental_score\": float,   // -1..+1\n",
        "  \"analysis\": string,\n",
        "  \"key_factors\": [string],\n",
        "  \"confidence\": float           // 0..1\n",
        "}\n",
        "\n",
        "SCORING HINTS:\n",
        "- Strong positive if repeated beats, positive average surprise, improving trend.\n",
        "- Negative if repeated misses, negative average surprise, deteriorating margins (if provided).\n",
        "- Neutral if mixed or sparse data.\n",
        "\n",
        "Return ONLY valid JSON with keys: fundamental_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get(\"ticker\", \"UNKNOWN\")\n",
        "        rows = data.get(\"earnings\", []) or []\n",
        "\n",
        "        # Compact tabular summary to feed the model (top 8 most recent already supplied upstream)\n",
        "        def row_line(r: Dict[str, Any]) -> str:\n",
        "            return (\n",
        "                f\"- {r.get('date','?')}: estimate={r.get('EPS Estimate','n/a')}, \"\n",
        "                f\"reported={r.get('Reported EPS','n/a')}, surprise%={r.get('Surprise(%)','n/a')}\"\n",
        "            )\n",
        "        table = \"\\n\".join(row_line(r) for r in rows[:12])\n",
        "\n",
        "        user_message = f\"\"\"Company: {ticker}\n",
        "\n",
        "Recent quarterly earnings (most recent first):\n",
        "{table}\n",
        "\n",
        "Analyze this history and return only the JSON object described in the schema.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get(\"fundamental_score\", 0.0), 0.0))\n",
        "            analysis = result.get(\"analysis\", raw)\n",
        "            key_factors = result.get(\"key_factors\", [])\n",
        "            confidence = normalize_conf(result.get(\"confidence\", 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Technicals\n",
        "# -----------------------------------------------------------------------------\n",
        "class MarketSignalsAgent(BaseAgent):\n",
        "    \"\"\"Performs technical analysis on market data\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Market Signals Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a technical analyst specializing in market signals and price patterns.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze technical indicators objectively\n",
        "2. Assess technical strength from -1 (very bearish) to +1 (very bullish)\n",
        "3. Identify support/resistance levels\n",
        "4. Evaluate trend direction and momentum\n",
        "5. Consider volume patterns\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"technical_score\": 0.65,\n",
        "  \"analysis\": \"Bullish technical setup with price above key moving averages\",\n",
        "  \"key_factors\": [\"Price above 50-day MA\", \"RSI indicates strength\", \"Volume confirming uptrend\"],\n",
        "  \"confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: technical_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'UNKNOWN')\n",
        "        technicals = data.get('technicals', {})\n",
        "\n",
        "        technical_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Current Price: ${technicals.get('current_price', 'N/A')}\n",
        "50-day MA: ${technicals.get('ma_50', 'N/A')}\n",
        "200-day MA: ${technicals.get('ma_200', 'N/A')}\n",
        "RSI: {technicals.get('rsi', 'N/A')}\n",
        "MACD: {technicals.get('macd', 'N/A')}\n",
        "Volume: {technicals.get('volume', 'N/A')} (Avg: {technicals.get('avg_volume', 'N/A')})\n",
        "Support: ${technicals.get('support', 'N/A')}\n",
        "Resistance: ${technicals.get('resistance', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following technical data for {ticker}:\n",
        "\n",
        "{technical_summary}\n",
        "\n",
        "Assess technical strength and price momentum. Return only the JSON described above.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get('technical_score', 0), 0.0))\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Risk  (COMPLETED)\n",
        "# -----------------------------------------------------------------------------\n",
        "class RiskAssessmentAgent(BaseAgent):\n",
        "    \"\"\"Assesses investment risk and portfolio fit\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Risk Assessment Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a risk management analyst specializing in portfolio risk assessment.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze risk metrics objectively.\n",
        "2. Provide a risk level score from 0 (very low risk) to 1 (very high risk).\n",
        "3. Identify key risk drivers (beta, volatility, VaR, Sharpe, max drawdown, concentration/correlation).\n",
        "4. Explain portfolio implications and any risk mitigants.\n",
        "\n",
        "EXPECTED JSON SCHEMA:\n",
        "{\n",
        "  \"risk_score\": float,      // 0..1\n",
        "  \"analysis\": string,\n",
        "  \"key_factors\": [string],\n",
        "  \"confidence\": float       // 0..1\n",
        "}\n",
        "\n",
        "GUIDANCE:\n",
        "- Higher beta/volatility/drawdown/VaR => higher risk_score.\n",
        "- Higher Sharpe => lowers effective risk_score (risk-adjusted).\n",
        "- Lack of data => moderate confidence; be explicit.\n",
        "\n",
        "Return ONLY valid JSON with keys: risk_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'UNKNOWN')\n",
        "        risk_data = data.get('risk_metrics', {}) or {}\n",
        "\n",
        "        # Build a compact, explicit summary. We pass both short-term and full stats if provided.\n",
        "        risk_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Beta: {risk_data.get('beta', 'N/A')}\n",
        "Volatility (30-day): {risk_data.get('volatility', 'N/A')}%\n",
        "Sharpe Ratio: {risk_data.get('sharpe_ratio', 'N/A')}\n",
        "Max Drawdown (%): {risk_data.get('max_drawdown', 'N/A')}\n",
        "Value at Risk (5% daily return): {risk_data.get('var_5', 'N/A')}\n",
        "Sector Correlation: {risk_data.get('sector_correlation', 'N/A')}\n",
        "P/E Ratio: {risk_data.get('pe_ratio', 'N/A')}\n",
        "\n",
        "# Extended (may be None):\n",
        "Avg Daily Return: {risk_data.get('avg_daily_return', 'N/A')}\n",
        "Volatility (full window): {risk_data.get('volatility_full', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following risk metrics and return only the JSON per schema:\n",
        "\n",
        "{risk_summary}\n",
        "\n",
        "Give a 0..1 risk_score, analysis, key_factors (bullet-style phrases), and confidence.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "\n",
        "            # Keep 0..1 semantics but normalize/clamp\n",
        "            risk01 = to_float(result.get('risk_score', 0.5), 0.5)\n",
        "            if 1.0 < risk01 <= 100.0:\n",
        "                risk01 = risk01 / 100.0\n",
        "            elif 1.0 < risk01 <= 10.0:\n",
        "                risk01 = risk01 / 10.0\n",
        "            risk01 = clamp(risk01, 0.0, 1.0)\n",
        "\n",
        "            score = risk01\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.8))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.5\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Synthesis\n",
        "# -----------------------------------------------------------------------------\n",
        "class SynthesisAgent(BaseAgent):\n",
        "    \"\"\"Combines insights from all agents into final recommendation\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Research Synthesis Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a senior investment analyst who synthesizes multiple analyses into actionable recommendations.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review all agent analyses objectively\n",
        "2. Weigh different factors appropriately\n",
        "3. Provide clear investment recommendation (STRONG BUY, BUY, HOLD, SELL, STRONG SELL)\n",
        "4. State confidence level (0 to 1)\n",
        "5. Summarize key reasoning\n",
        "6. Note important risks\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"recommendation\": \"BUY\",\n",
        "  \"confidence\": 0.78,\n",
        "  \"analysis\": \"Strong fundamentals and positive technical signals support a buy recommendation despite moderate risk\",\n",
        "  \"key_points\": [\"Earnings beat expectations\", \"Technical breakout\", \"Acceptable risk profile\"],\n",
        "  \"risks\": [\"Market volatility\", \"Sector headwinds\"]\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: recommendation, confidence, analysis, key_points, risks\"\"\"\n",
        "\n",
        "    def process(self, agent_responses: List[AgentResponse]) -> AgentResponse:\n",
        "        analyses_summary = \"\\n\\n\".join([\n",
        "            f\"{resp.agent_name}:\\n\"\n",
        "            f\"Score: {resp.score}\\n\"\n",
        "            f\"Analysis: {resp.analysis}\\n\"\n",
        "            f\"Key Factors: {', '.join(resp.key_factors)}\"\n",
        "            for resp in agent_responses\n",
        "        ])\n",
        "\n",
        "        user_message = f\"\"\"Synthesize the following analyses into a final investment recommendation:\n",
        "\n",
        "{analyses_summary}\n",
        "\n",
        "Provide a comprehensive investment recommendation with supporting reasoning. Return only the JSON.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            recommendation = str(result.get('recommendation', 'HOLD')).upper()\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_points', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "\n",
        "            rec_to_score = {\n",
        "                'STRONG BUY': 1.0,\n",
        "                'BUY': 0.6,\n",
        "                'HOLD': 0.0,\n",
        "                'SELL': -0.6,\n",
        "                'STRONG SELL': -1.0\n",
        "            }\n",
        "            score = rec_to_score.get(recommendation, 0.0)\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Critique\n",
        "# -----------------------------------------------------------------------------\n",
        "class CritiqueAgent(BaseAgent):\n",
        "    \"\"\"Reviews and validates analysis quality\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"Critique & Validation Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a critique analyst who reviews investment recommendations for biases, logical errors, and completeness.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the synthesis objectively\n",
        "2. Identify logical inconsistencies\n",
        "3. Detect potential biases\n",
        "4. Note missing considerations\n",
        "5. Assess data quality\n",
        "6. Recommend confidence adjustments\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"quality_score\": 0.82,\n",
        "  \"issues_found\": [\"Limited macroeconomic analysis\"],\n",
        "  \"suggestions\": [\"Consider Federal Reserve policy impact\", \"Add sector comparison\"],\n",
        "  \"adjusted_confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: quality_score, issues_found, suggestions, adjusted_confidence\"\"\"\n",
        "\n",
        "    def process(self, synthesis_response: AgentResponse) -> AgentResponse:\n",
        "        user_message = f\"\"\"Review this investment analysis for quality and completeness:\n",
        "\n",
        "Recommendation: {synthesis_response.analysis}\n",
        "Confidence: {synthesis_response.confidence}\n",
        "Key Factors: {', '.join(synthesis_response.key_factors)}\n",
        "\n",
        "Identify any issues, biases, or missing elements. Return only the JSON.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            quality_score = to_float(result.get('quality_score', 0.7), 0.7)\n",
        "            # normalize 0..10 or 0..100 to 0..1 (display-style)\n",
        "            if 1.0 < quality_score <= 10.0:\n",
        "                quality_score = quality_score / 10.0\n",
        "            elif 10.0 < quality_score <= 100.0:\n",
        "                quality_score = quality_score / 100.0\n",
        "            quality_score = clamp(quality_score, 0.0, 1.0)\n",
        "\n",
        "            issues = result.get('issues_found', [])\n",
        "            suggestions = result.get('suggestions', [])\n",
        "            adjusted_confidence = normalize_conf(\n",
        "                result.get('adjusted_confidence', synthesis_response.confidence)\n",
        "            )\n",
        "\n",
        "            analysis = f\"Quality Score: {quality_score}\\n\"\n",
        "            if issues:\n",
        "                analysis += f\"Issues Found: {', '.join(issues)}\\n\"\n",
        "            if suggestions:\n",
        "                analysis += f\"Suggestions: {', '.join(suggestions)}\"\n",
        "\n",
        "            key_factors = issues if issues else [\"No major issues found\"]\n",
        "        except json.JSONDecodeError:\n",
        "            quality_score = 0.7\n",
        "            analysis = raw\n",
        "            adjusted_confidence = synthesis_response.confidence\n",
        "            key_factors = [\"No major issues found\"]\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(quality_score),\n",
        "            confidence=float(adjusted_confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14963947",
      "metadata": {
        "id": "14963947"
      },
      "source": [
        "# Orchestrator -> run the whole agentic pipeline, end-to-end\n",
        "\n",
        "\n",
        "&nbsp;**We build one “traffic controller” that runs the full flow for a ticker:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**fetch** data (prices, news, earnings, risk), clean and filter news,\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**Preprocess** news (clean text, add tags/numbers) and retrieve the most recent, relevant headlines,\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**Route**decide which agent lanes to run (news / technical / earnings / risk),\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**Run agents** for each lane and collect their JSON outputs.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**Synthesize** a first pass (synth_v1), then critique it\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**If the critique flags low quality or data issues, optimize** with a second pass (synth_v2) using the critique as explicit feedback.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;**Save memory** (light for the run) and package evidence DataFrames for the UI.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **Result:** every agent returns simple, parseable JSON with aligned scales, so synthesis and critique stay reliable. When the optimizer runs, the final answer is synth_v2; otherwise we keep synth_v1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "4a539fc5",
      "metadata": {
        "id": "4a539fc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "from datetime import datetime, timezone\n",
        "import time\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.data_io.news import fetch_news\n",
        "from src.data_io.indicators import fetch_indicator\n",
        "from src.data_io.earnings import fetch_earnings\n",
        "from src.data_io.risk import fetch_risk_metrics\n",
        "from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "from src.system.router import choose_agents\n",
        "from src.system.memory import append_memory\n",
        "from src.agents import (\n",
        "    NewsAnalysisAgent,\n",
        "    MarketSignalsAgent,\n",
        "    RiskAssessmentAgent,\n",
        "    SynthesisAgent,\n",
        "    CritiqueAgent,\n",
        "    AgentResponse,\n",
        "    EarningsAnalysisAgent,\n",
        ")\n",
        "\n",
        "# ------------- helpers -------------\n",
        "def _as_text(x):\n",
        "    \"\"\"We coerce any object into a readable string (pretty JSON for dict/list).\"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _as_list_of_text(x):\n",
        "    \"\"\"We normalize arbitrary input into a non-null list of strings.\"\"\"\n",
        "    if isinstance(x, list):\n",
        "        return [_as_text(i) for i in x]\n",
        "    if x is None:\n",
        "        return []\n",
        "    return [_as_text(x)]\n",
        "\n",
        "def now_utc_iso() -> str:\n",
        "    \"\"\"We return a stable, timezone-aware timestamp for logs and memory.\"\"\"\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "# We use a small stagger to be kind to API rate limits.\n",
        "_NET_STAGGER = float(getattr(SETTINGS, \"net_stagger_secs\", 0.5))\n",
        "\n",
        "@dataclass\n",
        "class OrchestratorResult:\n",
        "    \"\"\"We bundle everything the UI needs after one pipeline run.\"\"\"\n",
        "    plan: List[str]\n",
        "    evidence: Dict[str, DataFrame]\n",
        "    agent_outputs: List[AgentResponse]\n",
        "    final: AgentResponse\n",
        "    critique: AgentResponse\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    symbol: str,\n",
        "    start: str | None,\n",
        "    end: str | None,\n",
        "    required_tags: list[str] | None = None\n",
        ") -> OrchestratorResult:\n",
        "    # We keep the plan visible so the UI can show progress/explain steps.\n",
        "    plan = [\n",
        "        \"fetch_prices\", \"fetch_news\", \"fetch_earnings\", \"fetch_risk\",\n",
        "        \"preprocess\", \"classify_extract\", \"retrieve_topk\",\n",
        "        \"route\", \"run_agents\", \"synthesize\", \"critique\", \"save_memory\"\n",
        "    ]\n",
        "\n",
        "    # ---------------- 1) FETCH (staggered) ----------------\n",
        "    prices = fetch_prices(symbol, start, end);                      time.sleep(_NET_STAGGER)\n",
        "    news   = fetch_news(symbol);                                     time.sleep(_NET_STAGGER)\n",
        "    earn_df = fetch_earnings(symbol);                                time.sleep(_NET_STAGGER)\n",
        "    risk_ingested = fetch_risk_metrics(symbol, start, end);          time.sleep(_NET_STAGGER)\n",
        "\n",
        "    # ---------------- 2) PREPROCESS NEWS ----------------\n",
        "    # We clean the articles and add lightweight tags/numbers for filtering.\n",
        "    news_pp = add_tags_and_numbers(preprocess_news(news))\n",
        "\n",
        "    # ---------------- 3) RETRIEVAL ----------------\n",
        "    # We keep a small, recent slice for agents to read.\n",
        "    top_news = recent_topk(\n",
        "        news_pp,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags,\n",
        "    )\n",
        "\n",
        "    # ---------------- 4) ROUTE PRIMERS ----------------\n",
        "    has_news     = not top_news.empty\n",
        "    has_prices   = not prices.empty\n",
        "    has_earnings = (earn_df is not None) and (not earn_df.empty)\n",
        "\n",
        "    # We try indicators if we have prices or a provider key.\n",
        "    attempt_technicals = has_prices or bool(SETTINGS.alpha_api_key)\n",
        "\n",
        "    # ---------------- 5) INDICATORS (conditional) ----------------\n",
        "    rsi = sma20 = sma50 = sma200 = pd.DataFrame()\n",
        "    if attempt_technicals:\n",
        "        rsi    = fetch_indicator(symbol, \"RSI\", 14); time.sleep(_NET_STAGGER)\n",
        "        sma20  = fetch_indicator(symbol, \"SMA\", 20); time.sleep(_NET_STAGGER)\n",
        "        sma50  = fetch_indicator(symbol, \"SMA\", 50); time.sleep(_NET_STAGGER)\n",
        "        sma200 = fetch_indicator(symbol, \"SMA\", 200); time.sleep(_NET_STAGGER)\n",
        "\n",
        "    has_technicals = (not rsi.empty) or (not sma20.empty) or (not sma50.empty) or (not sma200.empty)\n",
        "\n",
        "    # We let the router decide which lanes to run (news/technical/earnings/risk).\n",
        "    lanes = choose_agents(has_news, has_prices, has_technicals, has_earnings)\n",
        "\n",
        "    # ---------------- 6) RUN AGENTS ----------------\n",
        "    outputs: List[AgentResponse] = []\n",
        "\n",
        "    # NEWS\n",
        "    if \"news\" in lanes and has_news:\n",
        "        # We map to the keys the NewsAnalysisAgent expects.\n",
        "        news_payload_records = (\n",
        "            top_news\n",
        "            .rename(columns={\"summary\": \"description\"})\n",
        "            .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "            .to_dict(orient=\"records\")\n",
        "        )\n",
        "        outputs.append(NewsAnalysisAgent().process({\"ticker\": symbol, \"news\": news_payload_records}))\n",
        "\n",
        "    # TECHNICALS\n",
        "    if \"technical\" in lanes and (has_technicals or has_prices):\n",
        "        # We compute a tiny snapshot of technical state.\n",
        "        current_price = float(prices[\"close\"].iloc[-1]) if has_prices else None\n",
        "        volume = int(prices[\"volume\"].iloc[-1]) if has_prices else None\n",
        "        avg_volume = int(prices[\"volume\"].tail(20).mean()) if has_prices else None\n",
        "\n",
        "        technicals = {\n",
        "            \"current_price\": current_price,\n",
        "            \"rsi\": (float(rsi[\"RSI\"].iloc[-1]) if not rsi.empty else None),\n",
        "            \"ma_50\": (float(sma50[\"SMA\"].iloc[-1]) if not sma50.empty else\n",
        "                      (float(sma20[\"SMA\"].iloc[-1]) if not sma20.empty else None)),\n",
        "            \"ma_200\": (float(sma200[\"SMA\"].iloc[-1]) if not sma200.empty else None),\n",
        "            \"macd\": None,      # reserved for future\n",
        "            \"volume\": volume,\n",
        "            \"avg_volume\": avg_volume,\n",
        "            \"support\": None,   # reserved for future\n",
        "            \"resistance\": None # reserved for future\n",
        "        }\n",
        "        outputs.append(MarketSignalsAgent().process({\"ticker\": symbol, \"technicals\": technicals}))\n",
        "\n",
        "    # EARNINGS\n",
        "    if \"earnings\" in lanes and has_earnings:\n",
        "        earn_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"earnings\": (\n",
        "                earn_df.sort_values(\"date\", ascending=False)\n",
        "                       .head(8)\n",
        "                       .to_dict(orient=\"records\")\n",
        "            )\n",
        "        }\n",
        "        outputs.append(EarningsAnalysisAgent().process(earn_payload))\n",
        "\n",
        "    # RISK (merge ingestion + a quick realized 30d vol for the UI)\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if has_prices else None\n",
        "    risk_payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\":              risk_ingested.get(\"beta\"),\n",
        "            \"volatility\":        vol_30d,                        # short-term display (%)\n",
        "            \"var_5\":             risk_ingested.get(\"var_5\"),\n",
        "            \"sharpe_ratio\":      risk_ingested.get(\"sharpe_ratio\"),\n",
        "            \"max_drawdown\":      risk_ingested.get(\"max_drawdown\"),\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\":          None,\n",
        "            \"avg_daily_return\":  risk_ingested.get(\"avg_daily_return\"),\n",
        "            \"volatility_full\":   risk_ingested.get(\"volatility\"),\n",
        "        }\n",
        "    }\n",
        "    outputs.append(RiskAssessmentAgent().process(risk_payload))\n",
        "\n",
        "    # ---------------- 7) SYNTHESIZE + CRITIQUE ----------------\n",
        "    synth_v1 = SynthesisAgent().process(outputs)     # first pass\n",
        "    crit     = CritiqueAgent().process(synth_v1)     # critique of first pass\n",
        "\n",
        "    # We gate synth_v2 behind a simple rule to avoid unnecessary extra calls.\n",
        "    needs_rerun = (crit.score < 0.90) or (\n",
        "        \"data quality\" in \" \".join(_as_list_of_text(crit.key_factors)).lower()\n",
        "    )\n",
        "\n",
        "    synth_final = synth_v1  # default to v1 unless we improve it\n",
        "    synth_v2 = None         # we keep a handle for telemetry/UI if needed\n",
        "\n",
        "    if needs_rerun:\n",
        "        # We turn the critique into an explicit feedback message the synthesizer can read.\n",
        "        critique_feedback = AgentResponse(\n",
        "            agent_name=\"Critique Feedback\",\n",
        "            analysis=_as_text(synth_v1.analysis) + \"\\n\\n[CRITIQUE]\\n\" + _as_text(crit.analysis),\n",
        "            score=crit.score,\n",
        "            confidence=crit.confidence,\n",
        "            key_factors=_as_list_of_text(crit.key_factors),\n",
        "            timestamp=now_utc_iso()\n",
        "        )\n",
        "        # We re-run synthesis with the feedback appended to the agent outputs.\n",
        "        synth_v2_inputs = outputs + [critique_feedback]\n",
        "        synth_v2 = SynthesisAgent().process(synth_v2_inputs)\n",
        "        synth_final = synth_v2\n",
        "\n",
        "    # ---------------- 8) MEMORY ----------------\n",
        "    # We store whether the optimizer path ran and both confidences for later review.\n",
        "    append_memory({\n",
        "        \"ticker\": symbol,\n",
        "        \"lanes\": lanes,\n",
        "        \"issues\": crit.key_factors,\n",
        "        \"final_confidence_v1\": synth_v1.confidence,\n",
        "        \"final_confidence_v2\": (synth_v2.confidence if synth_v2 else None),\n",
        "        \"optimizer_triggered\": bool(needs_rerun),\n",
        "        \"timestamp\": now_utc_iso()\n",
        "    })\n",
        "\n",
        "    # ---------------- 9) EVIDENCE FOR UI ----------------\n",
        "    earn_evidence = (\n",
        "        earn_df.sort_values(\"date\", ascending=False).head(8)\n",
        "        if has_earnings else pd.DataFrame()\n",
        "    )\n",
        "    risk_evidence = pd.DataFrame([risk_payload[\"risk_metrics\"]])\n",
        "\n",
        "    evidence = {\n",
        "        \"top_news\": top_news,\n",
        "        \"prices_tail\": prices.tail(5),\n",
        "        \"earnings_head\": earn_evidence,\n",
        "        \"risk_metrics\": risk_evidence,\n",
        "    }\n",
        "\n",
        "    # We add the initial synthesis as a separate output so the UI can compare v1 vs final.\n",
        "    outputs.append(AgentResponse(\n",
        "        agent_name=\"Initial Synthesis\",\n",
        "        analysis=_as_text(synth_v1.analysis),\n",
        "        score=float(synth_v1.score),\n",
        "        confidence=float(synth_v1.confidence),\n",
        "        key_factors=_as_list_of_text(synth_v1.key_factors),\n",
        "        timestamp=synth_v1.timestamp\n",
        "    ))\n",
        "\n",
        "    return OrchestratorResult(plan, evidence, outputs, synth_final, crit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ecb8452",
      "metadata": {
        "id": "1ecb8452"
      },
      "source": [
        "# Demonstration Notebook — Agentic Workflows in Action\n",
        "\n",
        "- **This notebook section demonstrates how the full agentic pipeline works in practice.**\n",
        "\n",
        "- **We already have the real orchestration logic defined above (run_pipeline in the orchestrator module), which runs automatically inside the system and the Gradio app.**\n",
        "\n",
        "- **However, in a notebook we want to visually and interactively show what happens inside that pipeline — step by step — for clarity, debugging, and presentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6f07aca2",
      "metadata": {
        "id": "6f07aca2",
        "outputId": "3d666d86-7d86-4f60-d912-5b09d83cc6ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[offline] Patched src.data_io.prices.fetch_prices → CSV/synthetic loader.\n",
            "\n",
            "****************************************\n",
            "  DEMONSTRATING 3 AGENTIC WORKFLOW PATTERNS (offline prices)\n",
            "****************************************\n",
            "\n",
            "Ticker: AAPL\n",
            "Date Range: 2025-09-20 → 2025-10-20\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 1: PROMPT CHAINING\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-20 → 2025-10-20\n",
            "================================================================================\n",
            "\n",
            "┌─ STEP 1/5: INGEST ─────────────────────────────────────────────┐\n",
            "│ Fetching news (provider: NEWS_SENTIMENT)                       │\n",
            "└─────────────────────────────────────────────────────────────────┘\n",
            "  fetched_articles:  19\n",
            "\n",
            "┌─ STEP 2/5: PREPROCESS ─────────────────────────────────────────┐\n",
            "  after_preprocess:  19\n",
            "\n",
            "┌─ STEP 3/5: CLASSIFY ───────────────────────────────────────────┐\n",
            "  after_tagging:     19\n",
            "\n",
            "┌─ STEP 4/5: EXTRACT ────────────────────────────────────────────┐\n",
            "  top_articles:      5\n",
            "\n",
            "┌─ STEP 5/5: SUMMARIZE ──────────────────────────────────────────┐\n",
            "  sentiment_score:   +0.85\n",
            "  confidence:        90%\n",
            "\n",
            "================================================================================\n",
            "PROMPT CHAINING COMPLETE\n",
            "Pattern: Raw → Clean → Tagged → Top-K → Analysis\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 2: PARALLEL EXECUTION\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-20 → 2025-10-20\n",
            "================================================================================\n",
            "\n",
            "[Preparation] Fetching base data (offline prices)…\n",
            "\n",
            "[Parallel] Running News + Technical + Risk + Earnings (4 agents)…\n",
            "  News       Score=+0.85  Conf=90%\n",
            "  Technical  Score=+0.00  Conf=20%\n",
            "  Risk       Score=+0.45  Conf=55%\n",
            "  Earnings   Score=+0.80  Conf=90%\n",
            "\n",
            "================================================================================\n",
            "PARALLEL EXECUTION COMPLETE (2.77s)\n",
            "Pattern: Agents run concurrently to shorten wall time.\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-20 → 2025-10-20\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] GENERATE: Running pipeline (prices are CSV/synthetic via monkey patch)…\n",
            "  initial_score:     +0.60\n",
            "  initial_conf:      72%\n",
            "\n",
            "[Phase 2] EVALUATE: Critique\n",
            "  quality_score:     0.68\n",
            "  adj_confidence:    65%\n",
            "  issues_found:      3\n",
            "\n",
            "[Phase 3] OPTIMIZE: Re-synthesized with critique feedback (v2)\n",
            "  final_score:       +0.60\n",
            "  final_conf:        72%\n",
            "  conf_change:       +0%\n",
            "\n",
            "================================================================================\n",
            "EVALUATOR-OPTIMIZER COMPLETE\n",
            "================================================================================\n",
            "\n",
            "\n",
            "########################################\n",
            "  ALL 3 WORKFLOW PATTERNS DEMONSTRATED\n",
            "########################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# src/system/workflows_offline.py\n",
        "# Offline workflows: prefer CSV prices (from UI saves or mock dir) over APIs.\n",
        "from __future__ import annotations\n",
        "\n",
        "# stdlib\n",
        "import json, time, traceback, os\n",
        "from datetime import datetime, timedelta, date\n",
        "from typing import List, Dict, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "# third-party\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# project\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.news import fetch_news\n",
        "from src.data_io.earnings import fetch_earnings\n",
        "from src.data_io.risk import fetch_risk_metrics\n",
        "from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "from src.agents import (\n",
        "    NewsAnalysisAgent,\n",
        "    MarketSignalsAgent,\n",
        "    RiskAssessmentAgent,\n",
        "    EarningsAnalysisAgent,\n",
        "    AgentResponse,\n",
        ")\n",
        "from src.system.orchestrator import run_pipeline, OrchestratorResult\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# CSV-first prices loader + monkey patch\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _as_text(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _print_kv(k: str, v) -> None:\n",
        "    print(f\"  {k:<18} {v}\")\n",
        "\n",
        "def _normalize_text(s: str) -> str:\n",
        "    s = _as_text(s).strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = s.strip(\"`\").strip()\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def _coerce_prices_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Normalize columns and types: require ['date','close','volume'].\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\"date\",\"close\",\"volume\"])\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Find a date-like column\n",
        "    date_cols = [c for c in df.columns if str(c).lower() in {\"date\",\"datetime\",\"timestamp\"}]\n",
        "    if not date_cols:\n",
        "        # attempt to infer if index is datetime-like\n",
        "        if isinstance(df.index, pd.DatetimeIndex):\n",
        "            df = df.reset_index().rename(columns={\"index\":\"date\"})\n",
        "            date_cols = [\"date\"]\n",
        "        else:\n",
        "            # give up\n",
        "            return pd.DataFrame(columns=[\"date\",\"close\",\"volume\"])\n",
        "\n",
        "    date_col = date_cols[0]\n",
        "    df[\"date\"] = pd.to_datetime(df[date_col]).dt.tz_localize(None)\n",
        "\n",
        "    # Close / adj close candidates\n",
        "    close_col = None\n",
        "    for c in [\"close\",\"Close\",\"adj_close\",\"Adj Close\",\"adjclose\"]:\n",
        "        if c in df.columns:\n",
        "            close_col = c\n",
        "            break\n",
        "    if close_col is None:\n",
        "        # sometimes 'price' or 'close_price'\n",
        "        for c in df.columns:\n",
        "            if \"close\" in str(c).lower() or str(c).lower() == \"price\":\n",
        "                close_col = c\n",
        "                break\n",
        "    if close_col is None:\n",
        "        return pd.DataFrame(columns=[\"date\",\"close\",\"volume\"])\n",
        "\n",
        "    # Volume\n",
        "    vol_col = None\n",
        "    for c in [\"volume\",\"Volume\",\"vol\",\"Vol\"]:\n",
        "        if c in df.columns:\n",
        "            vol_col = c\n",
        "            break\n",
        "    if vol_col is None:\n",
        "        # allow missing; fill later\n",
        "        df[\"volume\"] = np.nan\n",
        "    else:\n",
        "        df[\"volume\"] = pd.to_numeric(df[vol_col], errors=\"coerce\")\n",
        "\n",
        "    df[\"close\"] = pd.to_numeric(df[close_col], errors=\"coerce\")\n",
        "    df = df.loc[~df[\"close\"].isna()].drop_duplicates(subset=[\"date\"]).sort_values(\"date\")\n",
        "    return df[[\"date\",\"close\",\"volume\"]]\n",
        "\n",
        "def _filter_range(df: pd.DataFrame, start: str, end: str) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    s = pd.to_datetime(start)\n",
        "    e = pd.to_datetime(end) + pd.Timedelta(days=1)  # inclusive end\n",
        "    return df[(df[\"date\"] >= s) & (df[\"date\"] < e)].reset_index(drop=True)\n",
        "\n",
        "def _find_latest_ui_prices_csv(symbol: str) -> Optional[Path]:\n",
        "    \"\"\"\n",
        "    Look under data/runs/ui_runs/* for the newest folder matching the symbol prefix\n",
        "    and return its prices.csv if present.\n",
        "    \"\"\"\n",
        "    ui_root = SETTINGS.runs_dir / \"ui_runs\"\n",
        "    if not ui_root.exists():\n",
        "        return None\n",
        "    # candidates like AAPL_20241020_101010\n",
        "    candidates = [p for p in ui_root.iterdir() if p.is_dir() and p.name.upper().startswith(symbol.upper() + \"_\")]\n",
        "    if not candidates:\n",
        "        return None\n",
        "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    for run_dir in candidates:\n",
        "        p = run_dir / \"prices.csv\"\n",
        "        if p.exists():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _mock_prices_from_dir(symbol: str) -> Optional[Path]:\n",
        "    \"\"\"\n",
        "    Optional project-level mock file, e.g. data/mock/prices/AAPL.csv\n",
        "    \"\"\"\n",
        "    base = getattr(SETTINGS, \"data_dir\", Path(\"data\"))\n",
        "    for rel in [\n",
        "        Path(\"mock/prices\") / f\"{symbol.upper()}.csv\",\n",
        "        Path(\"mocks/prices\") / f\"{symbol.upper()}.csv\",\n",
        "        Path(\"prices\") / f\"{symbol.upper()}.csv\",\n",
        "    ]:\n",
        "        p = Path(base) / rel\n",
        "        if p.exists():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _load_prices_from_csv(symbol: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Try (1) latest UI run prices.csv, then (2) data/mock/prices/SYMBOL.csv.\n",
        "    Return filtered, normalized DataFrame with ['date','close','volume'].\n",
        "    \"\"\"\n",
        "    path = _find_latest_ui_prices_csv(symbol) or _mock_prices_from_dir(symbol)\n",
        "    if path is None:\n",
        "        return pd.DataFrame(columns=[\"date\",\"close\",\"volume\"])\n",
        "\n",
        "    try:\n",
        "        raw = pd.read_csv(path)\n",
        "    except Exception:\n",
        "        return pd.DataFrame(columns=[\"date\",\"close\",\"volume\"])\n",
        "\n",
        "    df = _coerce_prices_df(raw)\n",
        "    df = _filter_range(df, start, end)\n",
        "    return df\n",
        "\n",
        "def _synthetic_prices(symbol: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"Deterministic synthetic series (seeded by symbol) for demos without CSV.\"\"\"\n",
        "    rng = np.random.default_rng(abs(hash(symbol)) % (2**32))\n",
        "    idx = pd.date_range(start, end, freq=\"B\")  # business days\n",
        "    if len(idx) == 0:\n",
        "        return pd.DataFrame(columns=[\"date\",\"close\",\"volume\"])\n",
        "\n",
        "    base = 100 + (abs(hash(symbol)) % 500) / 10.0  # 100..150-ish base\n",
        "    steps = rng.normal(0, 0.01, size=len(idx))  # ~1% daily std\n",
        "    prices = base * np.exp(np.cumsum(steps))\n",
        "    volume = rng.integers(1_000_000, 10_000_000, size=len(idx))\n",
        "\n",
        "    df = pd.DataFrame({\"date\": idx, \"close\": prices, \"volume\": volume})\n",
        "    return df\n",
        "\n",
        "def fetch_prices_offline(symbol: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    CSV-first price loader. If not found, generates synthetic data so\n",
        "    downstream agents/risk have something useful to work with.\n",
        "    \"\"\"\n",
        "    df = _load_prices_from_csv(symbol, start, end)\n",
        "    if df.empty:\n",
        "        df = _synthetic_prices(symbol, start, end)\n",
        "        df = _filter_range(df, start, end)\n",
        "    return df\n",
        "\n",
        "def enable_offline_prices_monkey_patch() -> None:\n",
        "    \"\"\"\n",
        "    Patch src.data_io.prices.fetch_prices → fetch_prices_offline so any call\n",
        "    (including inside run_pipeline) uses CSV/synthetic instead of APIs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import src.data_io.prices as prices_mod\n",
        "        prices_mod.fetch_prices = fetch_prices_offline  # <-- the patch\n",
        "        print(\"[offline] Patched src.data_io.prices.fetch_prices → CSV/synthetic loader.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[offline] WARNING: Could not monkey-patch fetch_prices: {e}\")\n",
        "\n",
        "# Enable at import time so callers don’t have to remember.\n",
        "enable_offline_prices_monkey_patch()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# WORKFLOW 1: Prompt chaining (News focused; unchanged except prices aren’t needed)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_prompt_chaining_workflow(\n",
        "    symbol: str,\n",
        "    start: str,\n",
        "    end: str,\n",
        "    required_tags: list[str] | None = None\n",
        ") -> AgentResponse:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"WORKFLOW PATTERN 1: PROMPT CHAINING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} → {end}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if getattr(SETTINGS, \"skip_news\", False):\n",
        "        print(\"\\n[Notice] News fetching is disabled by SETTINGS.skip_news=True.\")\n",
        "        return AgentResponse(\n",
        "            agent_name=\"News Analysis Agent\",\n",
        "            analysis=\"News workflow skipped by configuration.\",\n",
        "            score=0.0,\n",
        "            confidence=0.95,\n",
        "            key_factors=[\"skip_news=True\"],\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    # 1) Ingest\n",
        "    print(\"\\n┌─ STEP 1/5: INGEST ─────────────────────────────────────────────┐\")\n",
        "    print(\"│ Fetching news (provider: NEWS_SENTIMENT)                       │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    raw_news = fetch_news(symbol)\n",
        "    _print_kv(\"fetched_articles:\", 0 if raw_news is None else len(raw_news))\n",
        "    if raw_news is None or raw_news.empty:\n",
        "        print(\"  No news data available.\")\n",
        "        return AgentResponse(\n",
        "            agent_name=\"News Analysis Agent\",\n",
        "            analysis=\"No news returned from provider.\",\n",
        "            score=0.0,\n",
        "            confidence=0.4,\n",
        "            key_factors=[\"no_news_data\"],\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    # 2) Preprocess\n",
        "    print(\"\\n┌─ STEP 2/5: PREPROCESS ─────────────────────────────────────────┐\")\n",
        "    clean = preprocess_news(raw_news)\n",
        "    _print_kv(\"after_preprocess:\", len(clean))\n",
        "\n",
        "    # 3) Tag & numbers\n",
        "    print(\"\\n┌─ STEP 3/5: CLASSIFY ───────────────────────────────────────────┐\")\n",
        "    tagged = add_tags_and_numbers(clean)\n",
        "    _print_kv(\"after_tagging:\", len(tagged))\n",
        "\n",
        "    # 4) Recent top-K (+ optional tag filter)\n",
        "    print(\"\\n┌─ STEP 4/5: EXTRACT ────────────────────────────────────────────┐\")\n",
        "    topk = recent_topk(\n",
        "        tagged,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags\n",
        "    )\n",
        "    _print_kv(\"top_articles:\", len(topk))\n",
        "\n",
        "    # 5) Summarize\n",
        "    print(\"\\n┌─ STEP 5/5: SUMMARIZE ──────────────────────────────────────────┐\")\n",
        "    payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"news\": (\n",
        "            topk.rename(columns={\"summary\": \"description\"})\n",
        "                .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "                .to_dict(\"records\")\n",
        "            if not topk.empty else []\n",
        "        ),\n",
        "    }\n",
        "    res = NewsAnalysisAgent().process(payload)\n",
        "    _print_kv(\"sentiment_score:\", f\"{res.score:+.2f}\")\n",
        "    _print_kv(\"confidence:\", f\"{res.confidence:.0%}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PROMPT CHAINING COMPLETE\")\n",
        "    print(\"Pattern: Raw → Clean → Tagged → Top-K → Analysis\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    return res\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# WORKFLOW 2: Parallel execution (uses CSV/synthetic prices via offline loader)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_parallel_workflow(symbol: str, start: str, end: str) -> List[AgentResponse]:\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"WORKFLOW PATTERN 2: PARALLEL EXECUTION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} → {end}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n[Preparation] Fetching base data (offline prices)…\")\n",
        "    prices = fetch_prices_offline(symbol, start, end)  # <— CSV-first\n",
        "    news = pd.DataFrame()\n",
        "    if not getattr(SETTINGS, \"skip_news\", False):\n",
        "        news = fetch_news(symbol)\n",
        "    earnings = fetch_earnings(symbol)\n",
        "    risk_ingested = fetch_risk_metrics(symbol, start, end)\n",
        "\n",
        "    # News agent input\n",
        "    news_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"news\": (\n",
        "            news.head(5).rename(columns={\"summary\": \"description\"})\n",
        "                .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "                .to_dict(\"records\")\n",
        "            if not news.empty else []\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # Technicals (price-only snapshot)\n",
        "    tech_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"technicals\": {\n",
        "            \"current_price\": float(prices[\"close\"].iloc[-1]) if not prices.empty else None,\n",
        "            \"volume\": int(prices[\"volume\"].iloc[-1]) if (not prices.empty and not pd.isna(prices[\"volume\"].iloc[-1])) else None,\n",
        "            \"avg_volume\": int(prices[\"volume\"].tail(20).mean()) if (not prices.empty and not prices[\"volume\"].tail(20).isna().all()) else None,\n",
        "            \"rsi\": None, \"ma_50\": None, \"ma_200\": None,\n",
        "            \"macd\": None, \"support\": None, \"resistance\": None,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Risk: realized 30d vol from prices + ingested metrics\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if not prices.empty else None\n",
        "    risk_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\":              risk_ingested.get(\"beta\"),\n",
        "            \"volatility\":        vol_30d,\n",
        "            \"var_5\":             risk_ingested.get(\"var_5\"),\n",
        "            \"sharpe_ratio\":      risk_ingested.get(\"sharpe_ratio\"),\n",
        "            \"max_drawdown\":      risk_ingested.get(\"max_drawdown\"),\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\":           None,\n",
        "            \"avg_daily_return\":   risk_ingested.get(\"avg_daily_return\"),\n",
        "            \"volatility_full\":    risk_ingested.get(\"volatility\"),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    earn_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"earnings\": (\n",
        "            earnings.sort_values(\"date\", ascending=False).head(8).to_dict(\"records\")\n",
        "            if earnings is not None and not earnings.empty else []\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    print(\"\\n[Parallel] Running News + Technical + Risk + Earnings (4 agents)…\")\n",
        "    t0 = time.time()\n",
        "    futures = {}\n",
        "    with ThreadPoolExecutor(max_workers=4) as pool:\n",
        "        if not getattr(SETTINGS, \"skip_news\", False):\n",
        "            futures[\"news\"] = pool.submit(NewsAnalysisAgent().process, news_input)\n",
        "        futures[\"technical\"] = pool.submit(MarketSignalsAgent().process, tech_input)\n",
        "        futures[\"risk\"] = pool.submit(RiskAssessmentAgent().process, risk_input)\n",
        "        futures[\"earnings\"] = pool.submit(EarningsAnalysisAgent().process, earn_input)\n",
        "\n",
        "        results: Dict[str, AgentResponse] = {}\n",
        "        for name, fut in futures.items():\n",
        "            results[name] = fut.result()\n",
        "            print(f\"  {name.capitalize():<10} Score={results[name].score:+.2f}  Conf={results[name].confidence:.0%}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"PARALLEL EXECUTION COMPLETE ({elapsed:.2f}s)\")\n",
        "    print(\"Pattern: Agents run concurrently to shorten wall time.\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return list(results.values())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# WORKFLOW 3: Evaluator-Optimizer (wraps orchestrator; patched to CSV prices)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_evaluator_optimizer_workflow(\n",
        "    symbol: str,\n",
        "    start: str,\n",
        "    end: str,\n",
        "    required_tags: list[str] | None = None,\n",
        "    force_optimizer: bool = False\n",
        ") -> OrchestratorResult:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} → {end}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n[Phase 1] GENERATE: Running pipeline (prices are CSV/synthetic via monkey patch)…\")\n",
        "    result = run_pipeline(symbol, start, end, required_tags)\n",
        "\n",
        "    initial = next((a for a in result.agent_outputs if \"Initial Synthesis\" in a.agent_name), None)\n",
        "    if initial:\n",
        "        _print_kv(\"initial_score:\", f\"{initial.score:+.2f}\")\n",
        "        _print_kv(\"initial_conf:\", f\"{initial.confidence:.0%}\")\n",
        "\n",
        "    print(\"\\n[Phase 2] EVALUATE: Critique\")\n",
        "    _print_kv(\"quality_score:\", f\"{result.critique.score:.2f}\")\n",
        "    _print_kv(\"adj_confidence:\", f\"{result.critique.confidence:.0%}\")\n",
        "    _print_kv(\"issues_found:\", len(result.critique.key_factors))\n",
        "\n",
        "    optimizer_ran = initial and (_normalize_text(initial.analysis) != _normalize_text(result.final.analysis))\n",
        "    if force_optimizer:\n",
        "        optimizer_ran = True\n",
        "\n",
        "    if optimizer_ran:\n",
        "        print(\"\\n[Phase 3] OPTIMIZE: Re-synthesized with critique feedback (v2)\")\n",
        "        _print_kv(\"final_score:\", f\"{result.final.score:+.2f}\")\n",
        "        _print_kv(\"final_conf:\", f\"{result.final.confidence:.0%}\")\n",
        "        if initial:\n",
        "            delta = result.final.confidence - initial.confidence\n",
        "            _print_kv(\"conf_change:\", f\"{delta:+.0%}\")\n",
        "    else:\n",
        "        print(\"\\n[Phase 3] OPTIMIZE: Not needed (quality acceptable)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EVALUATOR-OPTIMIZER COMPLETE\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# DEMO\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def demo_all_workflows(symbol: str = \"AAPL\"):\n",
        "    start = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "    end = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    print(\"\\n\" + \"*\" * 40)\n",
        "    print(\"  DEMONSTRATING 3 AGENTIC WORKFLOW PATTERNS (offline prices)\")\n",
        "    print(\"*\" * 40)\n",
        "    print(f\"\\nTicker: {symbol}\")\n",
        "    print(f\"Date Range: {start} → {end}\\n\")\n",
        "\n",
        "    r1 = run_prompt_chaining_workflow(symbol, start, end)\n",
        "    r2 = run_parallel_workflow(symbol, start, end)\n",
        "    r3 = run_evaluator_optimizer_workflow(symbol, start, end, force_optimizer=False)\n",
        "\n",
        "    print(\"\\n\" + \"#\" * 40)\n",
        "    print(\"  ALL 3 WORKFLOW PATTERNS DEMONSTRATED\")\n",
        "    print(\"#\" * 40 + \"\\n\")\n",
        "\n",
        "    return {\"prompt_chaining\": r1, \"parallel\": r2, \"evaluator_optimizer\": r3}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_all_workflows(\"AAPL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f913beb",
      "metadata": {
        "id": "9f913beb"
      },
      "source": [
        "# Mock Gradio App — Evaluation and Iteration, Demonstration Interface for the Agentic Pipeline\n",
        "\n",
        "- **This notebook section provides a mock Gradio user interface designed purely for demonstration and research purposes.**\n",
        "\n",
        "- **It uses saved data from the local directory:**\n",
        "\n",
        "   data/runs/ui_runs/\n",
        "\n",
        "- **to showcase what a full agentic analysis would look like after execution.**\n",
        "\n",
        "\n",
        "### If we want a fully interactive and functional agentic app — where agents fetch live data, process it, and collaborate in real-time we must use the layered codebase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f61af973",
      "metadata": {
        "id": "f61af973",
        "outputId": "917422ff-5b03-41c5-ae14-0e14dad1d2c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7865\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ui/gradio_app.py — Gradio front-end for the agentic pipeline\n",
        "# We run one ticker analysis, render agent outputs + evidence, and\n",
        "# add simple persistence (Save / Load last run) for quick comparisons.\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "# --- stdlib / paths ---\n",
        "import os, sys, traceback, socket, json\n",
        "from datetime import date, timedelta, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 3rd-party ---\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "# --- import path shim (repo root) ---\n",
        "# We make sure ../ is importable so src/ modules resolve when we run from ui/.\n",
        "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
        "\n",
        "# --- project imports ---\n",
        "from src.system.orchestrator import run_pipeline\n",
        "from src.config.settings import SETTINGS  # used for runs_dir\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Persistence (Save / Load) — we keep lightweight artifacts under data/runs/ui_runs\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# We store UI runs under SETTINGS.runs_dir/ui_runs so the app and orchestrator share a root.\n",
        "RUNS_UI_DIR = SETTINGS.runs_dir / \"ui_runs\"\n",
        "RUNS_UI_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _df_to_csv(path: Path, df: pd.DataFrame) -> None:\n",
        "    \"\"\"We write a DataFrame to CSV if it exists and is non-empty.\"\"\"\n",
        "    try:\n",
        "        if df is None or (hasattr(df, \"empty\") and df.empty):\n",
        "            return\n",
        "        df.to_csv(path, index=False)\n",
        "    except Exception:\n",
        "        # We keep the UI robust — no hard crashes on save failure.\n",
        "        pass\n",
        "\n",
        "def _df_from_csv(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"We read a CSV into a DataFrame; return empty frame on any error.\"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(path) if path.exists() else pd.DataFrame()\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def save_current_run(symbol, days_back, tags,\n",
        "                     plan_txt, agents_txt, crit_txt, final_txt,\n",
        "                     news_df, prices_df, earnings_df, risk_df) -> str:\n",
        "    \"\"\"We persist the current UI state into a timestamped folder.\"\"\"\n",
        "    # Guard: only save if something meaningful is present.\n",
        "    if not any([plan_txt, agents_txt, crit_txt, final_txt]):\n",
        "        return \"Nothing to save yet. Run the analysis first.\"\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = RUNS_UI_DIR / f\"{(symbol or 'UNKNOWN').strip()}_{ts}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # We keep a small JSON meta for quick reloads.\n",
        "    meta = {\n",
        "        \"symbol\": symbol,\n",
        "        \"days_back\": int(days_back) if str(days_back).strip() else None,\n",
        "        \"tags\": tags,\n",
        "        \"timestamp\": ts,\n",
        "        \"plan\": plan_txt,\n",
        "        \"agents\": agents_txt,\n",
        "        \"critique\": crit_txt,\n",
        "        \"final\": final_txt,\n",
        "    }\n",
        "    (run_dir / \"meta.json\").write_text(\n",
        "        json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    # Evidence tables (optional; saved if non-empty)\n",
        "    _df_to_csv(run_dir / \"news.csv\", news_df)\n",
        "    _df_to_csv(run_dir / \"prices.csv\", prices_df)\n",
        "    _df_to_csv(run_dir / \"earnings.csv\", earnings_df)\n",
        "    _df_to_csv(run_dir / \"risk.csv\", risk_df)\n",
        "\n",
        "    return f\"Saved to: {run_dir}\"\n",
        "\n",
        "def load_last_run():\n",
        "    \"\"\"We load the newest saved run and return both text panels and tables, plus input defaults.\"\"\"\n",
        "    runs = [p for p in RUNS_UI_DIR.iterdir() if p.is_dir()]\n",
        "    if not runs:\n",
        "        # When nothing saved yet, return empty panels and sane input defaults.\n",
        "        return (\n",
        "            \"\", \"\", \"\", \"\",\n",
        "            pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(),\n",
        "            \"\", 30, \"\"   # days_back must be int for the Slider\n",
        "        )\n",
        "\n",
        "    # Newest first by mtime\n",
        "    runs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    run_dir = runs[0]\n",
        "\n",
        "    meta_path = run_dir / \"meta.json\"\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
        "\n",
        "    plan_txt   = meta.get(\"plan\", \"\")\n",
        "    agents_txt = meta.get(\"agents\", \"\")\n",
        "    crit_txt   = meta.get(\"critique\", \"\")\n",
        "    final_txt  = meta.get(\"final\", \"\")\n",
        "    symbol     = str(meta.get(\"symbol\", \"\") or \"\")\n",
        "    tags       = str(meta.get(\"tags\", \"\") or \"\")\n",
        "\n",
        "    # We coerce days_back to int for the Slider widget.\n",
        "    raw_days = meta.get(\"days_back\", 30)\n",
        "    try:\n",
        "        days_back = int(raw_days)\n",
        "    except Exception:\n",
        "        days_back = 30\n",
        "\n",
        "    # Evidence tables\n",
        "    news_df     = _df_from_csv(run_dir / \"news.csv\")\n",
        "    prices_df   = _df_from_csv(run_dir / \"prices.csv\")\n",
        "    earnings_df = _df_from_csv(run_dir / \"earnings.csv\")\n",
        "    risk_df     = _df_from_csv(run_dir / \"risk.csv\")\n",
        "\n",
        "    return (plan_txt, agents_txt, crit_txt, final_txt,\n",
        "            news_df, prices_df, earnings_df, risk_df,\n",
        "            symbol, days_back, tags)\n",
        "\n",
        "def _apply_loaded(plan_txt, agents_txt, crit_txt, final_txt,\n",
        "                  news_df, prices_df, earnings_df, risk_df,\n",
        "                  sym, days, tagstr):\n",
        "    \"\"\"We push loaded values into UI components; ensure Slider gets a number.\"\"\"\n",
        "    try:\n",
        "        days_val = int(days)\n",
        "    except Exception:\n",
        "        days_val = 30\n",
        "    status = f\"Loaded last run from: {RUNS_UI_DIR}\"\n",
        "    return (\n",
        "        plan_txt, agents_txt, crit_txt, final_txt,\n",
        "        news_df, prices_df, earnings_df, risk_df,\n",
        "        gr.update(value=str(sym or \"\")),\n",
        "        gr.update(value=days_val),            # Slider expects a numeric value\n",
        "        gr.update(value=str(tagstr or \"\")),\n",
        "        status\n",
        "    )\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Small UI helpers\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _truncate(s: str, max_len: int = 8000) -> str:\n",
        "    \"\"\"We shorten long text for responsive UI and websocket limits.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return (s[: max_len - 20] + \" … (truncated)\") if len(s) > max_len else s\n",
        "\n",
        "def _as_text(x):\n",
        "    \"\"\"We coerce anything to text; pretty-print lists/dicts.\"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    if isinstance(x, (dict, list)):\n",
        "        return json.dumps(x, ensure_ascii=False, indent=2, sort_keys=True)\n",
        "    return str(x)\n",
        "\n",
        "def _clean(s: str) -> str:\n",
        "    \"\"\"We normalize whitespace and strip accidental code fences.\"\"\"\n",
        "    s = _as_text(s).strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = s.strip(\"`\").strip()\n",
        "    return s\n",
        "\n",
        "def _synth_to_prose(obj):\n",
        "    \"\"\"We render a compact, readable summary when synthesis is a dict.\"\"\"\n",
        "    if not isinstance(obj, dict):\n",
        "        return _clean(_as_text(obj))\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    # --- Technicals block ---\n",
        "    ms = obj.get(\"market_signals\") or {}\n",
        "    if ms:\n",
        "        ms_bits = []\n",
        "        cp = ms.get(\"current_price\")\n",
        "        if isinstance(cp, (int, float)):\n",
        "            ms_bits.append(f\"price ${cp:,.2f}\")\n",
        "        ma = ms.get(\"moving_averages\") or {}\n",
        "        ma50 = ma.get(\"50_day\")\n",
        "        ma200 = ma.get(\"200_day\")\n",
        "        if (ma50 is not None) or (ma200 is not None):\n",
        "            ms_bits.append(f\"vs 50D {ma50}, 200D {ma200}\")\n",
        "        rsi = ms.get(\"RSI\")\n",
        "        if rsi is not None:\n",
        "            ms_bits.append(f\"RSI {rsi}\")\n",
        "        trend = ms.get(\"trend\")\n",
        "        if trend:\n",
        "            ms_bits.append(trend)\n",
        "        vol = ms.get(\"volume\") or {}\n",
        "        vcur, vavg = vol.get(\"current\"), vol.get(\"average\")\n",
        "        if vcur is not None and vavg is not None:\n",
        "            ms_bits.append(f\"volume {vcur:,} vs avg {vavg:,}\")\n",
        "        if ms_bits:\n",
        "            parts.append(\"Technicals: \" + \", \".join(str(x) for x in ms_bits if x))\n",
        "\n",
        "    # --- News block ---\n",
        "    news = obj.get(\"news\") or {}\n",
        "    if news:\n",
        "        news_bits = []\n",
        "        for k in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "            if k in news:\n",
        "                news_bits.append(f\"{k}: {news[k]}\")\n",
        "        for k, v in news.items():\n",
        "            if k not in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                news_bits.append(f\"{k}: {v}\")\n",
        "        parts.append(\"News: \" + \"; \".join(news_bits))\n",
        "\n",
        "    # --- Risk block ---\n",
        "    risk = obj.get(\"risk_assessment\") or {}\n",
        "    if risk:\n",
        "        risk_bits = []\n",
        "        for k in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "            if k in risk:\n",
        "                risk_bits.append(f\"{k}: {risk[k]}\")\n",
        "        for k, v in risk.items():\n",
        "            if k not in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                risk_bits.append(f\"{k}: {v}\")\n",
        "        parts.append(\"Risk: \" + \"; \".join(risk_bits))\n",
        "\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "def _to_df(x):\n",
        "    \"\"\"We coerce any input to a DataFrame; return empty on failure.\"\"\"\n",
        "    if isinstance(x, pd.DataFrame):\n",
        "        return x\n",
        "    if x is None:\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.DataFrame(x)\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Core handler — run the orchestrator and format panels for the UI\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run(symbol, days_back, required_tags_csv):\n",
        "    \"\"\"We execute the pipeline and build text panels + evidence tables.\"\"\"\n",
        "    try:\n",
        "        # Inputs → time window and optional tag filter\n",
        "        start = (date.today() - timedelta(days=int(days_back))).isoformat()\n",
        "        end = date.today().isoformat()\n",
        "        tags = [t.strip() for t in required_tags_csv.split(\",\")] if required_tags_csv else None\n",
        "\n",
        "        # Orchestrator: returns final = synth_v2 if optimizer ran, else synth_v1\n",
        "        res = run_pipeline(symbol.strip().upper(), start, end, required_tags=tags)\n",
        "\n",
        "        # Did the optimizer re-synthesize? We compare the initial synthesis vs final text.\n",
        "        optimizer_ran = False\n",
        "        init = next(\n",
        "            (a for a in res.agent_outputs\n",
        "             if a.agent_name in {\"Initial Synthesis\", \"Research Synthesis Agent\", \"SynthesisAgent\"}),\n",
        "            None\n",
        "        )\n",
        "        if init is not None:\n",
        "            init_txt = _clean(_as_text(init.analysis))\n",
        "            final_txt_norm = _clean(_as_text(res.final.analysis))\n",
        "            optimizer_ran = (init_txt != final_txt_norm)\n",
        "\n",
        "        # Step plan\n",
        "        plan = \"\\n\".join([f\"• {step}\" for step in res.plan])\n",
        "\n",
        "        # Agents panel — we keep it compact and truncate long text\n",
        "        agents_txt = \"\\n\\n\".join([\n",
        "            (\n",
        "                f\"[{a.agent_name}] score={a.score:.2f} conf={a.confidence:.2f}\\n\"\n",
        "                f\"{_synth_to_prose(a.analysis) if ('synthesis' in a.agent_name.lower()) else _clean(_as_text(a.analysis))}\"\n",
        "            )\n",
        "            for a in res.agent_outputs\n",
        "        ])\n",
        "        agents_txt = _truncate(agents_txt, 15000)\n",
        "\n",
        "        # Evidence tables for UI\n",
        "        news_rows      = _to_df(res.evidence.get(\"top_news\", []))\n",
        "        prices_rows    = _to_df(res.evidence.get(\"prices_tail\", []))\n",
        "        earnings_rows  = _to_df(res.evidence.get(\"earnings_head\", []))\n",
        "        risk_rows      = _to_df(res.evidence.get(\"risk_metrics\", []))  # likely single-row DF\n",
        "\n",
        "        if news_rows.empty:\n",
        "            agents_txt += \"\\n\\n[Note] No news items matched filters or provider limits today.\"\n",
        "\n",
        "        # Critique panel\n",
        "        crit_txt = (\n",
        "            f\"[Critique]\\n\"\n",
        "            f\"score={res.critique.score:.2f} adj_conf={res.critique.confidence:.2f}\\n\"\n",
        "            f\"{_clean(_as_text(res.critique.analysis))}\"\n",
        "        )\n",
        "        crit_txt = _truncate(crit_txt, 6000)\n",
        "\n",
        "        # Final panel (labels v2 when optimizer ran)\n",
        "        headline = \"FINAL (v2 after Critique)\" if optimizer_ran else \"FINAL (v1)\"\n",
        "        opt_line = \"[Optimizer ran: YES]\" if optimizer_ran else \"[Optimizer ran: NO]\"\n",
        "        final_txt = (\n",
        "            f\"{headline}\\n{opt_line}\\n\"\n",
        "            f\"score={res.final.score:.2f} conf={res.final.confidence:.2f}\\n\"\n",
        "            f\"{_synth_to_prose(res.final.analysis)}\\n\\nKey: {', '.join(res.final.key_factors)}\"\n",
        "        )\n",
        "        final_txt = _truncate(final_txt, 8000)\n",
        "\n",
        "        # Return order MUST match the outputs wiring below.\n",
        "        return (\n",
        "            plan,\n",
        "            agents_txt,\n",
        "            crit_txt,\n",
        "            final_txt,\n",
        "            news_rows,\n",
        "            prices_rows,\n",
        "            earnings_rows,\n",
        "            risk_rows\n",
        "        )\n",
        "\n",
        "    except Exception:\n",
        "        # We catch all exceptions so the UI stays alive and shows the traceback.\n",
        "        tb = traceback.format_exc()\n",
        "        err = f\"[FATAL] An exception occurred in run():\\n{tb}\"\n",
        "        blank_df = pd.DataFrame()\n",
        "        return (\n",
        "            \"run() error — see Critique tab\",\n",
        "            _truncate(err, 15000),\n",
        "            _truncate(err, 6000),\n",
        "            _truncate(err, 8000),\n",
        "            blank_df, blank_df, blank_df, blank_df\n",
        "        )\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Gradio layout\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "with gr.Blocks(title=\"Agentic Finance\") as demo:\n",
        "    gr.Markdown(\"# Agentic Finance — Interactive Tester\")\n",
        "\n",
        "    # Inputs row\n",
        "    with gr.Row():\n",
        "        symbol = gr.Textbox(label=\"Ticker\", value=\"AAPL\")                 # we set the ticker\n",
        "        days_back = gr.Slider(7, 120, value=30, step=1, label=\"Days Back\")# we choose lookback window\n",
        "        tags = gr.Textbox(label=\"Required Tags (optional, comma-sep)\", placeholder=\"earnings, product\")\n",
        "    run_btn = gr.Button(\"Run\")\n",
        "\n",
        "    # Persistence controls\n",
        "    with gr.Row():\n",
        "        save_btn = gr.Button(\"Save run\")\n",
        "        load_btn = gr.Button(\"Load last run\")\n",
        "    save_status = gr.Textbox(label=\"Save/Load status\", interactive=False)\n",
        "\n",
        "    # Text panels\n",
        "    plan   = gr.Textbox(label=\"Plan\", lines=6)\n",
        "    agents = gr.Textbox(label=\"Agent Outputs\", lines=14)\n",
        "    crit   = gr.Textbox(label=\"Critique\", lines=8)\n",
        "    final  = gr.Textbox(label=\"Final Recommendation\", lines=10)\n",
        "\n",
        "    # Evidence tables\n",
        "    news_tbl     = gr.Dataframe(\n",
        "        headers=[\"published_at\",\"source\",\"title\",\"summary\",\"url\",\"overall_sentiment\",\"tags\",\"numbers\"],\n",
        "        label=\"Top News (evidence)\",\n",
        "        wrap=True\n",
        "    )\n",
        "    prices_tbl   = gr.Dataframe(label=\"Recent Prices (evidence)\")\n",
        "    earnings_tbl = gr.Dataframe(label=\"Earnings (evidence)\")\n",
        "    risk_tbl     = gr.Dataframe(label=\"Risk Metrics (evidence)\")\n",
        "\n",
        "    # Run click → pipeline\n",
        "    run_btn.click(\n",
        "        run,\n",
        "        inputs=[symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl]\n",
        "    )\n",
        "\n",
        "    # Save the current run (panels + tables + inputs)\n",
        "    save_btn.click(\n",
        "        save_current_run,\n",
        "        inputs=[symbol, days_back, tags, plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl],\n",
        "        outputs=[save_status],\n",
        "    )\n",
        "\n",
        "    # Load last run into both outputs and inputs\n",
        "    load_btn.click(\n",
        "        load_last_run,\n",
        "        inputs=[],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "    ).then(\n",
        "        _apply_loaded,\n",
        "        inputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags, save_status],\n",
        "    )\n",
        "\n",
        "    # Auto-load last run at app start (nice for demos)\n",
        "    demo.load(\n",
        "        load_last_run,\n",
        "        inputs=[],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "    ).then(\n",
        "        _apply_loaded,\n",
        "        inputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags, save_status],\n",
        "    )\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Launch\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _get_free_port(start=7860, end=7890):\n",
        "    \"\"\"We scan for a free localhost port so the app starts reliably.\"\"\"\n",
        "    for p in range(start, end + 1):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "            try:\n",
        "                s.bind((\"127.0.0.1\", p))\n",
        "                return p\n",
        "            except OSError:\n",
        "                continue\n",
        "    return None  # let Gradio auto-pick if nothing is free\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # We queue for concurrency; handle older Gradio signatures gracefully.\n",
        "    try:\n",
        "        demo.queue()\n",
        "    except TypeError:\n",
        "        try:\n",
        "            demo.queue(max_size=16)\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    port = _get_free_port()  # None → Gradio will auto-pick\n",
        "\n",
        "    try:\n",
        "        demo.launch(\n",
        "            share=False,\n",
        "            server_name=\"127.0.0.1\",\n",
        "            server_port=port,      # may be None\n",
        "            show_error=True\n",
        "        )\n",
        "    except OSError:\n",
        "        # Fallback: force auto-pick if the chosen port gets taken meanwhile\n",
        "        demo.launch(\n",
        "            share=False,\n",
        "            server_name=\"127.0.0.1\",\n",
        "            server_port=None,\n",
        "            show_error=True\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7892759",
      "metadata": {},
      "source": [
        "# Agentic Finance — Interactive Demonstration Summary\n",
        "\n",
        "- **This interactive experiment was executed using the Agentic Finance tester with the ticker AAPL, a 30-day look-back period**\n",
        "\n",
        "    - **The system automatically triggered the full agentic pipeline: data ingestion (prices, news, earnings, risk), preprocessing, classification, retrieval, routing, multi-agent execution, synthesis, critique, and final recommendation.**\n",
        "\n",
        "- **Agent Interactions**\n",
        "\n",
        "    - **News Analysis Agent:** Identified positive sentiment driven by strategic partnerships and exclusive content rights.\n",
        "\n",
        "    - **Market Signals Agent:** Reported a bullish technical pattern, with price action above key moving averages and an RSI indicating strong upward momentum.\n",
        "\n",
        "    - **Risk Assessment Agent:** Detected moderate risk: a negative Sharpe ratio and small negative average daily return imply weak risk-adjusted performance despite low volatility.\n",
        "\n",
        "    - **Synthesis Agent (Initial Synthesis):** Merged the optimistic signals from the news and technical agents with the moderate risk findings. The model recommended \"buy with caution\".\n",
        "\n",
        "    - **Critique Agent:** Flagged two key issues: insufficient coverage of the competitive landscape and incomplete risk analysis. Suggested adding macroeconomic context and assessing partnership sustainability.\n",
        "\n",
        "    - **Optimizer (Final Recommendation Synth v2):** After incorporating critique feedback, the final analysis maintained cautious optimism. The confidence level remained steady at 0.72, indicating consistent reliability across synthesis rounds.\n",
        "\n",
        "- **Evidence Integration**\n",
        "\n",
        "    - **Each agent supplied structured \"evidence tables\"** (news, prices, earnings, risk) so that the synthesis could align numeric data with narrative analysis.\n",
        "\n",
        "- **Current Limitations and Future Work**\n",
        "\n",
        "    - **Score Calibration: Some final scores** Agent confidence and scoring logic require fine-tuning so aggregate confidence reflects the combined evidence strength.\n",
        "\n",
        "    - **Earnings Fetch:** The absence of earnings information is one of the limitations known. A secondary data provider must be included in future versions.\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "    overall, this demonstration proves that the multi-agent system coordination going as planned. Collectively, these interactions build a balanced financial evaluation pipeline that is realistic. The organizer was able to achieve multi-agent reasoning, data alignment, and critique-based optimization, exemplifying how Agentic Finance is capable of creating adaptive and explainable market analysis to be used in future studies and incorporation into an enterprise.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c35539ff",
      "metadata": {},
      "source": [
        "### Acknowledgment: AI Assistance\n",
        "\n",
        "Parts of this project (e.g., debugging helper functions, improving documentation clarity, and suggesting modularization patterns) were supported using OpenAI ChatGPT (GPT-5).\n",
        "The team used AI-generated suggestions as guidance only — all code was reviewed, edited, and verified manually to ensure understanding and compliance with course requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbebmotQx-zP",
      "metadata": {
        "id": "cbebmotQx-zP"
      },
      "source": [
        "# References\n",
        "\n",
        "<div style=\"text-indent:-0.5in;margin-left:0.5in;\">\n",
        "Yahoo Inc (2025). <i>yfinance</i> [Computer software]. https://pypi.org/project/yfinance/\n",
        "</div>\n",
        "\n",
        "<div style=\"text-indent:-0.5in;margin-left:0.5in;\">\n",
        "Alpha Vantage Inc. (2025). <i>Alpha Vantage API (free tier)</i> [Web API]. https://www.alphavantage.co/\n",
        "</div>\n",
        "\n",
        "<div style=\"text-indent:-0.5in;margin-left:0.5in;\">\n",
        "Hugging Face. (2025). <i>Introduction (Unit 0 - 2): Agents course</i> [Online course module]. https://huggingface.co/learn/agents-course/en/unit0/introduction\n",
        "</div>\n",
        "\n",
        "<div style=\"text-indent:-0.5in;margin-left:0.5in;\">\n",
        "University of San Diego, MSAAI. (2025). <i>Presentation 7.1: Agentic AI</i> [Lecture video]. University of San Diego.\n",
        "</div>\n",
        "\n",
        "<div style=\"text-indent:-0.5in;margin-left:0.5in;\">\n",
        "University of San Diego, MSAAI. (2025). <i>Module 7 Lab</i> [Laboratory manual]. University of San Diego.\n",
        "</div>\n",
        "\n",
        "<div style=\"text-indent:-0.5in;margin-left:0.5in;\">\n",
        "OpenAI. (2025). <i>ChatGPT</i> [Large language model; used for Markdown text cell formatting]. https://chat.openai.com/\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "a520",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
