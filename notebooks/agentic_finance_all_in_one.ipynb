{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "kl2KAh90u7JO",
      "metadata": {
        "id": "kl2KAh90u7JO"
      },
      "source": [
        "# Market Research Agent — Price, Indicators, and News Pipeline\n",
        "\n",
        "This project builds a lightweight research workflow for equities: we fetch historical prices, compute common technical indicators (SMA, RSI), and pull headline summaries with basic sentiment. A small JSON cache keeps runs fast and reproducible while avoiding API rate limits. The notebook(s) walk through data loading, quick EDA, feature prep, and simple evaluation.\n",
        "\n",
        "**Course:** MSAAI 520-02 — Group 5  \n",
        "**Date:** October 18, 2025\n",
        "\n",
        "## Group Members\n",
        "- Ali Azizi  \n",
        "- Sunitha Kosireddy  \n",
        "- Victor Salcedo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8e8972",
      "metadata": {
        "id": "8e8e8972"
      },
      "source": [
        "# Agentic Finance — All-in-One Notebook\n",
        "\n",
        "**This notebook is a demonstration notebook showing the full workflow and plan of our project.**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;It summarizes the key components we developed across the Agentic Finance system, including data ingestion, analysis, orchestration, and user interface layers\n",
        "\n",
        "3 explicit workflow patterns\n",
        "-  5 working AI agents\n",
        "- Full data pipeline\n",
        "- Interactive UI\n",
        "- Professional code structure\n",
        "\n",
        "**To run the full interactive application, please use the Gradio app located at:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;/src/ui/gradio_app.py\n",
        "\n",
        "**Run it from the project root with:**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;python -m ui.gradio_app\n",
        "\n",
        "**This will test the agents and see live results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mh3fvvuDikLQ",
      "metadata": {
        "id": "Mh3fvvuDikLQ"
      },
      "source": [
        "# Environment Initialization — Project Layout and Import Path\n",
        "\n",
        "**Purpose**  \n",
        "Establish a stable, reproducible project structure and import path so modules under `src/` (agents, tools, cache) import cleanly in the live runtime and the exported PDF/HTML.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Run once at the top of the notebook. Defines the project root, declares `src/` (code) and `ui/` (presentation assets), and updates `sys.path` to enable `from src...` imports.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** None  \n",
        "- **Outputs:** Project path variables (`ROOT`, `SRC`, `UI`) and a printed confirmation of the root path  \n",
        "- **Side Effects:** Prepends the project root to `sys.path` so `src` is importable in this session\n",
        "\n",
        "**Preconditions / Postconditions**  \n",
        "- **Preconditions:** Notebook kernel can write to `/mnt/data`  \n",
        "- **Postconditions:** Imports such as `from src.data_io.cache import load_cache` resolve without ad-hoc path manipulation\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Filesystem restrictions will cause downstream file-creation cells to fail fast  \n",
        "- Re-running the cell is idempotent with respect to the import path\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Fixed root path ensures consistent behavior across runs  \n",
        "- A short printout confirms the active project root for reviewers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f452ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21f452ac",
        "outputId": "128d0883-843e-402b-9f2a-db342ace4189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /mnt/data/agentic-finance\n"
          ]
        }
      ],
      "source": [
        "# Setup: define project layout and enable package-style importsimport pathlib, sys\n",
        "# Fixed, reproducible project root within the notebook environment.\n",
        "ROOT = pathlib.Path(\"/mnt/data/agentic-finance\")\n",
        "\n",
        "# Conventional subdirectories:\n",
        "# - SRC: importable application code\n",
        "# - UI:  optional presentation assets\n",
        "SRC = ROOT / \"src\"\n",
        "UI = ROOT / \"ui\"\n",
        "\n",
        "# Ensure the project root is importable in this kernel session.\n",
        "# This allows statements like: from src.data_io.cache import load_cache\n",
        "sys.path.insert(0, str(ROOT))\n",
        "print(\"Project root:\", ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bQuv9IKH90Q_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQuv9IKH90Q_",
        "outputId": "6eb9ea1d-8314-4cbf-ab10-048a6ef8bcb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac59b6",
      "metadata": {
        "id": "1dac59b6"
      },
      "source": [
        "# Repository Discovery — Locate Project Root and Initialize Packages\n",
        "\n",
        "**Purpose**  \n",
        "Locate the repository root by searching upward for a `src/` directory, make it importable in this session, and ensure required subpackages exist with `__init__.py` files.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Run near the top of the notebook, after environment initialization. This cell standardizes imports regardless of where the notebook is launched within the repo.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** Current working directory  \n",
        "- **Outputs:** Ensured package directories: `src/`, `src/config`, `src/data_io`, `src/system`, `src/analysis`  \n",
        "- **Side Effects:** Prepends the repo root to `sys.path` so `from src...` imports resolve\n",
        "\n",
        "**Preconditions / Postconditions**  \n",
        "- **Preconditions:** The repository contains a `src/` folder somewhere at or above the current directory  \n",
        "- **Postconditions:** `src` and listed subpackages exist and are importable in this kernel\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- If no `src/` is found at or above the current directory, the loop terminates at the filesystem root; imports may fail later.  \n",
        "- Directory creation is idempotent; re-running the cell is safe.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Upward search removes dependence on launch location (e.g., running from `notebooks/` vs repo root).  \n",
        "- Empty `__init__.py` files make package boundaries explicit for reviewers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee2be1c7",
      "metadata": {
        "id": "ee2be1c7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Discover the repo root by walking upward until we find a folder containing \"src\".\n",
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"src\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "# Expose the repository root on sys.path so `from src...` works in this session.\n",
        "sys.path.insert(0, str(ROOT))  # make \"src\" importable\n",
        "\n",
        "# Ensure packages (empty __init__.py files)\n",
        "for p in [\n",
        "    ROOT / \"src\",\n",
        "    ROOT / \"src\" / \"config\",\n",
        "    ROOT / \"src\" / \"data_io\",\n",
        "    ROOT / \"src\" / \"system\",\n",
        "    ROOT / \"src\" / \"analysis\",\n",
        "\n",
        "\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    (p / \"__init__.py\").touch(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54efe4b1",
      "metadata": {
        "id": "54efe4b1"
      },
      "source": [
        "# Feature Engineering — SMA and RSI Computation\n",
        "\n",
        "**Purpose**  \n",
        "Provide lightweight, dependency-minimal technical indicators for downstream analysis and agent decisions: Simple Moving Average (SMA) and Relative Strength Index (RSI).\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by analysis and strategy cells that require rolling features over a price series. Implemented in `src/analysis/features.py` as pure functions to simplify testing and reuse.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `prices: pd.DataFrame` with a `close` column; `window: int` (SMA), `window: int=14` (RSI)  \n",
        "- **Outputs:** `pd.Series` aligned to `prices.index`  \n",
        "- **Side Effects:** None (no mutation of inputs)\n",
        "\n",
        "**Preconditions / Postconditions**  \n",
        "- **Preconditions:** `prices` is a DataFrame with a numeric `close` column  \n",
        "- **Postconditions:** Returned Series may begin with `NaN` for the warm-up period (window - 1 rows)\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Empty or `None` `prices` → returns an empty float Series  \n",
        "- Non-numeric `close` values will propagate `NaN` through pandas operations  \n",
        "- Division by zero in RSI guarded with a small epsilon (`1e-10`)\n",
        "\n",
        "**Complexity & Performance**  \n",
        "- Time complexity: O(n) per feature; vectorized with pandas/numpy  \n",
        "- Memory: O(n) for intermediate rolling windows\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Deterministic given inputs; single-responsibility functions with clear signatures  \n",
        "- Suitable for unit tests (e.g., constant series, monotonic up/down series)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "383f0b0b",
      "metadata": {
        "id": "383f0b0b"
      },
      "outputs": [],
      "source": [
        "# src/analysis/features.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_sma(prices: pd.DataFrame, window: int) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    return prices[\"close\"].rolling(window=window).mean()\n",
        "\n",
        "def compute_rsi(prices: pd.DataFrame, window: int = 14) -> pd.Series:\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    delta = prices[\"close\"].diff()\n",
        "    gain = np.where(delta > 0, delta, 0.0)\n",
        "    loss = np.where(delta < 0, -delta, 0.0)\n",
        "    gain_s = pd.Series(gain, index=prices.index)\n",
        "    loss_s = pd.Series(loss, index=prices.index)\n",
        "    avg_gain = gain_s.rolling(window=window).mean()\n",
        "    avg_loss = loss_s.rolling(window=window).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99347f63",
      "metadata": {
        "id": "99347f63"
      },
      "source": [
        "# News Preprocessing & Agent Utilities — Tagging, Number Extraction, Recency Filter, and Scoring Normalization\n",
        "\n",
        "**Purpose**  \n",
        "Normalize raw news into a consistent schema, attach lightweight tags and numeric snippets for quick triage, filter to recent/top items, and provide generic utilities for cleaning LLM outputs and normalizing scores/confidence.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by ingestion/evidence-gathering steps prior to synthesis and evaluation. Implemented as pure functions for testability in `Second approach` cell.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `pd.DataFrame` with `published_at`, `title`, `summary`, `url` (Alpha Vantage-style timestamps); free-text strings from models/tools  \n",
        "- **Outputs:** Cleaned DataFrame with `published_at` (UTC), `tags`, `numbers`; helper functions produce primitives (`list[str]`, `float`, `str`)  \n",
        "- **Side Effects:** None (no mutation of inputs; all functions copy/return new objects)\n",
        "\n",
        "**Preconditions / Postconditions**  \n",
        "- **Preconditions:** DataFrame has `title` and `url` columns; timestamps may be strings in `YYYYMMDDTHHMMSS`  \n",
        "- **Postconditions:**\n",
        "  - `preprocess_news` returns rows with valid titles/URLs and parsed UTC datetimes (NaT retained; filtered later)  \n",
        "  - `add_tags_and_numbers` adds `tags` and `numbers` columns  \n",
        "  - `recent_topk` returns most recent `topk` rows within a `days` window, optionally constrained by tag intersection\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Empty/None DataFrame → returns an empty, correctly-typed frame  \n",
        "- Bad timestamps → parsed as `NaT` (not dropped here; date windowing happens later)  \n",
        "- Tag requirements with no matches → falls back to recency sort without tag filter  \n",
        "- Score/confidence inputs that are non-numeric → mapped via heuristics or clamped\n",
        "\n",
        "**Complexity & Performance**  \n",
        "- Vectorized operations over columns; O(n) per pass (regex/rolling window not used here)  \n",
        "- Regex for number extraction limited to first 6 matches per item to cap output size\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Deterministic outcomes given inputs; functions are small, single-purpose, and documented  \n",
        "- UTC timestamps avoid timezone drift in downstream filters and evaluations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fdc78099",
      "metadata": {
        "id": "fdc78099"
      },
      "outputs": [],
      "source": [
        "# Second approach\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "# from config.settings import SETTINGS\n",
        "\n",
        "# -----------------------------\n",
        "# Existing tagging / preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "TAG_RULES = {\n",
        "    \"earnings\": [\"earnings\", \"eps\", \"guidance\", \"outlook\", \"quarter\", \"revenue\"],\n",
        "    \"product\":  [\"launch\", \"iphone\", \"chip\", \"feature\", \"service\"],\n",
        "    \"legal\":    [\"lawsuit\", \"regulator\", \"antitrust\", \"fine\", \"settlement\"],\n",
        "    \"macro\":    [\"inflation\", \"rates\", \"fed\", \"recession\", \"gdp\"]\n",
        "}\n",
        "\n",
        "def preprocess_news(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"published_at\",\"source\",\"title\",\"summary\",\"url\",\n",
        "            \"overall_sentiment\",\"tags\",\"numbers\"\n",
        "        ])\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Alpha Vantage format is like \"20251017T200143\"\n",
        "    # Parse with explicit format; keep timezone-aware for safety\n",
        "    df[\"published_at\"] = pd.to_datetime(\n",
        "        df[\"published_at\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\", utc=True\n",
        "    )\n",
        "\n",
        "    # Drop rows with no title/url; keep others (don’t drop NaT here — the date filter happens later)\n",
        "    df = df.dropna(subset=[\"title\",\"url\"]).drop_duplicates(subset=[\"url\"])\n",
        "    df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def classify_tags(text: str) -> list[str]:\n",
        "    text_l = text.lower()\n",
        "    tags = [k for k, kws in TAG_RULES.items() if any(kw in text_l for kw in kws)]\n",
        "    return tags or [\"general\"]\n",
        "\n",
        "NUM_RE = re.compile(r'(\\$?\\b\\d+(?:\\.\\d+)?%?)')\n",
        "\n",
        "def extract_numbers(text: str) -> list[str]:\n",
        "    return NUM_RE.findall(text or \"\")[:6]\n",
        "\n",
        "def add_tags_and_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    df[\"tags\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(classify_tags)\n",
        "    df[\"numbers\"] = (df[\"title\"] + \" \" + df[\"summary\"]).apply(extract_numbers)\n",
        "    return df\n",
        "\n",
        "def recent_topk(df: pd.DataFrame, topk: int, days: int, required_tags: list[str] | None = None) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Make an aware UTC cutoff; df['published_at'] is already UTC-aware\n",
        "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n",
        "    f = df[df[\"published_at\"] >= cutoff]\n",
        "\n",
        "    if required_tags:\n",
        "        want = [t.strip().lower() for t in required_tags]\n",
        "        f_tags = f[f[\"tags\"].apply(lambda ts: any(t in [x.lower() for x in ts] for t in want))]\n",
        "        f = f_tags if not f_tags.empty else f\n",
        "\n",
        "    return f.sort_values(\"published_at\", ascending=False).head(topk)\n",
        "\n",
        "# -----------------------------\n",
        "# NEW: shared agent utilities\n",
        "# -----------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "def strip_code_fences(s: str) -> str:\n",
        "    \"\"\"Remove leading/trailing ``` blocks (optionally ```json).\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.IGNORECASE)\n",
        "\n",
        "def to_float(x, default: float = 0.0) -> float:\n",
        "    \"\"\"Best-effort conversion of model outputs or strings to float.\"\"\"\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            xs = x.strip().lower()\n",
        "            # map common words to numeric anchors\n",
        "            if xs in (\"high\", \"strong\", \"bullish\", \"overbought\"):\n",
        "                return 0.8\n",
        "            if xs in (\"medium\", \"moderate\", \"neutral\"):\n",
        "                return 0.5\n",
        "            if xs in (\"low\", \"weak\", \"bearish\", \"oversold\"):\n",
        "                return 0.2\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def clamp(x: float, lo: float, hi: float) -> float:\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "def normalize_score(v: float) -> float:\n",
        "    \"\"\"\n",
        "    Normalize arbitrary score ranges to [-1, 1].\n",
        "    Heuristics:\n",
        "      - If already in [-1,1], keep.\n",
        "      - If in [0,1], map to [-1,1] via (v-0.5)*2.\n",
        "      - If in (1,100], treat as percent.\n",
        "      - If in (1,10], treat as 0-10 and map.\n",
        "      - Else, clamp.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        v = float(v)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    if -1.0 <= v <= 1.0:\n",
        "        return v\n",
        "    if 0.0 <= v <= 1.0:\n",
        "        return (v - 0.5) * 2.0\n",
        "    if 1.0 < v <= 100.0:\n",
        "        v01 = v / 100.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    if 1.0 < v <= 10.0:\n",
        "        v01 = v / 10.0\n",
        "        return (v01 - 0.5) * 2.0\n",
        "    return clamp(v, -1.0, 1.0)\n",
        "\n",
        "def normalize_conf(v) -> float:\n",
        "    \"\"\"Normalize any confidence-like value to [0,1].\"\"\"\n",
        "    f = to_float(v, 0.7)\n",
        "    if 1.0 < f <= 100.0:\n",
        "        f = f / 100.0\n",
        "    return clamp(f, 0.0, 1.0)\n",
        "\n",
        "# Optional: helpers to render structured dicts into strings (for external tools)\n",
        "def pretty_json_block(obj: dict, max_chars: int = 4000) -> str:\n",
        "    \"\"\"Return a fenced JSON markdown block, truncated for UI safety.\"\"\"\n",
        "    try:\n",
        "        js = json.dumps(obj, ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        js = str(obj)\n",
        "    if len(js) > max_chars:\n",
        "        js = js[: max_chars - 20] + \"\\n... (truncated)\"\n",
        "    return f\"```json\\n{js}\\n```\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a110bf",
      "metadata": {
        "id": "a2a110bf"
      },
      "source": [
        "# Configuration & Environment — Project Root Discovery, `.env` Loading, and Runtime Settings\n",
        "\n",
        "**Purpose**  \n",
        "Centralize runtime configuration (paths, API keys, tunables) with a robust project-root heuristic and optional `.env` loading so downstream modules don’t hardcode environment details.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Imported early by notebooks and agents. Lives in `src/config/settings.py` (or equivalent) and exposes an immutable `SETTINGS` instance.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** Current working directory or `__file__` for root discovery; optional `.env` at project root  \n",
        "- **Outputs:** `SETTINGS` dataclass with resolved paths/keys; ensured directories (`data/cache`, `data/runs`)  \n",
        "- **Side Effects:** Creates missing directories; loads environment variables from `.env` without overriding existing process env vars\n",
        "\n",
        "**Root Detection Logic**  \n",
        "- If running from a module file, ascend two parents from `__file__` (repo layout assumption)  \n",
        "- Else (Notebook/REPL), walk upward from CWD and treat the first directory containing both `src/` and `data/` as the project root  \n",
        "- Fallback to CWD if no match is found\n",
        "\n",
        "**Security & Configuration Notes**  \n",
        "- `.env` is optional and **should not** be committed with real secrets  \n",
        "- Environment variables set by the OS/container take precedence (we do not override)  \n",
        "- Default API key placeholders are provided for local testing; replace via env vars in production\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Missing `.env` is non-fatal (keys fall back to process env or defaults)  \n",
        "- Non-existent `data/` tree is created on import for cache/runs paths\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Deterministic root selection within the stated heuristics  \n",
        "- All tunables captured in a single dataclass for easy inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "74137240",
      "metadata": {
        "id": "74137240"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _find_project_root(start: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Walk upward to find the repo root heuristically.\n",
        "    Treat a folder containing both 'src' and 'data' as the root.\n",
        "    Fallback to the starting directory if not found.\n",
        "    \"\"\"\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# project root = repo root\n",
        "if \"__file__\" in globals():\n",
        "    ROOT = Path(__file__).resolve().parents[2]\n",
        "else:\n",
        "    # Notebook / REPL: start from CWD and auto-detect root\n",
        "    ROOT = _find_project_root(Path.cwd())\n",
        "\n",
        "load_dotenv(ROOT / \".env\", override=False)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Settings:\n",
        "    data_dir: Path = ROOT / \"data\"\n",
        "    cache_dir: Path = ROOT / \"data\" / \"cache\"\n",
        "    runs_dir: Path = ROOT / \"data\" / \"runs\"\n",
        "    alpha_api_key: str = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"BVGUKZR1MHVS0T6B\")\n",
        "    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"sk-proj-\")\n",
        "    news_window_days: int = 14\n",
        "    topk_news: int = 5\n",
        "    cache_ttl_minutes: int = 60\n",
        "\n",
        "SETTINGS = Settings()\n",
        "SETTINGS.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "SETTINGS.runs_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XIKxbGY5r1ce",
      "metadata": {
        "id": "XIKxbGY5r1ce"
      },
      "source": [
        "# Component — Simple JSON Cache (TTL + Atomic Writes)\n",
        "\n",
        "**Purpose**  \n",
        "Provide a lightweight on-disk cache to reduce redundant network calls and stabilize latency. Each cache entry is stored as a single JSON file with a timestamp for TTL-based expiry. Writes are atomic to avoid corruption.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by data fetchers (e.g., price/news downloads) prior to any external HTTP call. Implemented as simple functions for ease of reuse and testing.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `key: str` (file name stem), optional `ttl_minutes: int | None`, arbitrary JSON-serializable `data`  \n",
        "- **Outputs:** `load_cache` returns the stored payload (`data`) or `None`; `save_cache` returns `None`  \n",
        "- **Side Effects:** Creates/updates files under `SETTINGS.cache_dir` as `<key>.json`\n",
        "\n",
        "**Behavior**  \n",
        "- `load_cache(key, ttl_minutes=None)`  \n",
        "  - If file missing/corrupt → `None`  \n",
        "  - If `ttl_minutes is None` → returns the stored `data` (ignore age)  \n",
        "  - Else → returns `data` only if `(now - _ts) <= ttl_minutes * 60`  \n",
        "- `save_cache(key, data)`  \n",
        "  - Serializes as `{\"_ts\": epoch_seconds, \"data\": <payload>}`  \n",
        "  - Writes to a temp file then atomically replaces the target\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Invalid JSON / partial writes → treated as a cache miss (returns `None`)  \n",
        "- Non-serializable objects → coerced via `_json_default` (ISO-8601 for dates, `str()` fallback, `repr()` last resort)\n",
        "\n",
        "**Configuration & Tunables**  \n",
        "- `SETTINGS.cache_dir` controls storage location  \n",
        "- TTL per call via `ttl_minutes`; absence means “return whatever is present”\n",
        "\n",
        "**Security & Data Handling**  \n",
        "- Do not cache secrets/PII. Payload is plain JSON on disk.\n",
        "\n",
        "**Testability**  \n",
        "- Unit tests: miss→save→hit, TTL expiry path, corrupt file → miss, atomic replace behavior (temp file present)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6a42293c",
      "metadata": {
        "id": "6a42293c"
      },
      "outputs": [],
      "source": [
        "# Purpose: lightweight disk cache with TTL and atomic writes\n",
        "# Context: used by data fetchers (e.g., price downloads) to avoid repeat network calls\n",
        "# Notes: filenames derive from key under SETTINGS.cache_dir; payload stored as JSON\n",
        "\n",
        "# cache.py\n",
        "from __future__ import annotations\n",
        "import json, time\n",
        "from datetime import date, datetime\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def _cache_path(key: str) -> Path:\n",
        "    return SETTINGS.cache_dir / f\"{key}.json\"\n",
        "\n",
        "def _json_default(o: Any):\n",
        "    # datetime & pandas.Timestamp (subclass of datetime) → ISO 8601\n",
        "    if isinstance(o, (datetime, date)):\n",
        "        return o.isoformat()\n",
        "    # Fallback: make a best-effort string (covers Decimal, Path, Enum, etc.)\n",
        "    try:\n",
        "        return str(o)\n",
        "    except Exception:\n",
        "        return repr(o)\n",
        "\n",
        "def load_cache(key: str, ttl_minutes: int | None = None) -> Any | None:\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        if ttl_minutes is None:\n",
        "            return obj.get(\"data\")  # consistent: always return payload\n",
        "        if (time.time() - obj.get(\"_ts\", 0)) <= ttl_minutes * 60:\n",
        "            return obj.get(\"data\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def save_cache(key: str, data: Any) -> None:\n",
        "    p = _cache_path(key)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
        "    payload = {\"_ts\": time.time(), \"data\": data}\n",
        "    tmp.write_text(json.dumps(payload, ensure_ascii=False, default=_json_default), encoding=\"utf-8\")\n",
        "    tmp.replace(p)  # atomic on most OS/filesystems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "909a0729",
      "metadata": {},
      "source": [
        "# Risk Ingestion — Yahoo Finance OHLCV with On-Disk Caching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0747e75a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from typing import Dict, Any, Optional\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "\n",
        "def _daily_returns(df: pd.DataFrame) -> pd.Series:\n",
        "    if df is None or df.empty or \"close\" not in df:\n",
        "        return pd.Series(dtype=float)\n",
        "    return df[\"close\"].astype(float).pct_change()\n",
        "\n",
        "def _max_drawdown_pct(prices: pd.DataFrame) -> float:\n",
        "    if prices is None or prices.empty or \"close\" not in prices:\n",
        "        return float(\"nan\")\n",
        "    series = prices[\"close\"].astype(float)\n",
        "    roll_max = series.cummax()\n",
        "    drawdown = (series / roll_max) - 1.0\n",
        "    mdd = drawdown.min()\n",
        "    return float(round(mdd * 100.0, 3))\n",
        "\n",
        "def _beta_vs_bench(asset_rets: pd.Series, bench_rets: pd.Series) -> float:\n",
        "    m = pd.concat([asset_rets, bench_rets], axis=1).dropna()\n",
        "    if m.empty:\n",
        "        return float(\"nan\")\n",
        "    cov = np.cov(m.iloc[:, 0], m.iloc[:, 1])[0, 1]\n",
        "    var = np.var(m.iloc[:, 1])\n",
        "    if var == 0:\n",
        "        return float(\"nan\")\n",
        "    return float(cov / var)\n",
        "\n",
        "def fetch_risk_metrics(symbol: str, start: Optional[str], end: Optional[str], benchmark: str = \"^GSPC\") -> Dict[str, Any]:\n",
        "    cache_key = f\"risk_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return cached\n",
        "\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    if prices is None or prices.empty:\n",
        "        save_cache(cache_key, {})\n",
        "        return {}\n",
        "\n",
        "    rets = _daily_returns(prices).dropna()\n",
        "    if rets.empty:\n",
        "        save_cache(cache_key, {})\n",
        "        return {}\n",
        "\n",
        "    mean_ret = float(rets.mean())\n",
        "    vol = float(rets.std())\n",
        "    sharpe = float(mean_ret / vol) if vol > 0 else float(\"nan\")\n",
        "    mdd_pct = _max_drawdown_pct(prices)\n",
        "    var_5 = float(np.nanpercentile(rets.values, 5))\n",
        "\n",
        "    # Download benchmark with explicit auto_adjust to avoid FutureWarning\n",
        "    beta = float(\"nan\")\n",
        "    try:\n",
        "        bench = yf.download(\n",
        "            benchmark,\n",
        "            start=prices[\"date\"].min(),\n",
        "            end=prices[\"date\"].max(),\n",
        "            progress=False,\n",
        "            auto_adjust=False,   # <— key change\n",
        "            threads=False,\n",
        "        )\n",
        "        if isinstance(bench.columns, pd.MultiIndex):\n",
        "            bench.columns = [c[0].lower() for c in bench.columns]\n",
        "        else:\n",
        "            bench.columns = [c.lower() for c in bench.columns]\n",
        "        bench = bench.reset_index().rename(columns={\"Date\": \"date\"})\n",
        "        bench[\"date\"] = bench[\"date\"].astype(str)\n",
        "        bench_rets = bench[\"close\"].astype(float).pct_change().dropna()\n",
        "        n = min(len(rets), len(bench_rets))\n",
        "        beta = _beta_vs_bench(rets.tail(n).reset_index(drop=True), bench_rets.tail(n).reset_index(drop=True))\n",
        "    except Exception:\n",
        "        beta = float(\"nan\")\n",
        "\n",
        "    metrics = {\n",
        "        \"avg_daily_return\": round(mean_ret, 6),\n",
        "        \"volatility\": round(vol, 6),\n",
        "        \"sharpe_ratio\": round(sharpe, 3) if not np.isnan(sharpe) else float(\"nan\"),\n",
        "        \"max_drawdown\": mdd_pct,       # percent (negative)\n",
        "        \"var_5\": round(var_5, 6),\n",
        "        \"beta\": round(beta, 3) if not np.isnan(beta) else float(\"nan\"),\n",
        "    }\n",
        "    save_cache(cache_key, metrics)\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28824d0",
      "metadata": {},
      "source": [
        "# Earnings Ingestion — Yahoo Finance OHLCV with On-Disk Caching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d776edf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# src/data_io/earnings.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from typing import Dict, Any\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_earnings(symbol: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Quarterly earnings with EPS actual/estimate/surprise.\n",
        "    Columns: ['date','EPS Estimate','Reported EPS','Surprise(%)']\n",
        "    \"\"\"\n",
        "    cache_key = f\"earnings_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    try:\n",
        "        tk = yf.Ticker(symbol)\n",
        "        df = tk.earnings_dates\n",
        "        if df is None or getattr(df, \"empty\", True):\n",
        "            df = pd.DataFrame(columns=[\"Earnings Date\",\"EPS Estimate\",\"Reported EPS\",\"Surprise(%)\"])\n",
        "\n",
        "        # Normalize column named 'Earnings Date' -> 'date'\n",
        "        if \"Earnings Date\" in df.columns:\n",
        "            df = df.reset_index(drop=True).rename(columns={\"Earnings Date\": \"date\"})\n",
        "        elif df.index.name == \"Earnings Date\":\n",
        "            df = df.reset_index().rename(columns={\"Earnings Date\": \"date\"})\n",
        "        else:\n",
        "            if \"date\" not in df.columns:\n",
        "                df = df.reset_index().rename(columns={\"index\": \"date\"})\n",
        "\n",
        "        keep = [\"date\",\"EPS Estimate\",\"Reported EPS\",\"Surprise(%)\"]\n",
        "        for k in keep:\n",
        "            if k not in df.columns:\n",
        "                df[k] = None\n",
        "        df = df[keep].copy()\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    except Exception:\n",
        "        df = pd.DataFrame(columns=[\"date\",\"EPS Estimate\",\"Reported EPS\",\"Surprise(%)\"])\n",
        "\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vIV91U3JrnJa",
      "metadata": {
        "id": "vIV91U3JrnJa"
      },
      "source": [
        "# Price Ingestion — Yahoo Finance OHLCV with On-Disk Caching\n",
        "\n",
        "**Purpose**  \n",
        "Download OHLCV time series from Yahoo Finance (`yfinance`) and return a normalized DataFrame. Use a disk cache to avoid redundant network calls and smooth over API throttling.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Called by data preparation steps before feature engineering/EDA. Lives in `src/...` and is imported by notebooks and agents.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:**  \n",
        "  - `symbol: str` — e.g., `\"AAPL\"`  \n",
        "  - `start: str | None` — ISO-like date (e.g., `\"2020-01-01\"`) or `None`  \n",
        "  - `end: str | None` — ISO-like date or `None`  \n",
        "- **Outputs:** `pd.DataFrame` with columns: `date`, `open`, `high`, `low`, `close`, `adj_close`, `volume`  \n",
        "- **Side Effects:** Reads/writes JSON records under `SETTINGS.cache_dir` via `load_cache`/`save_cache`\n",
        "\n",
        "**Behavior**  \n",
        "- Compose a cache key from `(symbol, start, end)` and honor `SETTINGS.cache_ttl_minutes`.  \n",
        "- On cache hit, materialize a DataFrame from the cached JSON records.  \n",
        "- On miss, call `yfinance.download`, flatten a possible MultiIndex, standardize column names, coerce `date` to string (JSON-safe), and cache the result.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Network/throttle issues → function returns whatever `yfinance` yields (may be empty); subsequent calls can hit cache if a prior success exists.  \n",
        "- Unknown symbols or empty ranges → valid but empty DataFrame.  \n",
        "- Column shape variations (e.g., MultiIndex) → flattened defensively.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- The cached JSON (records orient) makes runs reproducible for a TTL window and simplifies inspection.  \n",
        "- Deterministic column naming aids downstream merging and plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fb6014f6",
      "metadata": {
        "id": "fb6014f6"
      },
      "outputs": [],
      "source": [
        "# Purpose: download OHLCV from Yahoo Finance and return a normalized DataFrame with caching\n",
        "# Context: called by data prep steps before features/EDA; avoids repeated network calls\n",
        "# Notes: flattens MultiIndex cols, standardizes names, stores json-serializable cache\n",
        "\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "def fetch_prices(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:\n",
        "    cache_key = f\"prices_{symbol}_{start}_{end}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "    df = yf.download(symbol, start=start, end=end, progress=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0].lower() for c in df.columns]\n",
        "    df = df.reset_index().rename(columns={\n",
        "        \"Date\": \"date\", \"open\":\"open\",\"high\":\"high\",\"low\":\"low\",\"close\":\"close\",\"adj close\":\"adj_close\",\"volume\":\"volume\"\n",
        "    })\n",
        "    df[\"date\"] = df[\"date\"].astype(str)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTEo0-OksEFN",
      "metadata": {
        "id": "UTEo0-OksEFN"
      },
      "source": [
        "# Technical Indicators — Alpha Vantage SMA/RSI with Cached Local Fallback\n",
        "\n",
        "**Purpose**  \n",
        "Retrieve daily SMA/RSI time series using Alpha Vantage when available, with a deterministic local-compute fallback (from Yahoo Finance OHLCV) to maintain functionality under API limits or missing keys.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by feature pipelines that require daily technical indicators. Implemented in `src/data_io/indicators.py` and consumed by analysis/agent steps.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:**  \n",
        "  - `symbol: str` — ticker (e.g., `\"AAPL\"`)  \n",
        "  - `indicator: {\"SMA\",\"RSI\"}`  \n",
        "  - `time_period: int` — lookback window (default `14`)  \n",
        "- **Outputs:** `pd.DataFrame` with columns `date` and `SMA` or `RSI`, sorted ascending by `date`  \n",
        "- **Side Effects:** Caches JSON records under `SETTINGS.cache_dir` by `(symbol, indicator, time_period)`\n",
        "\n",
        "**Behavior**  \n",
        "1. Attempt cache → return on hit (honors `SETTINGS.cache_ttl_minutes`).  \n",
        "2. If Alpha Vantage is unavailable (no key/unknown indicator) or rate-limited/error, compute locally from `fetch_prices` using `compute_sma` or `compute_rsi`.  \n",
        "3. On successful API call, normalize Alpha Vantage payload to a tidy DataFrame (parsed dates, numeric columns), sort ascending, cache, and return.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Network errors / quota messages / malformed payload → fallback to local compute.  \n",
        "- Empty price data in fallback path → return empty DataFrame.  \n",
        "- Non-parsable dates or numeric fields → coerced with `errors=\"coerce\"` and dropped.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Cache persists list-of-dict records for deterministic reloads during the TTL window.  \n",
        "- Dates normalized to `datetime64[ns]`; output sorted for stable joins/plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "752d66ae",
      "metadata": {
        "id": "752d66ae"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch SMA/RSI via Alpha Vantage with a cached local-compute fallback\n",
        "# Context: used by feature pipelines that need daily indicators\n",
        "# Notes: caches by (symbol, indicator, time_period); normalizes dates and numeric types\n",
        "\n",
        "# src/data_io/indicators.py\n",
        "from __future__ import annotations\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.analysis.features import compute_sma, compute_rsi\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "KEYS = {\"SMA\": \"Technical Analysis: SMA\", \"RSI\": \"Technical Analysis: RSI\"}\n",
        "\n",
        "\n",
        "# If AV isn’t available (no key/limit), our code falls back to computing indicators locally from prices using our compute_sma / compute_rsi.\n",
        "def _fallback_from_prices(symbol: str, indicator: str, time_period: int) -> pd.DataFrame:\n",
        "    prices = fetch_prices(symbol, None, None)\n",
        "    if prices is None or prices.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if indicator == \"SMA\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"SMA\": compute_sma(prices, window=time_period)})\n",
        "    elif indicator == \"RSI\":\n",
        "        df = pd.DataFrame({\"date\": prices[\"date\"], \"RSI\": compute_rsi(prices, window=time_period)})\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna().sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def fetch_indicator(symbol: str, indicator: str, time_period: int = 14) -> pd.DataFrame:\n",
        "    key = KEYS.get(indicator)\n",
        "\n",
        "    # Try cache first\n",
        "    cache_key = f\"indicator_{symbol}_{indicator}_{time_period}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    if not SETTINGS.alpha_api_key or key is None:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    params = {\n",
        "        \"function\": indicator,\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": \"daily\",\n",
        "        \"time_period\": time_period,\n",
        "        \"series_type\": \"close\",\n",
        "        \"apikey\": SETTINGS.alpha_api_key,\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(BASE, params=params, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        # Alpha Vantage quota message handling:\n",
        "        if (not data or key not in data or not data[key] or \"Note\" in data or \"Information\" in data or \"Error Message\" in data):\n",
        "            df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "            save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "            return df\n",
        "    except Exception:\n",
        "        df = _fallback_from_prices(symbol, indicator, time_period)\n",
        "        save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "        return df\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "    df.reset_index(inplace=True)\n",
        "    df = df.rename(columns={\"index\": \"date\"})\n",
        "    for c in df.columns:\n",
        "        if c != \"date\":\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\", ascending=True).reset_index(drop=True)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iJxmkUXGsR3s",
      "metadata": {
        "id": "iJxmkUXGsR3s"
      },
      "source": [
        "# News Ingestion — Alpha Vantage Feed with Ticker/Relevance Filtering and Cache\n",
        "\n",
        "**Purpose**  \n",
        "Fetch symbol-specific headlines from Alpha Vantage’s News Sentiment API, filter to items that explicitly mention the target ticker with sufficient relevance, and cache the normalized rows to reduce redundant calls.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Called by downstream reporting/EDA steps to attach recent headlines and high-level sentiment to a ticker. Implemented as a single function for clarity and testability.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `symbol: str` (e.g., `\"AAPL\"`)  \n",
        "- **Outputs:** `pd.DataFrame` with columns: `published_at`, `source`, `title`, `summary`, `url`, `overall_sentiment`  \n",
        "- **Side Effects:**  \n",
        "  - Reads/writes JSON records under `SETTINGS.cache_dir` using `load_cache` / `save_cache`  \n",
        "  - Performs a network request to Alpha Vantage on cache miss\n",
        "\n",
        "**Behavior**  \n",
        "1. If no API key is configured, return an empty DataFrame (safe fail).  \n",
        "2. Check a per-symbol cache; return cached rows on hit.  \n",
        "3. On miss, call `NEWS_SENTIMENT` with the given ticker.  \n",
        "4. Keep only articles where the symbol appears in `ticker_sentiment` **and** `relevance_score ≥ 0.30`.  \n",
        "5. Normalize to a tidy DataFrame and cache as list-of-dict records.\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Missing `feed` key or malformed payload → return empty DataFrame.  \n",
        "- Network errors throw from `requests.get` by default; callers can handle exceptions upstream if desired.  \n",
        "- Inconsistent per-item fields are handled with `.get(...)` defaults; missing values propagate as `None/NaN`.\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Cached records (JSON) make runs deterministic for the TTL window configured in `SETTINGS`.  \n",
        "- Output schema is stable and designed for straightforward joins/plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fc9bb3f1",
      "metadata": {
        "id": "fc9bb3f1"
      },
      "outputs": [],
      "source": [
        "# Purpose: fetch and cache symbol-specific news via Alpha Vantage, filtered by relevance\n",
        "# Context: called by downstream reporting/EDA to attach headlines and sentiment\n",
        "# Notes: filters to items where ticker matches and relevance >= 0.30; caches by symbol\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, requests, pandas as pd\n",
        "from src.data_io.cache import load_cache, save_cache\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "\n",
        "def fetch_news(symbol: str) -> pd.DataFrame:\n",
        "    if not SETTINGS.alpha_api_key:\n",
        "        return pd.DataFrame()  # safe fail\n",
        "    cache_key = f\"news_{symbol}\"\n",
        "    cached = load_cache(cache_key, ttl_minutes=SETTINGS.cache_ttl_minutes)\n",
        "    if cached is not None:\n",
        "        return pd.DataFrame(cached)\n",
        "\n",
        "    params = {\"function\":\"NEWS_SENTIMENT\",\"tickers\":symbol,\"apikey\":SETTINGS.alpha_api_key}\n",
        "    r = requests.get(BASE, params=params, timeout=30)\n",
        "    data = r.json()\n",
        "    if \"feed\" not in data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for item in data.get(\"feed\", []):\n",
        "        tickers = item.get(\"ticker_sentiment\", []) or []\n",
        "        # keep only if our symbol is explicitly mentioned\n",
        "        keep = any(t.get(\"ticker\", \"\").upper() == symbol.upper() and float(t.get(\"relevance_score\", 0) or 0) >= 0.30\n",
        "                   for t in tickers)\n",
        "        if not keep:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"published_at\": item.get(\"time_published\"),\n",
        "            \"source\": item.get(\"source\"),\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"summary\": item.get(\"summary\"),\n",
        "            \"url\": item.get(\"url\"),\n",
        "            \"overall_sentiment\": item.get(\"overall_sentiment_label\")\n",
        "        })\n",
        "\n",
        "    # ====== Forth APPROACH =====\n",
        "    df = pd.DataFrame(rows)\n",
        "    save_cache(cache_key, df.to_dict(orient=\"records\"))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc9c201",
      "metadata": {
        "id": "6cc9c201"
      },
      "source": [
        "# Agent Selection — Minimal Heuristic for Pipeline Assembly\n",
        "\n",
        "**Purpose**  \n",
        "Choose which agents to run based on available evidence (news, prices, technical indicators). Keeps the pipeline lean by skipping agents that lack required inputs.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by the planner/driver prior to execution to assemble an ordered agent list. Implemented as a small, testable function.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:**  \n",
        "  - `has_news: bool` — preprocessed news available  \n",
        "  - `has_prices: bool` — OHLCV data available  \n",
        "  - `has_technicals: bool` — derived indicators (e.g., SMA/RSI) available  \n",
        "- **Outputs:** `list[str]` — ordered agent identifiers (e.g., `[\"news\",\"technical\",\"risk\"]`)  \n",
        "- **Side Effects:** None\n",
        "\n",
        "**Decision Logic**  \n",
        "- Include `\"news\"` if `has_news` is `True`.  \n",
        "- Include `\"technical\"` only when both prices **and** technicals are available.  \n",
        "- Always include `\"risk\"` (final guardrail/summary pass).\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Inputs are plain booleans; no runtime side effects.  \n",
        "- If upstream availability checks are wrong, the returned plan may include agents that later no-op (acceptable fallback).\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Deterministic list given the three boolean inputs; trivially unit-testable with 8 input combinations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f1200bf3",
      "metadata": {
        "id": "f1200bf3"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "def choose_agents(has_news: bool, has_prices: bool, has_technicals: bool) -> list[str]:\n",
        "    agents = []\n",
        "    if has_news: agents.append(\"news\")\n",
        "    # earnings optional if you add a financials fetch later\n",
        "    if has_technicals and has_prices: agents.append(\"technical\")\n",
        "    agents.append(\"risk\")\n",
        "    return agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa0a509",
      "metadata": {
        "id": "afa0a509"
      },
      "source": [
        "# Run Notes — Append-Only JSONL Memory for Experiments\n",
        "\n",
        "**Purpose**  \n",
        "Record lightweight, append-only metadata about each run (parameters, metrics, decisions) to support auditability, debugging, and iteration without a database.\n",
        "\n",
        "**Scope & Placement**  \n",
        "Used by agents/notebooks to persist per-run notes. Lives under `SETTINGS.runs_dir` as a line-delimited JSON file (`run_notes.jsonl`) for easy grep/diff/load.\n",
        "\n",
        "**Inputs / Outputs / Side Effects**  \n",
        "- **Inputs:** `record: dict[str, Any]` — JSON-serializable dictionary (e.g., `{\"ts\": ..., \"symbol\": ..., \"plan\": ..., \"metrics\": ...}`)  \n",
        "- **Outputs:** None (append-only)  \n",
        "- **Side Effects:** Creates `runs/` directory if missing; appends one JSON object per line to `run_notes.jsonl`\n",
        "\n",
        "**Failure Modes & Handling**  \n",
        "- Non-serializable objects → `json.dumps` will raise; callers should pass primitive/serializable types  \n",
        "- Concurrent writes in this simple form are best-effort; for multi-process safety use OS-level file locks in future work\n",
        "\n",
        "**Reproducibility & Reviewability**  \n",
        "- Plain-text JSONL enables quick reload via pandas/Polars and simple versioning  \n",
        "- Append-only pattern keeps historical context intact\n",
        "\n",
        "**Security & Data Handling**  \n",
        "- Do not store secrets/PII in run notes. Intended for operational metadata only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b260d5e1",
      "metadata": {
        "id": "b260d5e1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from src.config.settings import SETTINGS\n",
        "\n",
        "MEM_PATH = SETTINGS.runs_dir / \"run_notes.jsonl\"\n",
        "\n",
        "def append_memory(record: dict[str, Any]) -> None:\n",
        "    MEM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with MEM_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cba7d18",
      "metadata": {
        "id": "2cba7d18"
      },
      "source": [
        "## src/agents.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "93d94922",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY found: sk-pro***\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import os, json\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Import shared helpers from analysis.text\n",
        "from src.analysis.text import (\n",
        "    strip_code_fences,\n",
        "    to_float,\n",
        "    clamp,\n",
        "    normalize_score,\n",
        "    normalize_conf,\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# OpenAI client (safe stub for local/dev)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Use the standard env var name\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Optional: print a very short prefix to help you debug locally\n",
        "if api_key:\n",
        "    print(f\"OPENAI_API_KEY found: {api_key[:6]}***\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY NOT found! (running in MOCK mode)\")\n",
        "    # Don't set a fake key here; just run in mock.\n",
        "\n",
        "# Initialize client if possible; otherwise fall back to mock\n",
        "_client = None\n",
        "try:\n",
        "    # If you want to use the newer SDK:\n",
        "    # from openai import OpenAI\n",
        "    # _client = OpenAI()\n",
        "    #\n",
        "    # Or (legacy) openai.ChatCompletion API — but we'll stick to the new client interface:\n",
        "    from openai import OpenAI\n",
        "    if api_key:\n",
        "        _client = OpenAI()\n",
        "except Exception:\n",
        "    _client = None\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Shared response container\n",
        "# -----------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class AgentResponse:\n",
        "    agent_name: str\n",
        "    analysis: str\n",
        "    score: float\n",
        "    confidence: float\n",
        "    key_factors: List[str]\n",
        "    timestamp: str\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# BaseAgent\n",
        "# -----------------------------------------------------------------------------\n",
        "class BaseAgent:\n",
        "    def __init__(self, agent_name: str, model: str = \"gpt-4o\"):\n",
        "        self.agent_name = agent_name\n",
        "        self.model = model\n",
        "\n",
        "    def call_llm(self, system_prompt: str, user_message: str) -> str:\n",
        "        # Mock path (no API key / no client)\n",
        "        if _client is None:\n",
        "            return json.dumps({\n",
        "                \"analysis\": f\"MOCK: {self.agent_name} processed.\",\n",
        "                \"score\": 0.0,\n",
        "                \"key_factors\": [\"mock\"],\n",
        "                \"confidence\": 0.7\n",
        "            })\n",
        "        try:\n",
        "            resp = _client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=0.5,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "            return resp.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return json.dumps({\n",
        "                \"analysis\": f\"Error: {e}\",\n",
        "                \"score\": 0.0,\n",
        "                \"key_factors\": [\"error\"],\n",
        "                \"confidence\": 0.3\n",
        "            })\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# News\n",
        "# -----------------------------------------------------------------------------\n",
        "class NewsAnalysisAgent(BaseAgent):\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"News Analysis Agent\", model)\n",
        "        # IMPORTANT: keep everything inside one triple-quoted string\n",
        "        self.system_prompt = \"\"\"You are a senior financial analyst with 15+ years of experience in equity research.\n",
        "\n",
        "Analyze the provided news articles with focus on:\n",
        "1. SENTIMENT: Quantify market sentiment from -1 (very negative) to +1 (very positive)\n",
        "2. MATERIALITY: How much will this impact stock price? (high/medium/low)\n",
        "3. CATALYSTS: Identify specific events that could move the stock\n",
        "4. RISKS: Note any red flags or concerns mentioned\n",
        "\n",
        "SCORING GUIDELINES:\n",
        "+0.8 to +1.0: Major positive catalyst (earnings beat, breakthrough product, strategic win)\n",
        "+0.4 to +0.7: Positive news (growth signals, analyst upgrades, market share gains)\n",
        "-0.3 to +0.3: Neutral or mixed signals\n",
        "-0.7 to -0.4: Negative news (missed targets, regulatory issues, competitive threats)\n",
        "-1.0 to -0.8: Major negative catalyst (fraud, bankruptcy risk, losing key customers)\n",
        "\n",
        "IMPORTANT: \n",
        "- Use actual numbers from articles (revenue, EPS, growth rates)\n",
        "- Compare to analyst expectations when mentioned\n",
        "- Note if news is company-specific vs industry-wide\n",
        "- Higher confidence when multiple sources agree\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze news articles objectively\n",
        "2. Consider both positive and negative aspects\n",
        "3. Provide a sentiment score from -1 (very negative) to +1 (very positive)\n",
        "4. Identify key factors driving the sentiment\n",
        "5. Assess potential stock price impact\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"sentiment_score\": 0.75,\n",
        "  \"analysis\": \"Strong positive sentiment driven by earnings beat and product launch\",\n",
        "  \"key_factors\": [\"Earnings exceeded expectations\", \"New product well-received\"],\n",
        "  \"confidence\": 0.85\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: sentiment_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'AAPL')\n",
        "        news_articles = data.get('news', [])\n",
        "\n",
        "        news_summary = \"\\n\".join([\n",
        "            f\"- {a.get('title','')}: {a.get('description') or a.get('summary','')}\"\n",
        "            for a in news_articles[:5]\n",
        "        ])\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following recent news about {ticker}:\n",
        "\n",
        "{news_summary}\n",
        "\n",
        "Provide sentiment analysis and impact assessment. Return only the JSON.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get('sentiment_score', 0), 0.0))\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Earnings  (COMPLETED)\n",
        "# ------------------------------------------------------------------------------\n",
        "class EarningsAnalysisAgent(BaseAgent):\n",
        "    \"\"\"Analyzes earnings reports and patterns (EPS actual vs estimate, surprise history).\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Earnings Analysis Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a financial analyst specializing in earnings and fundamental analysis.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze the earnings series objectively (EPS actual vs. estimates, surprises).\n",
        "2. Identify recent beats/misses, average surprise, and beat ratio.\n",
        "3. Provide a fundamental strength score from -1 (very weak) to +1 (very strong).\n",
        "4. List concise key factors that justify the score.\n",
        "5. Be specific with numbers when available.\n",
        "\n",
        "EXPECTED JSON SCHEMA:\n",
        "{\n",
        "  \"fundamental_score\": float,   // -1..+1\n",
        "  \"analysis\": string,\n",
        "  \"key_factors\": [string],\n",
        "  \"confidence\": float           // 0..1\n",
        "}\n",
        "\n",
        "SCORING HINTS:\n",
        "- Strong positive if repeated beats, positive average surprise, improving trend.\n",
        "- Negative if repeated misses, negative average surprise, deteriorating margins (if provided).\n",
        "- Neutral if mixed or sparse data.\n",
        "\n",
        "Return ONLY valid JSON with keys: fundamental_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get(\"ticker\", \"UNKNOWN\")\n",
        "        rows = data.get(\"earnings\", []) or []\n",
        "\n",
        "        # Compact tabular summary to feed the model (top 8 most recent already supplied upstream)\n",
        "        def row_line(r: Dict[str, Any]) -> str:\n",
        "            return (\n",
        "                f\"- {r.get('date','?')}: estimate={r.get('EPS Estimate','n/a')}, \"\n",
        "                f\"reported={r.get('Reported EPS','n/a')}, surprise%={r.get('Surprise(%)','n/a')}\"\n",
        "            )\n",
        "        table = \"\\n\".join(row_line(r) for r in rows[:12])\n",
        "\n",
        "        user_message = f\"\"\"Company: {ticker}\n",
        "\n",
        "Recent quarterly earnings (most recent first):\n",
        "{table}\n",
        "\n",
        "Analyze this history and return only the JSON object described in the schema.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get(\"fundamental_score\", 0.0), 0.0))\n",
        "            analysis = result.get(\"analysis\", raw)\n",
        "            key_factors = result.get(\"key_factors\", [])\n",
        "            confidence = normalize_conf(result.get(\"confidence\", 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Technicals\n",
        "# -----------------------------------------------------------------------------\n",
        "class MarketSignalsAgent(BaseAgent):\n",
        "    \"\"\"Performs technical analysis on market data\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Market Signals Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a technical analyst specializing in market signals and price patterns.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze technical indicators objectively\n",
        "2. Assess technical strength from -1 (very bearish) to +1 (very bullish)\n",
        "3. Identify support/resistance levels\n",
        "4. Evaluate trend direction and momentum\n",
        "5. Consider volume patterns\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"technical_score\": 0.65,\n",
        "  \"analysis\": \"Bullish technical setup with price above key moving averages\",\n",
        "  \"key_factors\": [\"Price above 50-day MA\", \"RSI indicates strength\", \"Volume confirming uptrend\"],\n",
        "  \"confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: technical_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'UNKNOWN')\n",
        "        technicals = data.get('technicals', {})\n",
        "\n",
        "        technical_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Current Price: ${technicals.get('current_price', 'N/A')}\n",
        "50-day MA: ${technicals.get('ma_50', 'N/A')}\n",
        "200-day MA: ${technicals.get('ma_200', 'N/A')}\n",
        "RSI: {technicals.get('rsi', 'N/A')}\n",
        "MACD: {technicals.get('macd', 'N/A')}\n",
        "Volume: {technicals.get('volume', 'N/A')} (Avg: {technicals.get('avg_volume', 'N/A')})\n",
        "Support: ${technicals.get('support', 'N/A')}\n",
        "Resistance: ${technicals.get('resistance', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following technical data for {ticker}:\n",
        "\n",
        "{technical_summary}\n",
        "\n",
        "Assess technical strength and price momentum. Return only the JSON described above.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            score = normalize_score(to_float(result.get('technical_score', 0), 0.0))\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Risk  (COMPLETED)\n",
        "# -----------------------------------------------------------------------------\n",
        "class RiskAssessmentAgent(BaseAgent):\n",
        "    \"\"\"Assesses investment risk and portfolio fit\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Risk Assessment Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a risk management analyst specializing in portfolio risk assessment.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze risk metrics objectively.\n",
        "2. Provide a risk level score from 0 (very low risk) to 1 (very high risk).\n",
        "3. Identify key risk drivers (beta, volatility, VaR, Sharpe, max drawdown, concentration/correlation).\n",
        "4. Explain portfolio implications and any risk mitigants.\n",
        "\n",
        "EXPECTED JSON SCHEMA:\n",
        "{\n",
        "  \"risk_score\": float,      // 0..1\n",
        "  \"analysis\": string,\n",
        "  \"key_factors\": [string],\n",
        "  \"confidence\": float       // 0..1\n",
        "}\n",
        "\n",
        "GUIDANCE:\n",
        "- Higher beta/volatility/drawdown/VaR => higher risk_score.\n",
        "- Higher Sharpe => lowers effective risk_score (risk-adjusted).\n",
        "- Lack of data => moderate confidence; be explicit.\n",
        "\n",
        "Return ONLY valid JSON with keys: risk_score, analysis, key_factors, confidence\"\"\"\n",
        "\n",
        "    def process(self, data: Dict[str, Any]) -> AgentResponse:\n",
        "        ticker = data.get('ticker', 'UNKNOWN')\n",
        "        risk_data = data.get('risk_metrics', {}) or {}\n",
        "\n",
        "        # Build a compact, explicit summary. We pass both short-term and full stats if provided.\n",
        "        risk_summary = f\"\"\"\n",
        "Ticker: {ticker}\n",
        "Beta: {risk_data.get('beta', 'N/A')}\n",
        "Volatility (30-day): {risk_data.get('volatility', 'N/A')}%\n",
        "Sharpe Ratio: {risk_data.get('sharpe_ratio', 'N/A')}\n",
        "Max Drawdown (%): {risk_data.get('max_drawdown', 'N/A')}\n",
        "Value at Risk (5% daily return): {risk_data.get('var_5', 'N/A')}\n",
        "Sector Correlation: {risk_data.get('sector_correlation', 'N/A')}\n",
        "P/E Ratio: {risk_data.get('pe_ratio', 'N/A')}\n",
        "\n",
        "# Extended (may be None):\n",
        "Avg Daily Return: {risk_data.get('avg_daily_return', 'N/A')}\n",
        "Volatility (full window): {risk_data.get('volatility_full', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Analyze the following risk metrics and return only the JSON per schema:\n",
        "\n",
        "{risk_summary}\n",
        "\n",
        "Give a 0..1 risk_score, analysis, key_factors (bullet-style phrases), and confidence.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "\n",
        "            # Keep 0..1 semantics but normalize/clamp\n",
        "            risk01 = to_float(result.get('risk_score', 0.5), 0.5)\n",
        "            if 1.0 < risk01 <= 100.0:\n",
        "                risk01 = risk01 / 100.0\n",
        "            elif 1.0 < risk01 <= 10.0:\n",
        "                risk01 = risk01 / 10.0\n",
        "            risk01 = clamp(risk01, 0.0, 1.0)\n",
        "\n",
        "            score = risk01\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_factors', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.8))\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.5\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Synthesis\n",
        "# -----------------------------------------------------------------------------\n",
        "class SynthesisAgent(BaseAgent):\n",
        "    \"\"\"Combines insights from all agents into final recommendation\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(\"Research Synthesis Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a senior investment analyst who synthesizes multiple analyses into actionable recommendations.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review all agent analyses objectively\n",
        "2. Weigh different factors appropriately\n",
        "3. Provide clear investment recommendation (STRONG BUY, BUY, HOLD, SELL, STRONG SELL)\n",
        "4. State confidence level (0 to 1)\n",
        "5. Summarize key reasoning\n",
        "6. Note important risks\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"recommendation\": \"BUY\",\n",
        "  \"confidence\": 0.78,\n",
        "  \"analysis\": \"Strong fundamentals and positive technical signals support a buy recommendation despite moderate risk\",\n",
        "  \"key_points\": [\"Earnings beat expectations\", \"Technical breakout\", \"Acceptable risk profile\"],\n",
        "  \"risks\": [\"Market volatility\", \"Sector headwinds\"]\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: recommendation, confidence, analysis, key_points, risks\"\"\"\n",
        "\n",
        "    def process(self, agent_responses: List[AgentResponse]) -> AgentResponse:\n",
        "        analyses_summary = \"\\n\\n\".join([\n",
        "            f\"{resp.agent_name}:\\n\"\n",
        "            f\"Score: {resp.score}\\n\"\n",
        "            f\"Analysis: {resp.analysis}\\n\"\n",
        "            f\"Key Factors: {', '.join(resp.key_factors)}\"\n",
        "            for resp in agent_responses\n",
        "        ])\n",
        "\n",
        "        user_message = f\"\"\"Synthesize the following analyses into a final investment recommendation:\n",
        "\n",
        "{analyses_summary}\n",
        "\n",
        "Provide a comprehensive investment recommendation with supporting reasoning. Return only the JSON.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            recommendation = str(result.get('recommendation', 'HOLD')).upper()\n",
        "            analysis = result.get('analysis', raw)\n",
        "            key_factors = result.get('key_points', [])\n",
        "            confidence = normalize_conf(result.get('confidence', 0.7))\n",
        "\n",
        "            rec_to_score = {\n",
        "                'STRONG BUY': 1.0,\n",
        "                'BUY': 0.6,\n",
        "                'HOLD': 0.0,\n",
        "                'SELL': -0.6,\n",
        "                'STRONG SELL': -1.0\n",
        "            }\n",
        "            score = rec_to_score.get(recommendation, 0.0)\n",
        "        except json.JSONDecodeError:\n",
        "            score = 0.0\n",
        "            analysis = raw\n",
        "            key_factors = [\"Unable to parse structured response\"]\n",
        "            confidence = 0.6\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(score),\n",
        "            confidence=float(confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Critique\n",
        "# -----------------------------------------------------------------------------\n",
        "class CritiqueAgent(BaseAgent):\n",
        "    \"\"\"Reviews and validates analysis quality\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(\"Critique & Validation Agent\", model)\n",
        "        self.system_prompt = \"\"\"You are a critique analyst who reviews investment recommendations for biases, logical errors, and completeness.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the synthesis objectively\n",
        "2. Identify logical inconsistencies\n",
        "3. Detect potential biases\n",
        "4. Note missing considerations\n",
        "5. Assess data quality\n",
        "6. Recommend confidence adjustments\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "{\n",
        "  \"quality_score\": 0.82,\n",
        "  \"issues_found\": [\"Limited macroeconomic analysis\"],\n",
        "  \"suggestions\": [\"Consider Federal Reserve policy impact\", \"Add sector comparison\"],\n",
        "  \"adjusted_confidence\": 0.75\n",
        "}\n",
        "\n",
        "Return ONLY valid JSON with keys: quality_score, issues_found, suggestions, adjusted_confidence\"\"\"\n",
        "\n",
        "    def process(self, synthesis_response: AgentResponse) -> AgentResponse:\n",
        "        user_message = f\"\"\"Review this investment analysis for quality and completeness:\n",
        "\n",
        "Recommendation: {synthesis_response.analysis}\n",
        "Confidence: {synthesis_response.confidence}\n",
        "Key Factors: {', '.join(synthesis_response.key_factors)}\n",
        "\n",
        "Identify any issues, biases, or missing elements. Return only the JSON.\"\"\"\n",
        "        raw = self.call_llm(self.system_prompt, user_message)\n",
        "        js = strip_code_fences(raw)\n",
        "\n",
        "        try:\n",
        "            result = json.loads(js)\n",
        "            quality_score = to_float(result.get('quality_score', 0.7), 0.7)\n",
        "            # normalize 0..10 or 0..100 to 0..1 (display-style)\n",
        "            if 1.0 < quality_score <= 10.0:\n",
        "                quality_score = quality_score / 10.0\n",
        "            elif 10.0 < quality_score <= 100.0:\n",
        "                quality_score = quality_score / 100.0\n",
        "            quality_score = clamp(quality_score, 0.0, 1.0)\n",
        "\n",
        "            issues = result.get('issues_found', [])\n",
        "            suggestions = result.get('suggestions', [])\n",
        "            adjusted_confidence = normalize_conf(\n",
        "                result.get('adjusted_confidence', synthesis_response.confidence)\n",
        "            )\n",
        "\n",
        "            analysis = f\"Quality Score: {quality_score}\\n\"\n",
        "            if issues:\n",
        "                analysis += f\"Issues Found: {', '.join(issues)}\\n\"\n",
        "            if suggestions:\n",
        "                analysis += f\"Suggestions: {', '.join(suggestions)}\"\n",
        "\n",
        "            key_factors = issues if issues else [\"No major issues found\"]\n",
        "        except json.JSONDecodeError:\n",
        "            quality_score = 0.7\n",
        "            analysis = raw\n",
        "            adjusted_confidence = synthesis_response.confidence\n",
        "            key_factors = [\"No major issues found\"]\n",
        "\n",
        "        return AgentResponse(\n",
        "            agent_name=self.agent_name,\n",
        "            analysis=analysis,\n",
        "            score=float(quality_score),\n",
        "            confidence=float(adjusted_confidence),\n",
        "            key_factors=key_factors,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14963947",
      "metadata": {
        "id": "14963947"
      },
      "source": [
        "## src/system/orchestration.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4a539fc5",
      "metadata": {
        "id": "4a539fc5"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "from datetime import datetime, timezone\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "from src.config.settings import SETTINGS\n",
        "from src.data_io.prices import fetch_prices\n",
        "from src.data_io.news import fetch_news\n",
        "from src.data_io.indicators import fetch_indicator\n",
        "from src.data_io.earnings import fetch_earnings\n",
        "from src.data_io.risk import fetch_risk_metrics\n",
        "from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "from src.system.router import choose_agents\n",
        "from src.system.memory import append_memory\n",
        "from src.agents import (\n",
        "    NewsAnalysisAgent,\n",
        "    MarketSignalsAgent,\n",
        "    RiskAssessmentAgent,\n",
        "    SynthesisAgent,\n",
        "    CritiqueAgent,\n",
        "    AgentResponse,\n",
        "    EarningsAnalysisAgent,\n",
        ")\n",
        "\n",
        "# ------------- helpers -------------\n",
        "def _as_text(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _as_list_of_text(x):\n",
        "    if isinstance(x, list):\n",
        "        return [_as_text(i) for i in x]\n",
        "    if x is None:\n",
        "        return []\n",
        "    return [_as_text(x)]\n",
        "\n",
        "def now_utc_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "# configurable stagger between provider calls (defaults to 0.5s if unset)\n",
        "_NET_STAGGER = float(getattr(SETTINGS, \"net_stagger_secs\", 0.5))\n",
        "\n",
        "@dataclass\n",
        "class OrchestratorResult:\n",
        "    plan: List[str]\n",
        "    evidence: Dict[str, DataFrame]\n",
        "    agent_outputs: List[AgentResponse]\n",
        "    final: AgentResponse\n",
        "    critique: AgentResponse\n",
        "\n",
        "\n",
        "def run_pipeline(symbol: str, start: str | None, end: str | None,\n",
        "                 required_tags: list[str] | None = None) -> OrchestratorResult:\n",
        "    plan = [\n",
        "        \"fetch_prices\", \"fetch_news\", \"fetch_earnings\", \"fetch_risk\",\n",
        "        \"preprocess\", \"classify_extract\", \"retrieve_topk\",\n",
        "        \"route\", \"run_agents\", \"synthesize\", \"critique\", \"save_memory\"\n",
        "    ]\n",
        "\n",
        "    # ---------------- 1) FETCH (staggered) ----------------\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    time.sleep(_NET_STAGGER)\n",
        "\n",
        "    news = fetch_news(symbol)\n",
        "    time.sleep(_NET_STAGGER)\n",
        "\n",
        "    earn_df = fetch_earnings(symbol)\n",
        "    time.sleep(_NET_STAGGER)\n",
        "\n",
        "    risk_ingested = fetch_risk_metrics(symbol, start, end)  # dict\n",
        "    time.sleep(_NET_STAGGER)\n",
        "\n",
        "    # ---------------- 2) PREPROCESS NEWS ----------------\n",
        "    news_pp = add_tags_and_numbers(preprocess_news(news))\n",
        "\n",
        "    # ---------------- 3) RETRIEVAL ----------------\n",
        "    top_news = recent_topk(\n",
        "        news_pp,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags\n",
        "    )\n",
        "\n",
        "    # ---------------- 4) ROUTE PRIMERS ----------------\n",
        "    has_news     = not top_news.empty\n",
        "    has_prices   = not prices.empty\n",
        "    has_earnings = (earn_df is not None) and (not earn_df.empty)\n",
        "\n",
        "    # # Only attempt technicals if we have prices (or we have AV key for indicators)\n",
        "    attempt_technicals = has_prices or bool(SETTINGS.alpha_api_key)\n",
        "\n",
        "    # ---------------- 5) INDICATORS (conditional) ----------------\n",
        "    rsi = sma20 = sma50 = sma200 = pd.DataFrame()\n",
        "    if attempt_technicals:\n",
        "        rsi    = fetch_indicator(symbol, \"RSI\", 14);   time.sleep(_NET_STAGGER)\n",
        "        sma20  = fetch_indicator(symbol, \"SMA\", 20);   time.sleep(_NET_STAGGER)\n",
        "        sma50  = fetch_indicator(symbol, \"SMA\", 50);   time.sleep(_NET_STAGGER)\n",
        "        sma200 = fetch_indicator(symbol, \"SMA\", 200);  time.sleep(_NET_STAGGER)\n",
        "\n",
        "    has_technicals = (not rsi.empty) or (not sma20.empty) or (not sma50.empty) or (not sma200.empty)\n",
        "\n",
        "    \n",
        "    # # === QUICK DISABLE: skip fetching indicators entirely ===\n",
        "    # attempt_technicals = False  # <— set to False to skip indicator calls\n",
        "\n",
        "    # # ---------------- 5) INDICATORS (skipped) ----------------\n",
        "    # rsi = sma20 = sma50 = sma200 = pd.DataFrame()  # keep variables defined/empty\n",
        "\n",
        "    # has_technicals = False  # no technical lane when indicators are disabled\n",
        "\n",
        "    # Decide lanes now that we know what we actually have\n",
        "    lanes = choose_agents(has_news, has_prices, has_technicals, has_earnings)\n",
        "\n",
        "    # ---------------- 6) RUN AGENTS ----------------\n",
        "    outputs: List[AgentResponse] = []\n",
        "\n",
        "    # NEWS\n",
        "    if \"news\" in lanes and has_news:\n",
        "        news_payload_records = (\n",
        "            top_news\n",
        "            .rename(columns={\"summary\": \"description\"})\n",
        "            .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "            .to_dict(orient=\"records\")\n",
        "        )\n",
        "        outputs.append(NewsAnalysisAgent().process({\"ticker\": symbol, \"news\": news_payload_records}))\n",
        "\n",
        "    # TECHNICALS\n",
        "    if \"technical\" in lanes and (has_technicals or has_prices):\n",
        "        current_price = float(prices[\"close\"].iloc[-1]) if has_prices else None\n",
        "        volume = int(prices[\"volume\"].iloc[-1]) if has_prices else None\n",
        "        avg_volume = int(prices[\"volume\"].tail(20).mean()) if has_prices else None\n",
        "\n",
        "        technicals = {\n",
        "            \"current_price\": current_price,\n",
        "            \"rsi\": (float(rsi[\"RSI\"].iloc[-1]) if not rsi.empty else None),\n",
        "            \"ma_50\": (float(sma50[\"SMA\"].iloc[-1]) if not sma50.empty else (float(sma20[\"SMA\"].iloc[-1]) if not sma20.empty else None)),\n",
        "            \"ma_200\": (float(sma200[\"SMA\"].iloc[-1]) if not sma200.empty else None),\n",
        "            \"macd\": None,\n",
        "            \"volume\": volume,\n",
        "            \"avg_volume\": avg_volume,\n",
        "            \"support\": None,\n",
        "            \"resistance\": None,\n",
        "        }\n",
        "        outputs.append(MarketSignalsAgent().process({\"ticker\": symbol, \"technicals\": technicals}))\n",
        "\n",
        "    # EARNINGS\n",
        "    if \"earnings\" in lanes and has_earnings:\n",
        "        earn_payload = {\n",
        "            \"ticker\": symbol,\n",
        "            \"earnings\": (\n",
        "                earn_df.sort_values(\"date\", ascending=False)\n",
        "                       .head(8)\n",
        "                       .to_dict(orient=\"records\")\n",
        "            )\n",
        "        }\n",
        "        outputs.append(EarningsAnalysisAgent().process(earn_payload))\n",
        "\n",
        "    # RISK (merge ingestion + short-term realized vol for UI)\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if has_prices else None\n",
        "    risk_payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\":              risk_ingested.get(\"beta\"),\n",
        "            \"volatility\":        vol_30d,                          # short-term display (%)\n",
        "            \"var_5\":             risk_ingested.get(\"var_5\"),\n",
        "            \"sharpe_ratio\":      risk_ingested.get(\"sharpe_ratio\"),\n",
        "            \"max_drawdown\":      risk_ingested.get(\"max_drawdown\"),\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\":          None,\n",
        "            # Extended (optional)\n",
        "            \"avg_daily_return\":  risk_ingested.get(\"avg_daily_return\"),\n",
        "            \"volatility_full\":   risk_ingested.get(\"volatility\"),\n",
        "        }\n",
        "    }\n",
        "    outputs.append(RiskAssessmentAgent().process(risk_payload))\n",
        "\n",
        "    # ---------------- 7) SYNTHESIZE + CRITIQUE ----------------\n",
        "    synth_v1 = SynthesisAgent().process(outputs)\n",
        "    crit     = CritiqueAgent().process(synth_v1)\n",
        "\n",
        "    needs_rerun = (crit.score < 0.90) or (\"data quality\" in \" \".join(_as_list_of_text(crit.key_factors)).lower())\n",
        "    synth_final = synth_v1\n",
        "    if needs_rerun:\n",
        "        critique_feedback = AgentResponse(\n",
        "            agent_name=\"Critique Feedback\",\n",
        "            analysis=_as_text(synth_v1.analysis) + \"\\n\\n[CRITIQUE]\\n\" + _as_text(crit.analysis),\n",
        "            score=crit.score,\n",
        "            confidence=crit.confidence,\n",
        "            key_factors=_as_list_of_text(crit.key_factors),\n",
        "            timestamp=now_utc_iso()\n",
        "        )\n",
        "        synth_final = SynthesisAgent().process(outputs + [critique_feedback])\n",
        "\n",
        "    # ---------------- 8) MEMORY ----------------\n",
        "    append_memory({\n",
        "        \"ticker\": symbol,\n",
        "        \"lanes\": lanes,\n",
        "        \"issues\": crit.key_factors,\n",
        "        \"final_confidence_v1\": synth_v1.confidence,\n",
        "        \"final_confidence_v2\": synth_final.confidence if needs_rerun else None,\n",
        "        \"optimizer_triggered\": bool(needs_rerun),\n",
        "        \"timestamp\": now_utc_iso()\n",
        "    })\n",
        "\n",
        "    # ---------------- 9) EVIDENCE FOR UI ----------------\n",
        "    earn_evidence = (\n",
        "        earn_df.sort_values(\"date\", ascending=False).head(8)\n",
        "        if has_earnings else pd.DataFrame()\n",
        "    )\n",
        "    risk_evidence = pd.DataFrame([risk_payload[\"risk_metrics\"]])\n",
        "\n",
        "    evidence = {\n",
        "        \"top_news\": top_news,\n",
        "        \"prices_tail\": prices.tail(5),\n",
        "        \"earnings_head\": earn_evidence,\n",
        "        \"risk_metrics\": risk_evidence,\n",
        "    }\n",
        "\n",
        "    # Add Initial Synthesis to outputs for transparency in UI\n",
        "    outputs.append(AgentResponse(\n",
        "        agent_name=\"Initial Synthesis\",\n",
        "        analysis=_as_text(synth_v1.analysis),\n",
        "        score=float(synth_v1.score),\n",
        "        confidence=float(synth_v1.confidence),\n",
        "        key_factors=_as_list_of_text(synth_v1.key_factors),\n",
        "        timestamp=synth_v1.timestamp\n",
        "    ))\n",
        "\n",
        "    return OrchestratorResult(plan, evidence, outputs, synth_final, crit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ecb8452",
      "metadata": {},
      "source": [
        "# Orchestration Demo in notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6f07aca2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "****************************************\n",
            "  DEMONSTRATING 3 AGENTIC WORKFLOW PATTERNS\n",
            "****************************************\n",
            "\n",
            "Ticker: AAPL\n",
            "Date Range: 2025-09-20 → 2025-10-20\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 1: PROMPT CHAINING\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-20 → 2025-10-20\n",
            "================================================================================\n",
            "\n",
            "┌─ STEP 1/5: INGEST ─────────────────────────────────────────────┐\n",
            "│ Fetching news (Alpha Vantage NEWS_SENTIMENT)                   │\n",
            "└─────────────────────────────────────────────────────────────────┘\n",
            "  fetched_articles:  18\n",
            "\n",
            "┌─ STEP 2/5: PREPROCESS ─────────────────────────────────────────┐\n",
            "  after_preprocess:  18\n",
            "\n",
            "┌─ STEP 3/5: CLASSIFY ───────────────────────────────────────────┐\n",
            "  after_tagging:     18\n",
            "\n",
            "┌─ STEP 4/5: EXTRACT ────────────────────────────────────────────┐\n",
            "  top_articles:      5\n",
            "\n",
            "┌─ STEP 5/5: SUMMARIZE ──────────────────────────────────────────┐\n",
            "  sentiment_score:   +0.70\n",
            "  confidence:        85%\n",
            "\n",
            "================================================================================\n",
            "PROMPT CHAINING COMPLETE\n",
            "Pattern: Raw → Clean → Tagged → Top-K → Analysis\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 2: PARALLEL EXECUTION\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-20 → 2025-10-20\n",
            "================================================================================\n",
            "\n",
            "[Preparation] Fetching base data…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to get ticker 'AAPL' reason: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "1 Failed download:\n",
            "['AAPL']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
            "yfinance returned empty for AAPL (attempt 1)\n",
            "Failed to get ticker 'AAPL' reason: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "1 Failed download:\n",
            "['AAPL']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
            "yfinance returned empty for AAPL (attempt 2)\n",
            "Failed to get ticker 'AAPL' reason: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "1 Failed download:\n",
            "['AAPL']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
            "yfinance returned empty for AAPL (attempt 3)\n",
            "Failed to get ticker 'AAPL' reason: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "1 Failed download:\n",
            "['AAPL']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
            "yfinance returned empty for AAPL (attempt 4)\n",
            "Failed to get ticker 'AAPL' reason: Expecting value: line 1 column 1 (char 0)\n",
            "$AAPL: possibly delisted; no timezone found\n",
            "yfinance failed for AAPL: yfinance returned empty data repeatedly; attempting Alpha Vantage fallback.\n",
            "Alpha Vantage fallback missing time series for AAPL. Keys: ['Information']\n",
            "AAPL: $AAPL: possibly delisted; no earnings dates found\n",
            "Failed to get ticker '^GSPC' reason: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "1 Failed download:\n",
            "['^GSPC']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Parallel] Running News + Technical + Risk + Earnings (4 agents)…\n",
            "  News       Score=+0.70  Conf=80%\n",
            "  Technical  Score=+0.00  Conf=20%\n",
            "  Risk       Score=+0.68  Conf=60%\n",
            "  Earnings   Score=+0.70  Conf=90%\n",
            "\n",
            "================================================================================\n",
            "PARALLEL EXECUTION COMPLETE (4.08s)\n",
            "Pattern: Agents run concurrently to shorten wall time.\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER\n",
            "================================================================================\n",
            "Analyzing: AAPL | Period: 2025-09-20 → 2025-10-20\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] GENERATE: Running pipeline…\n",
            "  initial_score:     +0.00\n",
            "  initial_conf:      68%\n",
            "\n",
            "[Phase 2] EVALUATE: Critique\n",
            "  quality_score:     0.65\n",
            "  adj_confidence:    60%\n",
            "  issues_found:      3\n",
            "\n",
            "[Phase 3] OPTIMIZE: Re-synthesized with critique feedback\n",
            "  final_score:       +0.00\n",
            "  final_conf:        68%\n",
            "  conf_change:       +0%\n",
            "\n",
            "================================================================================\n",
            "EVALUATOR-OPTIMIZER COMPLETE\n",
            "================================================================================\n",
            "\n",
            "\n",
            "########################################\n",
            "  ALL 3 WORKFLOW PATTERNS DEMONSTRATED\n",
            "########################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # src/system/workflows_demo.py\n",
        "# from __future__ import annotations\n",
        "\n",
        "# from dataclasses import dataclass\n",
        "# from typing import List, Dict\n",
        "# from datetime import datetime, timedelta\n",
        "# import time\n",
        "# import json\n",
        "# import pandas as pd\n",
        "\n",
        "# from src.system.orchestrator import run_pipeline, OrchestratorResult\n",
        "# from src.config.settings import SETTINGS\n",
        "# from src.data_io.prices import fetch_prices\n",
        "# from src.data_io.news import fetch_news\n",
        "# from src.data_io.earnings import fetch_earnings\n",
        "# from src.data_io.risk import fetch_risk_metrics\n",
        "# from src.analysis.text import preprocess_news, add_tags_and_numbers, recent_topk\n",
        "# from src.agents import (\n",
        "#     NewsAnalysisAgent,\n",
        "#     MarketSignalsAgent,\n",
        "#     RiskAssessmentAgent,\n",
        "#     EarningsAnalysisAgent,\n",
        "#     AgentResponse,\n",
        "# )\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Small helpers\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _as_text(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(x, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def _print_kv(k: str, v) -> None:\n",
        "    print(f\"  {k:<18} {v}\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# WORKFLOW PATTERN 1: PROMPT CHAINING (News-focused)\n",
        "# Raw News → Clean → Tag → Top-K → LLM Summary\n",
        "# Skips entirely if SETTINGS.skip_news=True\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_prompt_chaining_workflow(\n",
        "    symbol: str,\n",
        "    start: str,\n",
        "    end: str,\n",
        "    required_tags: list[str] | None = None\n",
        ") -> AgentResponse:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"WORKFLOW PATTERN 1: PROMPT CHAINING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} → {end}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if getattr(SETTINGS, \"skip_news\", False):\n",
        "        print(\"\\n[Notice] News fetching is disabled by SETTINGS.skip_news=True.\")\n",
        "        return AgentResponse(\n",
        "            agent_name=\"News Analysis Agent\",\n",
        "            analysis=\"News workflow skipped by configuration.\",\n",
        "            score=0.0,\n",
        "            confidence=0.95,\n",
        "            key_factors=[\"skip_news=True\"],\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    # 1) Ingest\n",
        "    print(\"\\n┌─ STEP 1/5: INGEST ─────────────────────────────────────────────┐\")\n",
        "    print(\"│ Fetching news (Alpha Vantage NEWS_SENTIMENT)                   │\")\n",
        "    print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
        "    raw_news = fetch_news(symbol)\n",
        "    _print_kv(\"fetched_articles:\", 0 if raw_news is None else len(raw_news))\n",
        "    if raw_news is None or raw_news.empty:\n",
        "        print(\"  No news data available.\")\n",
        "        return AgentResponse(\n",
        "            agent_name=\"News Analysis Agent\",\n",
        "            analysis=\"No news returned from provider.\",\n",
        "            score=0.0,\n",
        "            confidence=0.4,\n",
        "            key_factors=[\"no_news_data\"],\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    # 2) Preprocess\n",
        "    print(\"\\n┌─ STEP 2/5: PREPROCESS ─────────────────────────────────────────┐\")\n",
        "    clean = preprocess_news(raw_news)\n",
        "    _print_kv(\"after_preprocess:\", len(clean))\n",
        "\n",
        "    # 3) Classify (tags + number extraction)\n",
        "    print(\"\\n┌─ STEP 3/5: CLASSIFY ───────────────────────────────────────────┐\")\n",
        "    tagged = add_tags_and_numbers(clean)\n",
        "    _print_kv(\"after_tagging:\", len(tagged))\n",
        "\n",
        "    # 4) Extract (recent top-K, optional tag filter)\n",
        "    print(\"\\n┌─ STEP 4/5: EXTRACT ────────────────────────────────────────────┐\")\n",
        "    topk = recent_topk(\n",
        "        tagged,\n",
        "        topk=SETTINGS.topk_news,\n",
        "        days=SETTINGS.news_window_days,\n",
        "        required_tags=required_tags\n",
        "    )\n",
        "    _print_kv(\"top_articles:\", len(topk))\n",
        "\n",
        "    # 5) Summarize (LLM agent)\n",
        "    print(\"\\n┌─ STEP 5/5: SUMMARIZE ──────────────────────────────────────────┐\")\n",
        "    payload = {\n",
        "        \"ticker\": symbol,\n",
        "        \"news\": (\n",
        "            topk.rename(columns={\"summary\": \"description\"})\n",
        "                .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "                .to_dict(\"records\")\n",
        "            if not topk.empty else []\n",
        "        ),\n",
        "    }\n",
        "    res = NewsAnalysisAgent().process(payload)\n",
        "    _print_kv(\"sentiment_score:\", f\"{res.score:+.2f}\")\n",
        "    _print_kv(\"confidence:\", f\"{res.confidence:.0%}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PROMPT CHAINING COMPLETE\")\n",
        "    print(\"Pattern: Raw → Clean → Tagged → Top-K → Analysis\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    return res\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# WORKFLOW PATTERN 2: PARALLEL EXECUTION\n",
        "# Run News + Technical (price-only) + Risk + Earnings concurrently.\n",
        "# Indicators are **not** fetched here to avoid API usage; technicals use price/volume only.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_parallel_workflow(symbol: str, start: str, end: str) -> List[AgentResponse]:\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"WORKFLOW PATTERN 2: PARALLEL EXECUTION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} → {end}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n[Preparation] Fetching base data…\")\n",
        "    prices = fetch_prices(symbol, start, end)\n",
        "    news = pd.DataFrame()\n",
        "    if not getattr(SETTINGS, \"skip_news\", False):\n",
        "        news = fetch_news(symbol)\n",
        "    earnings = fetch_earnings(symbol)\n",
        "    risk_ingested = fetch_risk_metrics(symbol, start, end)\n",
        "\n",
        "    # Inputs\n",
        "    news_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"news\": (\n",
        "            news.head(5).rename(columns={\"summary\": \"description\"})\n",
        "                .loc[:, [\"title\", \"description\", \"source\", \"url\", \"published_at\"]]\n",
        "                .to_dict(\"records\")\n",
        "            if not news.empty else []\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # Technicals (no indicators fetched here; keep it lightweight)\n",
        "    tech_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"technicals\": {\n",
        "            \"current_price\": float(prices[\"close\"].iloc[-1]) if not prices.empty else None,\n",
        "            \"volume\": int(prices[\"volume\"].iloc[-1]) if not prices.empty else None,\n",
        "            \"avg_volume\": int(prices[\"volume\"].tail(20).mean()) if not prices.empty else None,\n",
        "            \"rsi\": None, \"ma_50\": None, \"ma_200\": None,\n",
        "            \"macd\": None, \"support\": None, \"resistance\": None,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    vol_30d = float(prices[\"close\"].pct_change().tail(30).std() * 100) if not prices.empty else None\n",
        "    risk_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"risk_metrics\": {\n",
        "            \"beta\":              risk_ingested.get(\"beta\"),\n",
        "            \"volatility\":        vol_30d,\n",
        "            \"var_5\":             risk_ingested.get(\"var_5\"),\n",
        "            \"sharpe_ratio\":      risk_ingested.get(\"sharpe_ratio\"),\n",
        "            \"max_drawdown\":      risk_ingested.get(\"max_drawdown\"),\n",
        "            \"sector_correlation\": None,\n",
        "            \"pe_ratio\":           None,\n",
        "            \"avg_daily_return\":   risk_ingested.get(\"avg_daily_return\"),\n",
        "            \"volatility_full\":    risk_ingested.get(\"volatility\"),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    earn_input = {\n",
        "        \"ticker\": symbol,\n",
        "        \"earnings\": (\n",
        "            earnings.sort_values(\"date\", ascending=False).head(8).to_dict(\"records\")\n",
        "            if earnings is not None and not earnings.empty else []\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    print(\"\\n[Parallel] Running News + Technical + Risk + Earnings (4 agents)…\")\n",
        "    t0 = time.time()\n",
        "    futures = {}\n",
        "    with ThreadPoolExecutor(max_workers=4) as pool:\n",
        "        if not getattr(SETTINGS, \"skip_news\", False):\n",
        "            futures[\"news\"] = pool.submit(NewsAnalysisAgent().process, news_input)\n",
        "        futures[\"technical\"] = pool.submit(MarketSignalsAgent().process, tech_input)\n",
        "        futures[\"risk\"] = pool.submit(RiskAssessmentAgent().process, risk_input)\n",
        "        futures[\"earnings\"] = pool.submit(EarningsAnalysisAgent().process, earn_input)\n",
        "\n",
        "        results: Dict[str, AgentResponse] = {}\n",
        "        for name, fut in futures.items():\n",
        "            results[name] = fut.result()\n",
        "            print(f\"  {name.capitalize():<10} Score={results[name].score:+.2f}  Conf={results[name].confidence:.0%}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"PARALLEL EXECUTION COMPLETE ({elapsed:.2f}s)\")\n",
        "    print(\"Pattern: Agents run concurrently to shorten wall time.\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return list(results.values())\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER (wraps the new orchestrator)\n",
        "# Uses your new run_pipeline() which already does: fetch→preprocess→route→agents→\n",
        "# synthesize→critique→(optional)re-synthesize→memory→evidence.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_evaluator_optimizer_workflow(\n",
        "    symbol: str,\n",
        "    start: str,\n",
        "    end: str,\n",
        "    required_tags: list[str] | None = None\n",
        ") -> OrchestratorResult:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"WORKFLOW PATTERN 3: EVALUATOR-OPTIMIZER\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Analyzing: {symbol} | Period: {start} → {end}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n[Phase 1] GENERATE: Running pipeline…\")\n",
        "    result = run_pipeline(symbol, start, end, required_tags)\n",
        "\n",
        "    initial = next((a for a in result.agent_outputs if \"Initial Synthesis\" in a.agent_name), None)\n",
        "    if initial:\n",
        "        _print_kv(\"initial_score:\", f\"{initial.score:+.2f}\")\n",
        "        _print_kv(\"initial_conf:\", f\"{initial.confidence:.0%}\")\n",
        "\n",
        "    print(\"\\n[Phase 2] EVALUATE: Critique\")\n",
        "    _print_kv(\"quality_score:\", f\"{result.critique.score:.2f}\")\n",
        "    _print_kv(\"adj_confidence:\", f\"{result.critique.confidence:.0%}\")\n",
        "    _print_kv(\"issues_found:\", len(result.critique.key_factors))\n",
        "\n",
        "    optimizer_ran = initial and (initial.analysis != result.final.analysis)\n",
        "    if optimizer_ran:\n",
        "        print(\"\\n[Phase 3] OPTIMIZE: Re-synthesized with critique feedback\")\n",
        "        _print_kv(\"final_score:\", f\"{result.final.score:+.2f}\")\n",
        "        _print_kv(\"final_conf:\", f\"{result.final.confidence:.0%}\")\n",
        "        delta = result.final.confidence - initial.confidence\n",
        "        _print_kv(\"conf_change:\", f\"{delta:+.0%}\")\n",
        "    else:\n",
        "        print(\"\\n[Phase 3] OPTIMIZE: Not needed (quality acceptable)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EVALUATOR-OPTIMIZER COMPLETE\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# DEMO RUNNER\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def demo_all_workflows(symbol: str = \"AAPL\"):\n",
        "    \"\"\"\n",
        "    Run all three patterns to demonstrate orchestration strategies aligned with\n",
        "    the NEW pipeline (prices + news + earnings + risk; indicators optional).\n",
        "    \"\"\"\n",
        "    start = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "    end = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    print(\"\\n\" + \"*\" * 40)\n",
        "    print(\"  DEMONSTRATING 3 AGENTIC WORKFLOW PATTERNS\")\n",
        "    print(\"*\" * 40)\n",
        "    print(f\"\\nTicker: {symbol}\")\n",
        "    print(f\"Date Range: {start} → {end}\\n\")\n",
        "\n",
        "    # 1) Prompt chaining (skips if skip_news=True)\n",
        "    r1 = run_prompt_chaining_workflow(symbol, start, end)\n",
        "\n",
        "    # 2) Parallel execution (no indicator calls; earnings+risk included)\n",
        "    r2 = run_parallel_workflow(symbol, start, end)\n",
        "\n",
        "    # 3) Evaluator-optimizer (uses new orchestrator)\n",
        "    r3 = run_evaluator_optimizer_workflow(symbol, start, end)\n",
        "\n",
        "    print(\"\\n\" + \"#\" * 40)\n",
        "    print(\"  ALL 3 WORKFLOW PATTERNS DEMONSTRATED\")\n",
        "    print(\"#\" * 40 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"prompt_chaining\": r1,\n",
        "        \"parallel\": r2,\n",
        "        \"evaluator_optimizer\": r3,\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_all_workflows(\"AAPL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f913beb",
      "metadata": {
        "id": "9f913beb"
      },
      "source": [
        "## ui/gradio_app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61af973",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7865\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ui/gradio_app.py\n",
        "import os, sys, traceback\n",
        "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta, datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from src.system.orchestrator import run_pipeline\n",
        "from src.config.settings import SETTINGS  # for runs_dir\n",
        "\n",
        "# ---------- persistence setup ----------\n",
        "RUNS_UI_DIR = SETTINGS.runs_dir / \"ui_runs\"\n",
        "RUNS_UI_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _df_to_csv(path: Path, df: pd.DataFrame):\n",
        "    try:\n",
        "        if df is None or (hasattr(df, \"empty\") and df.empty):\n",
        "            return\n",
        "        df.to_csv(path, index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _df_from_csv(path: Path) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path) if path.exists() else pd.DataFrame()\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def save_current_run(symbol, days_back, tags,\n",
        "                     plan_txt, agents_txt, crit_txt, final_txt,\n",
        "                     news_df, prices_df, earnings_df, risk_df):\n",
        "    # Guard: only save if something ran\n",
        "    if not any([plan_txt, agents_txt, crit_txt, final_txt]):\n",
        "        return \"Nothing to save yet. Run the analysis first.\"\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = RUNS_UI_DIR / f\"{(symbol or 'UNKNOWN').strip()}_{ts}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    meta = {\n",
        "        \"symbol\": symbol,\n",
        "        \"days_back\": int(days_back) if str(days_back).strip() else None,\n",
        "        \"tags\": tags,\n",
        "        \"timestamp\": ts,\n",
        "        \"plan\": plan_txt,\n",
        "        \"agents\": agents_txt,\n",
        "        \"critique\": crit_txt,\n",
        "        \"final\": final_txt,\n",
        "    }\n",
        "    (run_dir / \"meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    _df_to_csv(run_dir / \"news.csv\", news_df)\n",
        "    _df_to_csv(run_dir / \"prices.csv\", prices_df)\n",
        "    _df_to_csv(run_dir / \"earnings.csv\", earnings_df)\n",
        "    _df_to_csv(run_dir / \"risk.csv\", risk_df)\n",
        "\n",
        "    return f\"Saved to: {run_dir}\"\n",
        "\n",
        "def load_last_run():\n",
        "    runs = [p for p in RUNS_UI_DIR.iterdir() if p.is_dir()]\n",
        "    if not runs:\n",
        "        # note: return a numeric default for the Slider (e.g., 30)\n",
        "        return (\n",
        "            \"\", \"\", \"\", \"\",\n",
        "            pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(),\n",
        "            \"\", 30, \"\"   # ← days_back as int, not str\n",
        "        )\n",
        "    runs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    run_dir = runs[0]\n",
        "\n",
        "    meta_path = run_dir / \"meta.json\"\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
        "\n",
        "    plan_txt   = meta.get(\"plan\", \"\")\n",
        "    agents_txt = meta.get(\"agents\", \"\")\n",
        "    crit_txt   = meta.get(\"critique\", \"\")\n",
        "    final_txt  = meta.get(\"final\", \"\")\n",
        "    symbol     = str(meta.get(\"symbol\", \"\") or \"\")\n",
        "    tags       = str(meta.get(\"tags\", \"\") or \"\")\n",
        "\n",
        "    # ensure numeric for Slider\n",
        "    raw_days = meta.get(\"days_back\", 30)\n",
        "    try:\n",
        "        days_back = int(raw_days)\n",
        "    except Exception:\n",
        "        days_back = 30\n",
        "\n",
        "    news_df     = _df_from_csv(run_dir / \"news.csv\")\n",
        "    prices_df   = _df_from_csv(run_dir / \"prices.csv\")\n",
        "    earnings_df = _df_from_csv(run_dir / \"earnings.csv\")\n",
        "    risk_df     = _df_from_csv(run_dir / \"risk.csv\")\n",
        "\n",
        "    return (plan_txt, agents_txt, crit_txt, final_txt,\n",
        "            news_df, prices_df, earnings_df, risk_df,\n",
        "            symbol, days_back, tags)   # ← days_back is int\n",
        "\n",
        "def _apply_loaded(plan_txt, agents_txt, crit_txt, final_txt,\n",
        "                  news_df, prices_df, earnings_df, risk_df,\n",
        "                  sym, days, tagstr):\n",
        "    # coerce days to an int for the Slider\n",
        "    try:\n",
        "        days_val = int(days)\n",
        "    except Exception:\n",
        "        days_val = 30\n",
        "    status = f\"Loaded last run from: {RUNS_UI_DIR}\"\n",
        "    return (\n",
        "        plan_txt, agents_txt, crit_txt, final_txt,\n",
        "        news_df, prices_df, earnings_df, risk_df,\n",
        "        gr.update(value=str(sym or \"\")),\n",
        "        gr.update(value=days_val),   # ← number, not string\n",
        "        gr.update(value=str(tagstr or \"\")),\n",
        "        status\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------- small helpers ----------\n",
        "def _truncate(s: str, max_len: int = 8000) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return (s[: max_len - 20] + \" … (truncated)\") if len(s) > max_len else s\n",
        "\n",
        "def _as_text(x):\n",
        "    import json\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    if isinstance(x, (dict, list)):\n",
        "        return json.dumps(x, ensure_ascii=False, indent=2, sort_keys=True)\n",
        "    return str(x)\n",
        "\n",
        "def _clean(s: str) -> str:\n",
        "    s = _as_text(s)\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = s.strip(\"`\").strip()\n",
        "    return s\n",
        "\n",
        "def _synth_to_prose(obj):\n",
        "    if not isinstance(obj, dict):\n",
        "        return _clean(_as_text(obj))\n",
        "    parts = []\n",
        "\n",
        "    ms = obj.get(\"market_signals\") or {}\n",
        "    if ms:\n",
        "        ms_bits = []\n",
        "        cp = ms.get(\"current_price\")\n",
        "        if isinstance(cp, (int, float)):\n",
        "            ms_bits.append(f\"price ${cp:,.2f}\")\n",
        "        ma = ms.get(\"moving_averages\") or {}\n",
        "        ma50 = ma.get(\"50_day\")\n",
        "        ma200 = ma.get(\"200_day\")\n",
        "        if (ma50 is not None) or (ma200 is not None):\n",
        "            ms_bits.append(f\"vs 50D {ma50}, 200D {ma200}\")\n",
        "        rsi = ms.get(\"RSI\")\n",
        "        if rsi is not None:\n",
        "            ms_bits.append(f\"RSI {rsi}\")\n",
        "        trend = ms.get(\"trend\")\n",
        "        if trend:\n",
        "            ms_bits.append(trend)\n",
        "        vol = ms.get(\"volume\") or {}\n",
        "        vcur, vavg = vol.get(\"current\"), vol.get(\"average\")\n",
        "        if vcur is not None and vavg is not None:\n",
        "            ms_bits.append(f\"volume {vcur:,} vs avg {vavg:,}\")\n",
        "        if ms_bits:\n",
        "            parts.append(\"Technicals: \" + \", \".join(str(x) for x in ms_bits if x))\n",
        "\n",
        "    news = obj.get(\"news\") or {}\n",
        "    if news:\n",
        "        news_bits = []\n",
        "        for k in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "            if k in news:\n",
        "                news_bits.append(f\"{k}: {news[k]}\")\n",
        "        for k, v in news.items():\n",
        "            if k not in (\"sentiment\", \"growth potential\", \"competitive landscape\"):\n",
        "                news_bits.append(f\"{k}: {v}\")\n",
        "        parts.append(\"News: \" + \"; \".join(news_bits))\n",
        "\n",
        "    risk = obj.get(\"risk_assessment\") or {}\n",
        "    if risk:\n",
        "        risk_bits = []\n",
        "        for k in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "            if k in risk:\n",
        "                risk_bits.append(f\"{k}: {risk[k]}\")\n",
        "        for k, v in risk.items():\n",
        "            if k not in (\"volatility\", \"data_gaps\", \"idiosyncratic_risks\"):\n",
        "                risk_bits.append(f\"{k}: {v}\")\n",
        "        parts.append(\"Risk: \" + \"; \".join(risk_bits))\n",
        "\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "def _to_df(x):\n",
        "    if isinstance(x, pd.DataFrame):\n",
        "        return x\n",
        "    if x is None:\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.DataFrame(x)\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "# ---------- /helpers ----------\n",
        "\n",
        "def run(symbol, days_back, required_tags_csv):\n",
        "    try:\n",
        "        start = (date.today() - timedelta(days=int(days_back))).isoformat()\n",
        "        end = date.today().isoformat()\n",
        "        tags = [t.strip() for t in required_tags_csv.split(\",\")] if required_tags_csv else None\n",
        "\n",
        "        res = run_pipeline(symbol.strip().upper(), start, end, required_tags=tags)\n",
        "\n",
        "        # Detect optimizer re-synthesis\n",
        "        optimizer_ran = False\n",
        "        init = next((a for a in res.agent_outputs if a.agent_name in {\"Initial Synthesis\", \"Research Synthesis Agent\", \"SynthesisAgent\"}), None)\n",
        "        if init is not None:\n",
        "            init_txt = _clean(_as_text(init.analysis))\n",
        "            final_txt_norm = _clean(_as_text(res.final.analysis))\n",
        "            optimizer_ran = (init_txt != final_txt_norm)\n",
        "\n",
        "        plan = \"\\n\".join([f\"• {step}\" for step in res.plan])\n",
        "\n",
        "        # Agents panel (truncate to keep websocket payload small)\n",
        "        agents_txt = \"\\n\\n\".join([\n",
        "            (\n",
        "                f\"[{a.agent_name}] score={a.score:.2f} conf={a.confidence:.2f}\\n\"\n",
        "                f\"{_synth_to_prose(a.analysis) if ('synthesis' in a.agent_name.lower()) else _clean(_as_text(a.analysis))}\"\n",
        "            )\n",
        "            for a in res.agent_outputs\n",
        "        ])\n",
        "        agents_txt = _truncate(agents_txt, 15000)\n",
        "\n",
        "        # Evidence tables\n",
        "        news_rows      = _to_df(res.evidence.get(\"top_news\", []))\n",
        "        prices_rows    = _to_df(res.evidence.get(\"prices_tail\", []))\n",
        "        earnings_rows  = _to_df(res.evidence.get(\"earnings_head\", []))   # NEW\n",
        "        risk_rows      = _to_df(res.evidence.get(\"risk_metrics\", []))    # NEW (single-row DF)\n",
        "\n",
        "        if news_rows.empty:\n",
        "            agents_txt += \"\\n\\n[Note] No news items matched filters or API limits were hit today.\"\n",
        "\n",
        "        crit_txt = (\n",
        "            f\"[Critique]\\n\"\n",
        "            f\"score={res.critique.score:.2f} adj_conf={res.critique.confidence:.2f}\\n\"\n",
        "            f\"{_clean(_as_text(res.critique.analysis))}\"\n",
        "        )\n",
        "        crit_txt = _truncate(crit_txt, 6000)\n",
        "\n",
        "        headline = \"FINAL (After Critique)\"\n",
        "        opt_line = \"[Optimizer ran: YES]\" if optimizer_ran else \"[Optimizer ran: NO]\"\n",
        "        final_txt = (\n",
        "            f\"{headline}\\n{opt_line}\\n\"\n",
        "            f\"score={res.final.score:.2f} conf={res.final.confidence:.2f}\\n\"\n",
        "            f\"{_synth_to_prose(res.final.analysis)}\\n\\nKey: {', '.join(res.final.key_factors)}\"\n",
        "        )\n",
        "        final_txt = _truncate(final_txt, 8000)\n",
        "\n",
        "        # Return order MUST match component outputs order\n",
        "        return (\n",
        "            plan,\n",
        "            agents_txt,\n",
        "            crit_txt,\n",
        "            final_txt,\n",
        "            news_rows,\n",
        "            prices_rows,\n",
        "            earnings_rows,   # NEW\n",
        "            risk_rows        # NEW\n",
        "        )\n",
        "\n",
        "    except Exception:\n",
        "        tb = traceback.format_exc()\n",
        "        err = f\"[FATAL] An exception occurred in run():\\n{tb}\"\n",
        "        blank_df = pd.DataFrame()\n",
        "        return \"run() error — see Critique tab\", _truncate(err, 15000), _truncate(err, 6000), _truncate(err, 8000), blank_df, blank_df, blank_df, blank_df\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Agentic Finance\") as demo:\n",
        "    gr.Markdown(\"# Agentic Finance — Interactive Tester\")\n",
        "\n",
        "    with gr.Row():\n",
        "        symbol = gr.Textbox(label=\"Ticker\", value=\"AAPL\")\n",
        "        days_back = gr.Slider(7, 120, value=30, step=1, label=\"Days Back\")\n",
        "        tags = gr.Textbox(label=\"Required Tags (optional, comma-sep)\", placeholder=\"earnings, product\")\n",
        "    run_btn = gr.Button(\"Run\")\n",
        "\n",
        "    # NEW: persistence controls\n",
        "    with gr.Row():\n",
        "        save_btn = gr.Button(\"Save run\")\n",
        "        load_btn = gr.Button(\"Load last run\")\n",
        "    save_status = gr.Textbox(label=\"Save/Load status\", interactive=False)\n",
        "\n",
        "    plan   = gr.Textbox(label=\"Plan\", lines=6)\n",
        "    agents = gr.Textbox(label=\"Agent Outputs\", lines=14)\n",
        "    crit   = gr.Textbox(label=\"Critique\", lines=8)\n",
        "    final  = gr.Textbox(label=\"Final Recommendation\", lines=10)\n",
        "\n",
        "    news_tbl     = gr.Dataframe(\n",
        "        headers=[\"published_at\",\"source\",\"title\",\"summary\",\"url\",\"overall_sentiment\",\"tags\",\"numbers\"],\n",
        "        label=\"Top News (evidence)\",\n",
        "        wrap=True\n",
        "    )\n",
        "    prices_tbl   = gr.Dataframe(label=\"Recent Prices (evidence)\")\n",
        "    earnings_tbl = gr.Dataframe(label=\"Earnings (evidence)\")           # NEW\n",
        "    risk_tbl     = gr.Dataframe(label=\"Risk Metrics (evidence)\")       # NEW\n",
        "\n",
        "    run_btn.click(\n",
        "        run,\n",
        "        inputs=[symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl]  # NEW outputs\n",
        "    )\n",
        "\n",
        "    # Wire up Save\n",
        "    save_btn.click(\n",
        "        save_current_run,\n",
        "        inputs=[symbol, days_back, tags, plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl],\n",
        "        outputs=[save_status],\n",
        "    )\n",
        "\n",
        "    # Wire up Load (prefill outputs + inputs)\n",
        "    load_btn.click(\n",
        "        load_last_run,\n",
        "        inputs=[],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "    ).then(\n",
        "        _apply_loaded,\n",
        "        inputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags, save_status],\n",
        "    )\n",
        "\n",
        "    # Auto-load last run on app open (optional but handy)\n",
        "    demo.load(\n",
        "        load_last_run,\n",
        "        inputs=[],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "    ).then(\n",
        "        _apply_loaded,\n",
        "        inputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags],\n",
        "        outputs=[plan, agents, crit, final, news_tbl, prices_tbl, earnings_tbl, risk_tbl, symbol, days_back, tags, save_status],\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Queue/launch shim for broad Gradio compatibility\n",
        "    try:\n",
        "        demo.queue()\n",
        "    except TypeError:\n",
        "        try:\n",
        "            demo.queue(max_size=16)\n",
        "        except TypeError:\n",
        "            pass\n",
        "    \n",
        "    if __name__ == \"__main__\":\n",
        "        import socket\n",
        "\n",
        "    def _get_free_port(start=7860, end=7890):\n",
        "        for p in range(start, end + 1):\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "                try:\n",
        "                    s.bind((\"127.0.0.1\", p))\n",
        "                    return p\n",
        "                except OSError:\n",
        "                    continue\n",
        "        return None  # let Gradio auto-pick if needed\n",
        "\n",
        "    # Try to queue; ignore older Gradio signatures\n",
        "    try:\n",
        "        demo.queue()\n",
        "    except TypeError:\n",
        "        try:\n",
        "            demo.queue(max_size=16)\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    port = _get_free_port()  # None → let Gradio auto-choose\n",
        "\n",
        "    try:\n",
        "        demo.launch(\n",
        "            share=False,\n",
        "            server_name=\"127.0.0.1\",\n",
        "            server_port=port,      # may be None; Gradio will auto-pick\n",
        "            show_error=True\n",
        "        )\n",
        "    except OSError:\n",
        "        # Fallback: force auto-pick any free port\n",
        "        demo.launch(\n",
        "            share=False,\n",
        "            server_name=\"127.0.0.1\",\n",
        "            server_port=None,\n",
        "            show_error=True\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "a520",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
